{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f48262",
   "metadata": {},
   "source": [
    "This notebook performs the synthesis using our proposed synthesization approach for the training data only (excludes the holdout data). The synthesis model is a logistic/multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import itertools\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from bayesian_bootstrap import bayesian_bootstrap\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26905f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmse_ratio(original_data, synthetic_data):\n",
    "    \n",
    "    N_synth = synthetic_data.shape[0]\n",
    "    N_orig = original_data.shape[0]\n",
    "    \n",
    "    # combine original and synthetic datasets\n",
    "    full_X = pd.concat([original_data, synthetic_data], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # generate interactions and powers of variables\n",
    "    poly = PolynomialFeatures(2, interaction_only=False, include_bias=False)\n",
    "    \n",
    "    full_X = poly.fit_transform(full_X)\n",
    "\n",
    "    # scale the combined dataset\n",
    "    full_X = preprocessing.StandardScaler().fit_transform(full_X)\n",
    "    \n",
    "    c = N_synth/(N_synth+N_orig)\n",
    "\n",
    "    y = np.repeat([0, 1], repeats=[N_orig, N_synth])\n",
    "    \n",
    "    pMSE_model = LogisticRegression(penalty=None, max_iter=1000).fit(full_X, y)\n",
    "    \n",
    "    probs = pMSE_model.predict_proba(full_X)\n",
    "    \n",
    "    pMSE = 1/(N_synth+N_orig) * np.sum((probs[:,1] - c)**2)\n",
    "    \n",
    "    e_pMSE = 2*(full_X.shape[1])*(1-c)**2 * c/(N_synth+N_orig)\n",
    "        \n",
    "    return pMSE/e_pMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8ccbc",
   "metadata": {},
   "source": [
    "Import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standardized lat/long location data\n",
    "train_data = pd.read_csv(\"../Data/train_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e00db4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ef6c3",
   "metadata": {},
   "source": [
    "Logistic and multinomial logistic regression synthesizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_and_standardize(dataset, poly_degree=3, interaction_only=False):\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=poly_degree, interaction_only=interaction_only, include_bias=False)\n",
    "    \n",
    "    X = poly.fit_transform(dataset)\n",
    "    \n",
    "    scaled_X = preprocessing.StandardScaler().fit_transform(X)\n",
    "    \n",
    "    return scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_synthesizer(orig_data, synth_data_sets, target, penalty_param, poly_degree=3, interaction_only=False):\n",
    "    \n",
    "    mn_model = LogisticRegression(penalty='l1', C=penalty_param, solver='saga', max_iter=1000, multi_class='multinomial', random_state=rng)\n",
    "    \n",
    "    X = polynomial_and_standardize(dataset=orig_data, poly_degree=poly_degree, interaction_only=interaction_only)\n",
    "    \n",
    "    sXs = [polynomial_and_standardize(dataset=Y, poly_degree=poly_degree, interaction_only=interaction_only) for Y in synth_data_sets]\n",
    "    \n",
    "    vals = []\n",
    "    \n",
    "    mn_model.fit(X, target)\n",
    "    \n",
    "    rng_mn = default_rng()\n",
    "    \n",
    "    for Y in sXs:\n",
    "        \n",
    "        probs = mn_model.predict_proba(Y)\n",
    "    \n",
    "        v = [np.argmax(rng_mn.multinomial(n=1, pvals=p, size=1)==1) for p in probs]\n",
    "    \n",
    "        vals.append(pd.Series(v, name=target.name))\n",
    "    \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_mn(#overall parameters\n",
    "                    train_data,\n",
    "                    number_synthetic_datasets,\n",
    "                    # hyperparameters for GMM, end with underscore means Bayesian optimization will choose\n",
    "                    number_gmm_initializations,\n",
    "                    num_components_,\n",
    "                    # hyperparameters for CART, end with underscore means Bayesian optimization will choose\n",
    "                    C_sex_,\n",
    "                    C_age_,\n",
    "                    C_state_):\n",
    "    \n",
    "    num_samples = train_data.shape[0]\n",
    "    \n",
    "    ########## Code for GMM ############\n",
    "    \n",
    "    # fit GMM model\n",
    "    GMM = GaussianMixture(num_components_, n_init=number_gmm_initializations, init_params=\"k-means++\", random_state=rng).fit(train_data.loc[:,[\"date\", \"latitude\", \"longitude\"]])\n",
    "    \n",
    "    # list for synthetic datasets\n",
    "    sXs = []\n",
    "    \n",
    "    # generate and store number_synthetic_datasets synthetic datasets\n",
    "    for i in range(number_synthetic_datasets):\n",
    "        sX = GMM.sample(num_samples)[0]\n",
    "        sX = pd.DataFrame(sX)\n",
    "        sX.columns = ['date', 'latitude', 'longitude']\n",
    "        sX['date'] = pd.Series(np.rint(sX['date']))\n",
    "        sXs.append(sX)\n",
    "        \n",
    "    ####################################################################################################\n",
    "        \n",
    "    ########### Code for sex MN ##########\n",
    "    \n",
    "    synth_sex_vars = multinomial_synthesizer(orig_data=train_data.loc[:,[\"date\", \"latitude\", \"longitude\"]], \n",
    "                                             synth_data_sets=sXs, \n",
    "                                             target=train_data.sex, \n",
    "                                             penalty_param=C_sex_)\n",
    "    \n",
    "    sXs = [pd.concat([Y, synth_sex_vars[i]], axis=1) for i,Y in enumerate(sXs)]\n",
    "        \n",
    "    ####################################################################################################\n",
    "        \n",
    "    ########### Code for age MN ##########\n",
    "    \n",
    "    synth_age_vars = multinomial_synthesizer(orig_data=train_data.loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\"]], \n",
    "                                             synth_data_sets=sXs, \n",
    "                                             target=train_data.age, \n",
    "                                             penalty_param=C_age_)\n",
    "    \n",
    "    sXs = [pd.concat([Y, synth_age_vars[i]], axis=1) for i,Y in enumerate(sXs)]\n",
    "        \n",
    "    ####################################################################################################\n",
    "    \n",
    "    ########### Code for state MN ##########\n",
    "    \n",
    "    synth_state_vars = multinomial_synthesizer(orig_data=train_data.loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\", \"age\"]], \n",
    "                                               synth_data_sets=sXs, \n",
    "                                               target=train_data.state, \n",
    "                                               penalty_param=C_state_)\n",
    "    \n",
    "    sXs = [pd.concat([Y, synth_state_vars[i]], axis=1) for i,Y in enumerate(sXs)]\n",
    "        \n",
    "    ###### Calculate pMSE ratios ######\n",
    "    pmse_ratios = [pmse_ratio(train_data, Y) for Y in sXs]\n",
    "    \n",
    "    return pmse_ratios, sXs, GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b65e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_models_mn(train_data,\n",
    "                       number_synthetic_datasets,\n",
    "                       number_gmm_initializations,\n",
    "                       random_state):\n",
    "\n",
    "    def evaluate_models(num_components_, C_sex_, C_age_, C_state_):\n",
    "\n",
    "        pmse_ratios, _, _ = train_models_mn(train_data=train_data,\n",
    "                                            number_synthetic_datasets=number_synthetic_datasets,\n",
    "                                            number_gmm_initializations=number_gmm_initializations,\n",
    "                                            num_components_=int(num_components_),\n",
    "                                            C_sex_=C_sex_,\n",
    "                                            C_age_=C_age_,\n",
    "                                            C_state_=C_state_)\n",
    "        \n",
    "        return -1 * ((1 - np.mean(pmse_ratios))**2)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=evaluate_models,\n",
    "        pbounds={\n",
    "            \"num_components_\": (200, 800.99),\n",
    "            \"C_sex_\": (0.0001, 3),\n",
    "            \"C_age_\": (0.0001, 3),\n",
    "            \"C_state_\": (0.0001, 3)\n",
    "        },\n",
    "        random_state=random_state)\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", xi=1e-10)\n",
    "    optimizer.maximize(init_points=5, n_iter=25, acquisition_function=utility)\n",
    "    print(\"Final Result: \", optimizer.max)\n",
    "    return optimizer.max, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d62c3",
   "metadata": {},
   "source": [
    "The default value for $\\alpha = 1e-06$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b295a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd = 20\n",
    "ngi = 5\n",
    "random_states = [np.random.RandomState(1234), np.random.RandomState(4321), np.random.RandomState(10620), np.random.RandomState(91695), np.random.RandomState(31296)]\n",
    "# random_states = [np.random.RandomState(1234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c78ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results = [optimize_models_mn(train_data=train_data, number_synthetic_datasets=nsd, number_gmm_initializations=ngi, random_state=r) for r in random_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_targets = [np.minimum.accumulate(-i[1].space.target) for i in optimization_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4225965",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_targets[0])\n",
    "plt.scatter(np.arange(len(run_targets[0])), run_targets[0], s=6)\n",
    "plt.plot(run_targets[1])\n",
    "plt.scatter(np.arange(len(run_targets[1])), run_targets[1], s=6)\n",
    "plt.plot(run_targets[2])\n",
    "plt.scatter(np.arange(len(run_targets[2])), run_targets[2], s=6)\n",
    "plt.plot(run_targets[3])\n",
    "plt.scatter(np.arange(len(run_targets[3])), run_targets[3], s=6)\n",
    "plt.plot(run_targets[4])\n",
    "plt.scatter(np.arange(len(run_targets[4])), run_targets[4], s=6)\n",
    "plt.title(\"Running Minimum Objective Value for MNL Synthesis\")\n",
    "plt.ylim(-0.01, 0.47)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34094ad9",
   "metadata": {},
   "source": [
    "Choose the params that gave the best objective value across all random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eccd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optimization_results[np.argmax([x[0]['target'] for x in optimization_results])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1e3fa",
   "metadata": {},
   "source": [
    "Generate 20 synthetic data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmse_ratios, full_sXs, GMM = train_models_mn(train_data=train_data,\n",
    "                                             number_synthetic_datasets=20,\n",
    "                                             # hyperparameters for GMM\n",
    "                                             number_gmm_initializations=ngi,\n",
    "                                             num_components_=int(best_params['params']['num_components_']),\n",
    "                                             # hyperparameters for CART, end with underscore means Bayesian optimization will choose\n",
    "                                             C_sex_=best_params['params']['C_sex_'],\n",
    "                                             C_age_=best_params['params']['C_age_'],\n",
    "                                             C_state_=best_params['params']['C_state_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pmse_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(pmse_ratios)\n",
    "plt.xlabel(\"Density\")\n",
    "plt.ylabel(\"pMSE Ratio\")\n",
    "plt.title(\"Distribution of pMSE Ratios\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4b464",
   "metadata": {},
   "source": [
    "# Save the synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sX in enumerate(full_sXs):\n",
    "    sX.to_csv(\"../Data/synthetic_datasets/logistic_logistic_pmse_\" + str(i) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cacd36",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b6f70",
   "metadata": {},
   "source": [
    "Use inference reduction algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4747d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_reduction(original_data, synthetic_data, mixture_model, deltas, c, prior_prob):\n",
    "    \n",
    "    # number of original records\n",
    "    num_records = original_data.shape[0]\n",
    "    \n",
    "    # record percentages\n",
    "    print_nums = [int(np.ceil(i*num_records)) for i in [0.25, 0.5, 0.75]]\n",
    "    \n",
    "    # random number generator\n",
    "    rng = default_rng()\n",
    "    \n",
    "    # copy the synthetic dataset\n",
    "    new_sX = synthetic_data\n",
    "    \n",
    "    # tree for synthetic locations\n",
    "    sX_tree = cKDTree(new_sX[[\"latitude\", \"longitude\"]])\n",
    "    \n",
    "    # store mixture component parameters\n",
    "    mus = mixture_model.means_\n",
    "    sigmas = mixture_model.covariances_\n",
    "    \n",
    "    # temporary count of the number of rows that violate one or more conditions\n",
    "    violator_count = num_records\n",
    "    \n",
    "    # number of anonymization loops required\n",
    "    num_loops = 1\n",
    "    \n",
    "    # while we have any violator rows\n",
    "    while violator_count > 0:\n",
    "        \n",
    "        # reset violator count\n",
    "        violator_count = 0\n",
    "        \n",
    "        # for each original record\n",
    "        # we shuffle the records each time so that the violating records are fixed in a random order\n",
    "        for i, original_record in original_data.sample(frac=1.0).reset_index(drop=True).iterrows():\n",
    "            \n",
    "            original_location = original_record.loc[[\"latitude\", \"longitude\"]]\n",
    "            original_categorical = original_record.loc[[\"sex\", \"age\", \"state\"]]\n",
    "            \n",
    "            # for each delta\n",
    "            for delta in deltas:\n",
    "                    \n",
    "                ##### Test the Inference Criterion\n",
    "                \n",
    "                # find synthetic neighbors based on location\n",
    "                location_neighbors = sX_tree.query_ball_point(original_location, r=delta, p=2.0)\n",
    "                \n",
    "                # matches on categorical attributes from location neighbors\n",
    "                categorical_matches = (new_sX.loc[location_neighbors,['sex', 'age']] == original_categorical[[\"sex\", \"age\"]]).all(1)\n",
    "                \n",
    "                matching_rows = new_sX.loc[location_neighbors,:].loc[categorical_matches.values,:]\n",
    "                \n",
    "                # if there are any records in the location neighborhood that match on sex and age\n",
    "                \n",
    "                if matching_rows.shape[0] > 0:\n",
    "                \n",
    "                    if original_categorical['state'] == 1.0:\n",
    "                        prior = prior_prob\n",
    "                    else:\n",
    "                        prior = 1-prior_prob\n",
    "                        \n",
    "                    num_matching = np.sum(matching_rows['state'] == original_categorical['state'])\n",
    "            \n",
    "                    cond = num_matching/matching_rows.shape[0] * 1/prior\n",
    "                \n",
    "                    if cond > c:\n",
    "                        \n",
    "                        # add one to number of violators\n",
    "                        violator_count += 1\n",
    "                        \n",
    "                        # number of records with non-matching sensitive variable needed to meet inference\n",
    "                        num_needed = int(np.ceil(num_matching/(prior*c) - matching_rows.shape[0]))\n",
    "                        \n",
    "                        # find the component with the highest responsibility for the confidential record\n",
    "                        component_index = np.argmax(GMM.predict_proba(pd.DataFrame(original_location).T), axis = 1)[0]\n",
    "                        current_mu = mus[component_index,:]\n",
    "                        current_sigma = sigmas[component_index,:,:]\n",
    "            \n",
    "                        valid_candidates = np.zeros((0,2))\n",
    "                        num_candidate_loops = 0\n",
    "                        while valid_candidates.shape[0] < num_needed:\n",
    "                        \n",
    "                            # add a very small number to the diagonal of the covariance matrix to increase\n",
    "                            # the likelihood of sampling candidate points - do this proportionally to the number\n",
    "                            # of times this loop has executed\n",
    "                            # np.fill_diagonal(current_sigma, np.diag(current_sigma) + num_candidate_loops*1e-6)\n",
    "                        \n",
    "                            # generate a bunch of candidate points\n",
    "                            candidate_points = rng.multivariate_normal(current_mu, current_sigma, size=100000)\n",
    "                            candidate_tree = cKDTree(candidate_points)\n",
    "                            valid_indices = candidate_tree.query_ball_point(original_location, delta, p=2.0, return_sorted=True)\n",
    "                            valid_candidates = np.vstack([valid_candidates, candidate_points[valid_indices,:]])\n",
    "                            num_candidate_loops += 1\n",
    "                            if num_candidate_loops > 100:\n",
    "                                print('Stuck in inference loop.')\n",
    "                            \n",
    "                        # print(num_candidate_loops)\n",
    "                    \n",
    "                        # select the number of needed candidates\n",
    "                        new_locations = valid_candidates[:num_needed,:]\n",
    "                    \n",
    "                        new_categorical = np.vstack([np.array(original_categorical).reshape(1,-1) for k in range(num_needed)])\n",
    "                    \n",
    "                        new_records = pd.DataFrame(np.hstack([new_locations, new_categorical]))\n",
    "                    \n",
    "                        new_records.columns = new_sX.columns\n",
    "                        \n",
    "                        new_records['state'] = 1.0 - original_categorical['state']\n",
    "                    \n",
    "                        new_sX = pd.concat([new_sX, new_records], axis=0).reset_index(drop=True)\n",
    "                    \n",
    "                        # rebuild the tree for synthetic locations\n",
    "                        sX_tree = cKDTree(new_sX[[\"latitude\", \"longitude\"]])\n",
    "                    \n",
    "#                     # put in a break statement - if we executed the previous code conditional on the if statement,\n",
    "#                     # we know the linkability criterion is satisfied for all larger delta values for this record\n",
    "#                     break\n",
    "    \n",
    "            if int(i) in print_nums:\n",
    "                print(\"Record \" + str(i) + \" completed.\")\n",
    "                \n",
    "        print(\"Full anonymization loop \" + str(num_loops) + \" completed.\")\n",
    "        \n",
    "        num_loops += 1\n",
    "                    \n",
    "    return new_sX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e49381",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_death_prob = 152/9583\n",
    "delta_vals = np.linspace(0.00001, 1.0, 20)\n",
    "c=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sXs = [inference_reduction(train_data, X, GMM, delta_vals, c, prior_death_prob) for X in full_sXs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in inference_sXs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1150f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sX in enumerate(inference_sXs):\n",
    "    sX.to_csv(\"Data/synthetic_datasets/logistic_logistic_inf_pmse_\" + str(i) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20687ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
