{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f48262",
   "metadata": {},
   "source": [
    "This notebook performs the synthesis using our proposed synthesization approach for the training data only (excludes the holdout data). The synthesis model is a CART. The model used for the KS-statistic calculation is also a CART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import itertools\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from bayesian_bootstrap import bayesian_bootstrap\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57505dd1",
   "metadata": {},
   "source": [
    "Steps for CART estimation of pmse ratio.\n",
    "\n",
    "* calculate the pMSE between pairs of synthetic data sets generated from the same original data\n",
    "* the pairs can be used to estimate the expected pMSE even when the synthesizing model is incorrect since both data are drawn from the same distribution\n",
    "* for most large complex data sets, synthesized by CART models, the expected pMSE from pairs will be close to, or slightly lower than the null pMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ad9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmse_ratio(original_data, synthetic_data):\n",
    "    \n",
    "    N_synth = synthetic_data.shape[0]\n",
    "    N_orig = original_data.shape[0]\n",
    "    \n",
    "    # combine original and synthetic datasets\n",
    "    full_X = pd.concat([original_data, synthetic_data], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # generate interactions and powers of variables\n",
    "    poly = PolynomialFeatures(2, interaction_only=False, include_bias=False)\n",
    "    \n",
    "    full_X = poly.fit_transform(full_X)\n",
    "\n",
    "    # scale the combined dataset\n",
    "    full_X = preprocessing.StandardScaler().fit_transform(full_X)\n",
    "    \n",
    "    c = N_synth/(N_synth+N_orig)\n",
    "\n",
    "    y = np.repeat([0, 1], repeats=[N_orig, N_synth])\n",
    "    \n",
    "    pMSE_model = LogisticRegression(penalty=None, max_iter=1000).fit(full_X, y)\n",
    "    \n",
    "    probs = pMSE_model.predict_proba(full_X)\n",
    "    \n",
    "    pMSE = 1/(N_synth+N_orig) * np.sum((probs[:,1] - c)**2)\n",
    "    \n",
    "    e_pMSE = 2*(full_X.shape[1])*(1-c)**2 * c/(N_synth+N_orig)\n",
    "        \n",
    "    return pMSE/e_pMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8ccbc",
   "metadata": {},
   "source": [
    "Import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standardized lat/long location data\n",
    "train_data = pd.read_csv(\"../Data/train_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62abff",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a741f",
   "metadata": {},
   "source": [
    "# Full Sequential Synthesis Driven by Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169fec7",
   "metadata": {},
   "source": [
    "Function to be used in Bayesian bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1688b8",
   "metadata": {},
   "source": [
    "Write function to train all models and generate the synthetic dataset, then evaluate the pMSE ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(#overall parameters\n",
    "                 train_data,\n",
    "                 number_synthetic_datasets,\n",
    "                 # hyperparameters for GMM, end with underscore means Bayesian optimization will choose\n",
    "                 number_gmm_initializations,\n",
    "                 num_components_,\n",
    "                 # hyperparameters for CART, end with underscore means Bayesian optimization will choose\n",
    "                 mb_sex_,\n",
    "                 mb_age_,\n",
    "                 mb_state_):\n",
    "    \n",
    "    num_samples = train_data.shape[0]\n",
    "    \n",
    "    ########## Code for GMM ############\n",
    "    \n",
    "    # fit GMM model\n",
    "    GMM = GaussianMixture(num_components_, n_init=number_gmm_initializations, init_params=\"k-means++\", random_state=rng).fit(train_data.loc[:,[\"date\", \"latitude\", \"longitude\"]])\n",
    "    \n",
    "    # list for synthetic datasets\n",
    "    sXs = []\n",
    "    \n",
    "    # generate and store number_synthetic_datasets synthetic datasets\n",
    "    for i in range(number_synthetic_datasets):\n",
    "        sX = GMM.sample(num_samples)[0]\n",
    "        sX = pd.DataFrame(sX)\n",
    "        sX.columns = ['date', 'latitude', 'longitude']\n",
    "        sX['date'] = pd.Series(np.rint(sX['date']))\n",
    "        sXs.append(sX)\n",
    "        \n",
    "    ####################################################################################################\n",
    "        \n",
    "    ########### Code for sex CART ##########\n",
    "    \n",
    "    cart_sex = DecisionTreeClassifier(min_samples_leaf=mb_sex_, random_state=rng)\n",
    "    \n",
    "    cart_sex.fit(X=train_data.loc[:,[\"date\", \"latitude\", \"longitude\"]], y=train_data.loc[:,\"sex\"])\n",
    "    \n",
    "    node_indicators = cart_sex.decision_path(train_data.loc[:,[\"date\", \"latitude\", \"longitude\"]]).toarray()\n",
    "    \n",
    "    node_outcomes = [train_data.sex[node_indicators[:,x]==1] for x in np.arange(node_indicators.shape[1])]\n",
    "    \n",
    "    # sample values according to a Bayesian bootstrap\n",
    "    \n",
    "    for i in range(number_synthetic_datasets):\n",
    "        \n",
    "        bst_vals = [bayesian_bootstrap(X=np.array(x), \n",
    "                                       statistic=stat,\n",
    "                                       n_replications=1,\n",
    "                                       resample_size=len(x))[0] for x in node_outcomes]\n",
    "        \n",
    "        synth_leaves = cart_sex.apply(sXs[i].loc[:,[\"date\", \"latitude\", \"longitude\"]])\n",
    "        \n",
    "        new_sex = np.zeros(len(synth_leaves), dtype=int)\n",
    "        \n",
    "        for j, x in enumerate(np.arange(node_indicators.shape[1])):\n",
    "            \n",
    "            new_sex[synth_leaves==x] = rng.choice(bst_vals[x], size=np.sum(synth_leaves==x))\n",
    "        \n",
    "        new_sex = pd.Series(new_sex)\n",
    "        \n",
    "        new_sex.name = \"sex\"\n",
    "        \n",
    "        sXs[i] = pd.concat([sXs[i], new_sex], axis=1)\n",
    "        \n",
    "    ####################################################################################################\n",
    "        \n",
    "    ########### Code for age CART ##########\n",
    "    cart_age = DecisionTreeClassifier(min_samples_leaf=mb_age_, random_state=rng)\n",
    "    \n",
    "    cart_age.fit(X=train_data.loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\"]], y=train_data.loc[:,\"age\"])\n",
    "    \n",
    "    node_indicators = cart_age.decision_path(train_data.loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\"]]).toarray()\n",
    "    \n",
    "    node_outcomes = [train_data.age[node_indicators[:,x]==1] for x in np.arange(node_indicators.shape[1])]\n",
    "    \n",
    "    # sample values according to a Bayesian bootstrap\n",
    "    \n",
    "    for i in range(number_synthetic_datasets):\n",
    "        \n",
    "        bst_vals = [bayesian_bootstrap(X=np.array(x), \n",
    "                                       statistic=stat,\n",
    "                                       n_replications=1,\n",
    "                                       resample_size=len(x))[0] for x in node_outcomes]\n",
    "        \n",
    "        synth_leaves = cart_age.apply(sXs[i].loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\"]])\n",
    "        \n",
    "        new_age = np.zeros(len(synth_leaves), dtype=int)\n",
    "        \n",
    "        for j, x in enumerate(np.arange(node_indicators.shape[1])):\n",
    "            \n",
    "            new_age[synth_leaves==x] = rng.choice(bst_vals[x], size=np.sum(synth_leaves==x))\n",
    "        \n",
    "        new_age = pd.Series(new_age)\n",
    "        \n",
    "        new_age.name = \"age\"\n",
    "        \n",
    "        sXs[i] = pd.concat([sXs[i], new_age], axis=1)\n",
    "        \n",
    "    ####################################################################################################\n",
    "    \n",
    "    ########### Code for state CART ##########\n",
    "    \n",
    "    cart_state = DecisionTreeClassifier(min_samples_leaf=mb_state_, random_state=rng)\n",
    "    \n",
    "    cart_state.fit(X=train_data.loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\", \"age\"]], y=train_data.loc[:,\"state\"])\n",
    "    \n",
    "    node_indicators = cart_state.decision_path(train_data.loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\", \"age\"]]).toarray()\n",
    "    \n",
    "    node_outcomes = [train_data.state[node_indicators[:,x]==1] for x in np.arange(node_indicators.shape[1])]\n",
    "    \n",
    "    # sample values according to a Bayesian bootstrap\n",
    "    \n",
    "    for i in range(number_synthetic_datasets):\n",
    "        \n",
    "        bst_vals = [bayesian_bootstrap(X=np.array(x), \n",
    "                                       statistic=stat,\n",
    "                                       n_replications=1,\n",
    "                                       resample_size=len(x))[0] for x in node_outcomes]\n",
    "        \n",
    "        synth_leaves = cart_state.apply(sXs[i].loc[:,[\"date\", \"latitude\", \"longitude\", \"sex\", \"age\"]])\n",
    "        \n",
    "        new_state = np.zeros(len(synth_leaves), dtype=int)\n",
    "        \n",
    "        for j, x in enumerate(np.arange(node_indicators.shape[1])):\n",
    "            \n",
    "            new_state[synth_leaves==x] = rng.choice(bst_vals[x], size=np.sum(synth_leaves==x))\n",
    "        \n",
    "        new_state = pd.Series(new_state)\n",
    "        \n",
    "        new_state.name = \"state\"\n",
    "        \n",
    "        sXs[i] = pd.concat([sXs[i], new_state], axis=1)\n",
    "        \n",
    "    ###### Calculate ks distances ######\n",
    "    pmse_ratios = [pmse_ratio(train_data, Y) for Y in sXs]\n",
    "    \n",
    "    return pmse_ratios, sXs, GMM, cart_sex, cart_age, cart_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_models(train_data,\n",
    "                    number_synthetic_datasets,\n",
    "                    number_gmm_initializations,\n",
    "                    random_state):\n",
    "\n",
    "    def evaluate_models(num_components_, mb_sex_, mb_age_, mb_state_):\n",
    "\n",
    "        pmse_ratios, _, _, _, _, _ = train_models(train_data=train_data,\n",
    "                                                  number_synthetic_datasets=number_synthetic_datasets,\n",
    "                                                  number_gmm_initializations=number_gmm_initializations,\n",
    "                                                  num_components_=int(num_components_),\n",
    "                                                  mb_sex_=int(mb_sex_),\n",
    "                                                  mb_age_=int(mb_age_),\n",
    "                                                  mb_state_=int(mb_state_))\n",
    "\n",
    "        return -1 * ((1 - np.mean(pmse_ratios))**2)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=evaluate_models,\n",
    "        pbounds={\n",
    "            \"num_components_\": (200, 800.99),\n",
    "            \"mb_sex_\": (2, 300.99),\n",
    "            \"mb_age_\": (2, 300.99),\n",
    "            \"mb_state_\": (2, 300.99)\n",
    "        },\n",
    "        random_state=random_state)\n",
    "\n",
    "    utility = UtilityFunction(kind=\"ei\", xi=1e-10)\n",
    "    optimizer.maximize(init_points=5, n_iter=25, acquisition_function=utility)\n",
    "    print(\"Final Result: \", optimizer.max)\n",
    "    return optimizer.max, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594760a",
   "metadata": {},
   "source": [
    "The default value of $\\alpha = 1e-06$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd = 20\n",
    "ngi = 5\n",
    "random_states = [np.random.RandomState(1234), np.random.RandomState(4321), np.random.RandomState(10620), np.random.RandomState(91695), np.random.RandomState(31296)]\n",
    "# random_states = [np.random.RandomState(1234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results = [optimize_models(train_data=train_data, number_synthetic_datasets=nsd, number_gmm_initializations=ngi, random_state=r) for r in random_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab452a9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_targets = [np.minimum.accumulate(-i[1].space.target) for i in optimization_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_targets[0])\n",
    "plt.scatter(np.arange(len(run_targets[0])), run_targets[0], s=6)\n",
    "plt.plot(run_targets[1])\n",
    "plt.scatter(np.arange(len(run_targets[1])), run_targets[1], s=6)\n",
    "plt.plot(run_targets[2])\n",
    "plt.scatter(np.arange(len(run_targets[2])), run_targets[2], s=6)\n",
    "plt.plot(run_targets[3])\n",
    "plt.scatter(np.arange(len(run_targets[3])), run_targets[3], s=6)\n",
    "plt.plot(run_targets[4])\n",
    "plt.scatter(np.arange(len(run_targets[4])), run_targets[4], s=6)\n",
    "plt.title(\"Running Minimum Objective Value for CART Synthesis\")\n",
    "plt.ylim(-0.01, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325348ce",
   "metadata": {},
   "source": [
    "Choose the params that gave the best objective value across all random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f179aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optimization_results[np.argmax([x[0]['target'] for x in optimization_results])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c940983",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1ea09",
   "metadata": {},
   "source": [
    "Generate 1000 synthetic datasets, choose the 20 that have the pMSE closest to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmse_ratios, full_sXs, GMM, cart_sex, cart_age, cart_state = train_models(train_data=train_data,\n",
    "                                                                          number_synthetic_datasets=20,\n",
    "                                                                          # hyperparameters for GMM\n",
    "                                                                          number_gmm_initializations=ngi,\n",
    "                                                                          num_components_=int(best_params['params']['num_components_']),\n",
    "                                                                          # hyperparameters for CART, end with underscore means Bayesian optimization will choose\n",
    "                                                                          mb_sex_=int(best_params['params']['mb_sex_']),\n",
    "                                                                          mb_age_=int(best_params['params']['mb_age_']),\n",
    "                                                                          mb_state_=int(best_params['params']['mb_state_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pmse_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(pmse_ratios)\n",
    "plt.xlabel(\"Density\")\n",
    "plt.ylabel(\"pMSE Ratio\")\n",
    "plt.title(\"Distribution of pMSE Ratios\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af137729",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4b464",
   "metadata": {},
   "source": [
    "# Save the synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sX in enumerate(full_sXs):\n",
    "    sX.to_csv(\"../Data/synthetic_datasets/cart_mb_logistic_pmse_\" + str(i) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af303b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sXs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ffaea",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f13472",
   "metadata": {},
   "source": [
    "Produce inference reduced data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc36492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_reduction(original_data, synthetic_data, mixture_model, deltas, c, prior_prob):\n",
    "    \n",
    "    # number of original records\n",
    "    num_records = original_data.shape[0]\n",
    "    \n",
    "    # record percentages\n",
    "    print_nums = [int(np.ceil(i*num_records)) for i in [0.25, 0.5, 0.75]]\n",
    "    \n",
    "    # random number generator\n",
    "    rng = default_rng()\n",
    "    \n",
    "    # copy the synthetic dataset\n",
    "    new_sX = synthetic_data\n",
    "    \n",
    "    # tree for synthetic locations\n",
    "    sX_tree = cKDTree(new_sX[[\"latitude\", \"longitude\"]])\n",
    "    \n",
    "    # store mixture component parameters\n",
    "    mus = mixture_model.means_\n",
    "    sigmas = mixture_model.covariances_\n",
    "    \n",
    "    # temporary count of the number of rows that violate one or more conditions\n",
    "    violator_count = num_records\n",
    "    \n",
    "    # number of anonymization loops required\n",
    "    num_loops = 1\n",
    "    \n",
    "    # while we have any violator rows\n",
    "    while violator_count > 0:\n",
    "        \n",
    "        # reset violator count\n",
    "        violator_count = 0\n",
    "        \n",
    "        # for each original record\n",
    "        # we shuffle the records each time so that the violating records are fixed in a random order\n",
    "        for i, original_record in original_data.sample(frac=1.0).reset_index(drop=True).iterrows():\n",
    "            \n",
    "            original_location = original_record.loc[[\"latitude\", \"longitude\"]]\n",
    "            original_categorical = original_record.loc[[\"sex\", \"age\", \"state\"]]\n",
    "            \n",
    "            # for each delta\n",
    "            for delta in deltas:\n",
    "                    \n",
    "                ##### Test the Inference Criterion\n",
    "                \n",
    "                # find synthetic neighbors based on location\n",
    "                location_neighbors = sX_tree.query_ball_point(original_location, r=delta, p=2.0)\n",
    "                \n",
    "                # matches on categorical attributes from location neighbors\n",
    "                categorical_matches = (new_sX.loc[location_neighbors,['sex', 'age']] == original_categorical[[\"sex\", \"age\"]]).all(1)\n",
    "                \n",
    "                matching_rows = new_sX.loc[location_neighbors,:].loc[categorical_matches.values,:]\n",
    "                \n",
    "                # if there are any records in the location neighborhood that match on sex and age\n",
    "                \n",
    "                if matching_rows.shape[0] > 0:\n",
    "                \n",
    "                    if original_categorical['state'] == 1.0:\n",
    "                        prior = prior_prob\n",
    "                    else:\n",
    "                        prior = 1-prior_prob\n",
    "                        \n",
    "                    num_matching = np.sum(matching_rows['state'] == original_categorical['state'])\n",
    "            \n",
    "                    cond = num_matching/matching_rows.shape[0] * 1/prior\n",
    "                \n",
    "                    if cond > c:\n",
    "                        \n",
    "                        # add one to number of violators\n",
    "                        violator_count += 1\n",
    "                        \n",
    "                        # number of records with non-matching sensitive variable needed to meet inference\n",
    "                        num_needed = int(np.ceil(num_matching/(prior*c) - matching_rows.shape[0]))\n",
    "                        \n",
    "                        # find the component with the highest responsibility for the confidential record\n",
    "                        component_index = np.argmax(GMM.predict_proba(pd.DataFrame(original_location).T), axis = 1)[0]\n",
    "                        current_mu = mus[component_index,:]\n",
    "                        current_sigma = sigmas[component_index,:,:]\n",
    "            \n",
    "                        valid_candidates = np.zeros((0,2))\n",
    "                        num_candidate_loops = 0\n",
    "                        while valid_candidates.shape[0] < num_needed:\n",
    "                        \n",
    "                            # add a very small number to the diagonal of the covariance matrix to increase\n",
    "                            # the likelihood of sampling candidate points - do this proportionally to the number\n",
    "                            # of times this loop has executed\n",
    "                            # np.fill_diagonal(current_sigma, np.diag(current_sigma) + num_candidate_loops*1e-6)\n",
    "                        \n",
    "                            # generate a bunch of candidate points\n",
    "                            candidate_points = rng.multivariate_normal(current_mu, current_sigma, size=100000)\n",
    "                            candidate_tree = cKDTree(candidate_points)\n",
    "                            valid_indices = candidate_tree.query_ball_point(original_location, delta, p=2.0, return_sorted=True)\n",
    "                            valid_candidates = np.vstack([valid_candidates, candidate_points[valid_indices,:]])\n",
    "                            num_candidate_loops += 1\n",
    "                            if num_candidate_loops > 100:\n",
    "                                print('Stuck in inference loop.')\n",
    "                            \n",
    "                        # print(num_candidate_loops)\n",
    "                    \n",
    "                        # select the number of needed candidates\n",
    "                        new_locations = valid_candidates[:num_needed,:]\n",
    "                    \n",
    "                        new_categorical = np.vstack([np.array(original_categorical).reshape(1,-1) for k in range(num_needed)])\n",
    "                    \n",
    "                        new_records = pd.DataFrame(np.hstack([new_locations, new_categorical]))\n",
    "                    \n",
    "                        new_records.columns = new_sX.columns\n",
    "                        \n",
    "                        new_records['state'] = 1.0 - original_categorical['state']\n",
    "                    \n",
    "                        new_sX = pd.concat([new_sX, new_records], axis=0).reset_index(drop=True)\n",
    "                    \n",
    "                        # rebuild the tree for synthetic locations\n",
    "                        sX_tree = cKDTree(new_sX[[\"latitude\", \"longitude\"]])\n",
    "                    \n",
    "#                     # put in a break statement - if we executed the previous code conditional on the if statement,\n",
    "#                     # we know the linkability criterion is satisfied for all larger delta values for this record\n",
    "#                     break\n",
    "    \n",
    "            if int(i) in print_nums:\n",
    "                print(\"Record \" + str(i) + \" completed.\")\n",
    "                \n",
    "        print(\"Full anonymization loop \" + str(num_loops) + \" completed.\")\n",
    "        \n",
    "        num_loops += 1\n",
    "                    \n",
    "    return new_sX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0405ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_death_prob = 152/9583\n",
    "delta_vals = np.linspace(0.00001, 1.0, 20)\n",
    "c=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f29663",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sXs = [inference_reduction(train_data, X, GMM, delta_vals, c, prior_death_prob) for X in full_sXs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in inference_sXs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee625db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sX in enumerate(inference_sXs):\n",
    "    sX.to_csv(\"Data/synthetic_datasets/cart_mb_inf_logistic_pmse_\" + str(i) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cafbdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
