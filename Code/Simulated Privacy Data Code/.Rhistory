# Variable 6 - Amount Spent -----------------------------------------------
# A Partial Function of all 5 previous variables
amount_spent <- c()
# region_probs <- c(
#   400, 120, 110, 100, 50, 220, 450
# )
for (i in 1:num_cust) {
abs_age <- abs(age[i] - 46)
age_impact <- abs_age/age[i]
amount_spent[i] <- 600 +
age_impact * -600 +
# region_probs[which(region_options == region[i])] +
num_visits[i] * 4 +
200 * hiking_int[i] +
400 * sustain_int[i] +
100 * online_int[i] +
rnorm(1, mean = 0, sd = 50)
}
for(i in 1:num_cust) {
amount_spent[i] <- ifelse(amount_spent[i] < 0, 0, amount_spent[i])
}
## Check Distribution
tibble(amount_spent = amount_spent) |>
ggplot(mapping = aes(x = amount_spent)) +
geom_density()
# Variable 7 - Churn ------------------------------------------------------
# A Function of All 6 Variables
# Create Churn first as a linear equation with different weights
# region_probs <- c(
#   4, 1, .5, .5, 1, 3, 3
# )
churn <- c()
for (i in 1:num_cust) {
abs_age <- abs(age[i] - 25)
age_impact <- abs_age/age[i]
churn[i] <- age_impact * -5 +
# region_probs[which(region_options == region[i])] +
(num_visits[i] / -30) +
6 * hiking_int[i] +
10 * sustain_int[i] +
4 * online_int[i] +
rnorm(1, mean = 0, sd = 2.5)
}
## Check Distribution
tibble(churn = churn) |>
ggplot(mapping = aes(x = churn)) +
geom_density()
churn_binomial <- ifelse(churn < median(churn), 1, 0)
## Create a Dataframe
simulated_data <- tibble(
churn = churn_binomial,
amount_spent = amount_spent,
num_visits = num_visits,
age = age,
hiking_int = hiking_int,
sustain_int = sustain_int,
online_int = online_int,
id = customer_id
)
# save simulated data
path_to_save <- "../../Data/Simulations/"
if (!dir.exists(path_to_save)){
dir.create(path_to_save, recursive=TRUE)
write.csv(simulated_data, paste0(path_to_save, "churn_simulated.csv"), row.names=FALSE)
} else {
write.csv(simulated_data, paste0(path_to_save, "churn_simulated.csv"), row.names=FALSE)
}
setwd("~/Repos/optimized-sequential-synthesis/Code/Simulated Privacy Data Code")
setwd("~/Repos/optimized-sequential-synthesis/Code/Simulated Privacy Data Code")
library(tidyverse)
library(tidymodels)
set.seed(100)
original_data <- read_csv("../../Data/Simulations/Churn/churn_simulated.csv")
synthesized_data <- read_csv("../../Data/Simulations/Churn/mnl_0.csv")
synthesized_data
# Prep
synthesized_data <- synthesized_data |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
synthesized_data
# summary statistics
summary(synthesized_data)
# Prep Everything - Simulated
split <- initial_split(original_data, prop = .9)
training <- training(split)
testing <- testing(split)
recipe_partial <- training |>
recipe(churn ~ amount_spent + age + num_visits) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
recipe_full <- training |>
recipe(churn ~ amount_spent + age + num_visits +
hiking_int + sustain_int + online_int) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
# v-fold
training_cv <- vfold_cv(training, v = 10, strata = churn)
# Prep Everything - Synthesized
split_synth <- initial_split(synthesized_data, prop = .9)
training_synth <- training(split_synth)
testing_synth <- testing(split_synth)
# v-fold
training_cv_synth <- vfold_cv(training_synth, v = 10, strata = churn)
# Partial Fit: Company has data for Age, Region, and Amount Spent ---------
workflow_partial <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_partial)
cv_results_partial <- workflow_partial |>
fit_resamples(
resamples = training_cv
)
# summary statistics
summary(synthesized_data)
original_data
set.seed(100)
original_data <- read_csv("../../Data/Simulations/Churn/churn_simulated.csv")
synthesized_data <- read_csv("../../Data/Simulations/Churn/mnl_0.csv")
# Prep
original_data <- original_data |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# Prep
synthesized_data <- synthesized_data |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# summary statistics
summary(synthesized_data)
# Prep Everything - Simulated
split <- initial_split(original_data, prop = .9)
training <- training(split)
testing <- testing(split)
recipe_partial <- training |>
recipe(churn ~ amount_spent + age + num_visits) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
recipe_full <- training |>
recipe(churn ~ amount_spent + age + num_visits +
hiking_int + sustain_int + online_int) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
# v-fold
training_cv <- vfold_cv(training, v = 10, strata = churn)
# Prep Everything - Synthesized
split_synth <- initial_split(synthesized_data, prop = .9)
training_synth <- training(split_synth)
testing_synth <- testing(split_synth)
# v-fold
training_cv_synth <- vfold_cv(training_synth, v = 10, strata = churn)
# Partial Fit: Company has data for Age, Region, and Amount Spent ---------
workflow_partial <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_partial)
cv_results_partial <- workflow_partial |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_partial)
# Synthesized Fit: Company has Synthesized Data ---------------------------
workflow_synth <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_synth <- workflow_synth |>
fit_resamples(
resamples = training_cv_synth
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_synth)
# Complete Fit: Company has data for all ----------------------------------
workflow_full <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_full <- workflow_full |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_full)
?initial_split
library(tidyverse)
library(tidymodels)
set.seed(100)
# source("Simulate Data - 6 Variables + Churn.R")
original_data <- read_csv("../../Data/Simulations/Churn/churn_simulated.csv")
synthesized_data <- read_csv("../../Data/Simulations/Churn/mnl_0.csv")
# Prep
original_data <- original_data |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# Prep
synthesized_data <- synthesized_data |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# summary statistics
summary(synthesized_data)
# Prep Everything - Simulated
split <- initial_split(original_data, prop = .9)
training <- training(split)
testing <- testing(split)
recipe_partial <- training |>
recipe(churn ~ amount_spent + age + num_visits) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
recipe_full <- training |>
recipe(churn ~ amount_spent + age + num_visits +
hiking_int + sustain_int + online_int) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
# v-fold
training_cv <- vfold_cv(training, v = 10, strata = churn)
# Prep Everything - Synthesized
split_synth <- initial_split(synthesized_data, prop = .9)
training_synth <- training(split_synth)
testing_synth <- testing(split_synth)
# v-fold
training_cv_synth <- vfold_cv(training_synth, v = 10, strata = churn)
# Partial Fit: Company has data for Age, Region, and Amount Spent ---------
workflow_partial <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_partial)
cv_results_partial <- workflow_partial |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_partial)
# Synthesized Fit: Company has Synthesized Data ---------------------------
workflow_synth <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_synth <- workflow_synth |>
fit_resamples(
resamples = training_cv_synth
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_synth)
# Complete Fit: Company has data for all ----------------------------------
workflow_full <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_full <- workflow_full |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_full)
# Compare model coefficients
tidy(cv_results_full)
cv_results_full
cv_results_full |>
extract_fit_engine()
logistic_reg() |>
fit(churn ~ ., data = original_data)
logistic_reg() |>
fit(churn ~ ., data = original_data) |>
tidy()
logistic_reg() |>
fit(churn ~ ., data = synthesized_data) |>
tidy()
library(tidyverse)
library(tidymodels)
set.seed(100)
# source("Simulate Data - 6 Variables + Churn.R")
original_data <- read_csv("../../Data/Simulations/Churn/churn_simulated.csv")
synthesized_data <- read_csv("../../Data/Simulations/Churn/mnl_0.csv")
# Prep
original_data <- original_data |>
select(-id) |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# Prep
synthesized_data <- synthesized_data |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# summary statistics
summary(synthesized_data)
# Prep Everything - Simulated
split <- initial_split(original_data, prop = .9)
training <- training(split)
testing <- testing(split)
recipe_partial <- training |>
recipe(churn ~ amount_spent + age + num_visits) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
recipe_full <- training |>
recipe(churn ~ amount_spent + age + num_visits +
hiking_int + sustain_int + online_int) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
# v-fold
training_cv <- vfold_cv(training, v = 10, strata = churn)
# Prep Everything - Synthesized
split_synth <- initial_split(synthesized_data, prop = .9)
training_synth <- training(split_synth)
testing_synth <- testing(split_synth)
# v-fold
training_cv_synth <- vfold_cv(training_synth, v = 10, strata = churn)
# Partial Fit: Company has data for Age, Region, and Amount Spent ---------
workflow_partial <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_partial)
cv_results_partial <- workflow_partial |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_partial)
# Synthesized Fit: Company has Synthesized Data ---------------------------
workflow_synth <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_synth <- workflow_synth |>
fit_resamples(
resamples = training_cv_synth
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_synth)
# Complete Fit: Company has data for all ----------------------------------
workflow_full <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_full <- workflow_full |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_full)
# examine model coefficients ----------------------------------------------
logistic_reg() |>
fit(churn ~ ., data = original_data) |>
tidy()
logistic_reg() |>
fit(churn ~ ., data = synthesized_data) |>
tidy()
library(tidyverse)
library(tidymodels)
set.seed(100)
# source("Simulate Data - 6 Variables + Churn.R")
original_data <- read_csv("../../Data/Simulations/Churn/churn_simulated.csv")
synthesized_data <- read_csv("../../Data/Simulations/Churn/mnl_0.csv")
# Prep
original_data <- original_data |>
select(-id) |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# Prep
synthesized_data <- synthesized_data |>
mutate(churn = as.factor(churn),
hiking_int = as.factor(hiking_int),
sustain_int = as.factor(sustain_int),
online_int = as.factor(online_int))
# summary statistics
summary(synthesized_data)
# Prep Everything - Simulated
split <- initial_split(original_data, prop = .9)
training <- training(split)
testing <- testing(split)
recipe_partial <- training |>
recipe(churn ~ amount_spent + age + num_visits) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
recipe_full <- training |>
recipe(churn ~ amount_spent + age + num_visits +
hiking_int + sustain_int + online_int) |>
step_log(c("amount_spent", "age", "num_visits"), offset = 1)
# v-fold
training_cv <- vfold_cv(training, v = 10, strata = churn)
# Prep Everything - Synthesized
split_synth <- initial_split(synthesized_data, prop = .9)
training_synth <- training(split_synth)
testing_synth <- testing(split_synth)
# v-fold
training_cv_synth <- vfold_cv(training_synth, v = 10, strata = churn)
# Partial Fit: Company has data for Age, Region, and Amount Spent ---------
workflow_partial <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_partial)
cv_results_partial <- workflow_partial |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_partial)
# Synthesized Fit: Company has Synthesized Data ---------------------------
workflow_synth <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_synth <- workflow_synth |>
fit_resamples(
resamples = training_cv_synth
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_synth)
# Complete Fit: Company has data for all ----------------------------------
workflow_full <- workflow() |>
add_model(logistic_reg()) |>
add_recipe(recipe_full)
cv_results_full <- workflow_full |>
fit_resamples(
resamples = training_cv
)
# Compute model accuracy - Threshold at .5
collect_metrics(cv_results_full)
# examine model coefficients ----------------------------------------------
logistic_reg() |>
fit(churn ~ ., data = original_data) |>
tidy()
logistic_reg() |>
fit(churn ~ ., data = synthesized_data) |>
tidy()
full_model_results <- logistic_reg() |>
fit(churn ~ ., data = original_data) |>
tidy()
full_synth_model_results <- logistic_reg() |>
fit(churn ~ ., data = synthesized_data) |>
tidy()
full_model_results <- logistic_reg() |>
fit(churn ~ ., data = original_data) |>
tidy(conf.int = TRUE)
full_synth_model_results <- logistic_reg() |>
fit(churn ~ ., data = synthesized_data) |>
tidy(conf.int = TRUE)
full_synth_model_results
# Compare parameter estimates.
full_model_results |>
ggplot(aes(y = term)) +
geom_point(aes(x = estimate)) +
geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .1) +
geom_vline(xintercept = 0, color = "red")
full_model_results <- logistic_reg() |>
fit(churn ~ ., data = original_data) |>
tidy(conf.int = TRUE) |>
mutate(Type = "Original")
full_synth_model_results <- logistic_reg() |>
fit(churn ~ ., data = synthesized_data) |>
tidy(conf.int = TRUE) |>
mutate(Type = "Synthetic")
# Compare parameter estimates.
full_model_results |>
bind_rows(full_synth_model_results) |>
ggplot(aes(y = term, color = Type)) +
geom_point(aes(x = estimate)) +
geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .1) +
geom_vline(xintercept = 0, color = "red")
# Compare parameter estimates.
full_model_results |>
bind_rows(full_synth_model_results) |>
ggplot(aes(y = term, color = Type)) +
geom_point(aes(x = estimate)) +
geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .1) +
geom_vline(xintercept = 0, color = "red") +
labs(x = "Parameter Estimate",
y = "Variable",
color = "Data Type",
title = "Coefficient Comparison - Original vs. Synthetic Data")
install.packages('DPpack')
library(DPpack)
?LogisticRegressionDP
lrdp <- LogisticRegressionDP$new(eps=5)
lrdp <- LogisticRegressionDP$new(regularizer = function(coeff) coeff%*%coeff/2,
regularizer.gr = coeff,
gamma = 0,
eps = 5)
lrdp <- LogisticRegressionDP$new(regularizer = function(coeff) coeff%*%coeff/2,
regularizer.gr = function(coeff) coeff,
gamma = 0,
eps = 5)
max(original_data$amount_spent)
original_data
original_data[, 2:ncol(original_data)]
dp_Y <- original_data[, 1]
original_data[, 1]
reg_func_grad <- function(coeff) coeff
# define upper and lower bounds for X variables
upper_bounds <- c(max(original_data$amount_spent),
max(original_data$num_visits),
max(original_data$age),
1,
1,
1)
lower_bounds <- c(0, 0, 0, 0, 0, 0)
dp_X <- original_data[, 2:ncol(original_data)]
dp_Y <- original_data[, 1]
lrdp <- LogisticRegressionDP$new(regularizer = reg_func,
regularizer.gr = reg_func_grad,
gamma = 0,
eps = 5)
# define regularization function and constant
reg_func <- function(coeff) coeff%*%coeff/2
reg_func_grad <- function(coeff) coeff
# define upper and lower bounds for X variables
upper_bounds <- c(max(original_data$amount_spent),
max(original_data$num_visits),
max(original_data$age),
1,
1,
1)
lower_bounds <- c(0, 0, 0, 0, 0, 0)
dp_X <- original_data[, 2:ncol(original_data)]
dp_Y <- original_data[, 1]
lrdp <- LogisticRegressionDP$new(regularizer = reg_func,
regularizer.gr = reg_func_grad,
gamma = 0,
eps = 5)
lrdp$fit(dp_X, dp_Y, upper_bounds, lower_bounds)
