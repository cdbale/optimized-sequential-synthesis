{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f2ea60",
   "metadata": {},
   "source": [
    "# Synthesize Criteo data using the DP-GAN approach of Ponte. et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e4765",
   "metadata": {},
   "source": [
    "Note that we use the following python modules:\n",
    "\n",
    "- tensorflow==2.15.0\n",
    "- keras==2.15.0\n",
    "- tensorflow-estimator==2.15.0\n",
    "- tensorflow-privacy==0.9.0\n",
    "- numpy==1.26.4\n",
    "- pandas==2.2.2\n",
    "- scikit-learn==1.4.2\n",
    "- scipy==1.11.4\n",
    "- absl-py==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf11bfe",
   "metadata": {},
   "source": [
    "The following statments can be used to install the required python modules:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow==2.15.0\n",
    "pip install keras==2.15.0\n",
    "pip install tensorflow-estimator==2.15.0\n",
    "pip install tensorflow-privacy==0.9.0\n",
    "pip install numpy==1.26.4\n",
    "pip install pandas==2.2.2\n",
    "pip install scikit-learn==1.4.2\n",
    "pip install scipy==1.11.4\n",
    "pip install absl-py==1.4.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244eec7",
   "metadata": {},
   "source": [
    "Perform a quick check that `tensorflow`, `keras` and `tensorflow_privacy` are installed and importable. Also check versions of `NumPy`, `Pandas`, and `Scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3208e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "TF: 2.15.0\n",
      "Keras: 2.15.0\n",
      "NumPy: 1.26.4\n",
      "Pandas: 2.2.2\n",
      "Sklearn: 1.4.2\n",
      "DP optimizer OK\n"
     ]
    }
   ],
   "source": [
    "# sanity check the environment\n",
    "import tensorflow as tf, keras, numpy as np, pandas as pd, sklearn\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "\n",
    "print(\"TF:\", tf.__version__)              # 2.15.0\n",
    "print(\"Keras:\", keras.__version__)        # 2.15.0\n",
    "print(\"NumPy:\", np.__version__)           # 1.26.4\n",
    "print(\"Pandas:\", pd.__version__)          # 2.2.2\n",
    "print(\"Sklearn:\", sklearn.__version__)    # 1.4.2\n",
    "_ = DPKerasAdamOptimizer(l2_norm_clip=1.0, noise_multiplier=0.5,\n",
    "                         num_microbatches=1, learning_rate=1e-3)\n",
    "print(\"DP optimizer OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeba3a8",
   "metadata": {},
   "source": [
    "Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa944af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import io\n",
    "from keras.models import load_model\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, LeakyReLU\n",
    "from keras.layers import UpSampling2D, Conv2D, Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from keras.models import load_model\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b703917",
   "metadata": {},
   "source": [
    "Import Criteo data (small version is for testing, results are based on 'full' version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c928f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(\"../../Data/Criteo/cleaned_criteo_small.gz\",\n",
    "#                          compression='gzip', \n",
    "#                          sep='\\,',\n",
    "#                          header=0,\n",
    "#                          engine='python')\n",
    "# data_set = \"small\"\n",
    "\n",
    "train_data = pd.read_csv(\"../../Data/Criteo/cleaned_criteo.gz\",\n",
    "                         compression='gzip', \n",
    "                         sep='\\,',\n",
    "                         header=0,\n",
    "                         engine='python')\n",
    "data_set = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500371e",
   "metadata": {},
   "source": [
    "View confidential data to synthesize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437e8f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>treatment</th>\n",
       "      <th>conversion</th>\n",
       "      <th>visit</th>\n",
       "      <th>exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.976429</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.955396</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>9.002689</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.955396</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.964775</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.955396</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>9.002801</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.955396</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>9.037999</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.955396</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13979587</th>\n",
       "      <td>26.297764</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>9.006250</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-3.282109</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.839578</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13979588</th>\n",
       "      <td>12.642207</td>\n",
       "      <td>10.679513</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>-1.700105</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>3.013064</td>\n",
       "      <td>-13.955150</td>\n",
       "      <td>6.269026</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13979589</th>\n",
       "      <td>12.976557</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.381868</td>\n",
       "      <td>0.842442</td>\n",
       "      <td>11.029584</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-8.281971</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.779212</td>\n",
       "      <td>23.570168</td>\n",
       "      <td>6.169187</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13979590</th>\n",
       "      <td>24.805064</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-1.288207</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13979591</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>3.013064</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>9.332563</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13979592 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 f0         f1        f2  ...  conversion  visit  exposure\n",
       "0         12.616365  10.059654  8.976429  ...           0      0         0\n",
       "1         12.616365  10.059654  9.002689  ...           0      0         0\n",
       "2         12.616365  10.059654  8.964775  ...           0      0         0\n",
       "3         12.616365  10.059654  9.002801  ...           0      0         0\n",
       "4         12.616365  10.059654  9.037999  ...           0      0         0\n",
       "...             ...        ...       ...  ...         ...    ...       ...\n",
       "13979587  26.297764  10.059654  9.006250  ...           0      0         0\n",
       "13979588  12.642207  10.679513  8.214383  ...           0      0         1\n",
       "13979589  12.976557  10.059654  8.381868  ...           0      1         0\n",
       "13979590  24.805064  10.059654  8.214383  ...           0      0         0\n",
       "13979591  12.616365  10.059654  8.214383  ...           0      0         0\n",
       "\n",
       "[13979592 rows x 16 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f5689",
   "metadata": {},
   "source": [
    "Define a class for estimating a differentially private GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679beec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# GANs with differential privacy\"\"\"\n",
    "class GAN():\n",
    "    def __init__(self, privacy):\n",
    "      self.img_rows = 1\n",
    "      self.img_cols = 16\n",
    "      self.img_shape = (self.img_cols,)\n",
    "      self.latent_dim = (16)\n",
    "      lr = 0.001\n",
    "\n",
    "      optimizer = keras.optimizers.Adam()\n",
    "      self.discriminator = self.build_discriminator()\n",
    "      self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                 optimizer=optimizer,\n",
    "                                 metrics=['accuracy'])\n",
    "      if privacy == True:\n",
    "        # print(noise_multiplier)\n",
    "        # print(\"using differential privacy\")\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(optimizer=DPKerasAdamOptimizer(\n",
    "            l2_norm_clip=4,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=lr),\n",
    "            loss= tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE), metrics=['accuracy'])\n",
    "\n",
    "      # Build the generator\n",
    "      self.generator = self.build_generator()\n",
    "\n",
    "      # The generator takes noise as input and generates imgs\n",
    "      z = Input(shape=(self.latent_dim,))\n",
    "      img = self.generator(z)\n",
    "\n",
    "      # For the combined model we will only train the generator\n",
    "      self.discriminator.trainable = False\n",
    "\n",
    "      # The discriminator takes generated images as input and determines validity\n",
    "      valid = self.discriminator(img)\n",
    "\n",
    "      # The combined model  (stacked generator and discriminator)\n",
    "      # Trains the generator to fool the discriminator\n",
    "      self.combined = Model(z, valid)\n",
    "      self.combined.compile(loss='binary_crossentropy', optimizer= optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "      model = Sequential()\n",
    "      model.add(Dense(self.latent_dim, input_dim=self.latent_dim))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(64, input_shape=self.img_shape))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(self.latent_dim))\n",
    "      model.add(Activation(\"tanh\"))\n",
    "\n",
    "      #model.summary()\n",
    "\n",
    "      noise = Input(shape=(self.latent_dim,))\n",
    "      img = model(noise)\n",
    "      return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, data, iterations, batch_size, sample_interval, model_name, generator_losses = [], discriminator_acc = [], correlations = [], accuracy = [], MAPD_collect = [],MSE_collect = [], MAE_collect = []):\n",
    "      # Adversarial ground truths\n",
    "      valid = np.ones((batch_size, 1))\n",
    "      fake = np.zeros((batch_size, 1))\n",
    "      corr = 0\n",
    "      MAPD = 0\n",
    "      MSE = 0\n",
    "      MAE = 0\n",
    "      #fake += 0.05 * np.random.random(fake.shape)\n",
    "      #valid += 0.05 * np.random.random(valid.shape)\n",
    "\n",
    "      for epoch in range(iterations):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            imgs = data[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise, verbose = False)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            if (epoch % 100) == 0:\n",
    "              print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "      self.generator.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743629d",
   "metadata": {},
   "source": [
    "Set number of samples (total number of observations). Determine epochs as a function of batch size, which we leave fixed at 100 (same as Ponte et al.), and scale iterations to have `epochs = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0ca59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "samples = int(train_data.shape[0])\n",
    "\n",
    "# setting epsilon\n",
    "N = len(train_data)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b31a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### change for different data sizes\n",
    "iterations = 10000\n",
    "epochs = iterations/(N/batch_size) # should be 10\n",
    "num_microbatches = batch_size # see validation section paper.\n",
    "\n",
    "# the noise_multiplier is not directly passed to the GAN, but the GAN code reads it from the global environment\n",
    "l2_norm_clip = 4 # see paper in validation section.\n",
    "delta = 1/N # should be 1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1a6807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07153284587990837"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557008df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of different noise multipliers to use for synthesis\n",
    "# noise multipliers for smaller criteo data\n",
    "# noise_multipliers = [0.419205, 0.6227, 0.91265, 1.2243, 6.5]\n",
    "# noise multipliers for full Criteo data\n",
    "noise_multipliers = [0.281585, 0.49147, 0.7668, 1.045, 3.2849]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee19e6b",
   "metadata": {},
   "source": [
    "Choose noise multipliers that map to $\\epsilon = 13, 3, 1, 0.5, 0.05$. The `tensorflow-privacy` package has deprecated the use of the `compute_dp_sgd_privacy` function, replacing it with `compute_dp_sgd_privacy_statement` which properly accounts for doubling sensitivity due to microbatching and does not assume Poisson subsampling. However, we use the existing methods from Ponte et al. for consistency, and note that the theoretical epsilon is higher than what is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52bbd953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n",
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n",
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n",
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n",
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.0, 3.0, 1.0, 0.5, 0.05]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the theoretical bound of epsilon\n",
    "[np.round(compute_dp_sgd_privacy(n = N, \n",
    "                                 batch_size = batch_size,\n",
    "                                 epochs = epochs,\n",
    "                                 noise_multiplier = x,\n",
    "                                 delta = delta)[0], 3) for x in noise_multipliers] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9dfed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "WARNING:tensorflow:From C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.717697, acc.: 48.00%] [G loss: 0.799186]\n",
      "100 [D loss: 0.679923, acc.: 48.50%] [G loss: 0.635298]\n",
      "200 [D loss: 0.600365, acc.: 80.50%] [G loss: 0.845760]\n",
      "300 [D loss: 0.485292, acc.: 88.00%] [G loss: 1.131266]\n",
      "400 [D loss: 0.708448, acc.: 51.50%] [G loss: 0.723726]\n",
      "500 [D loss: 0.545595, acc.: 78.00%] [G loss: 0.807405]\n",
      "600 [D loss: 0.429261, acc.: 92.00%] [G loss: 1.249392]\n",
      "700 [D loss: 0.731408, acc.: 48.00%] [G loss: 0.724719]\n",
      "800 [D loss: 0.726296, acc.: 56.00%] [G loss: 0.804484]\n",
      "900 [D loss: 0.678559, acc.: 52.50%] [G loss: 0.710721]\n",
      "1000 [D loss: 0.766765, acc.: 42.00%] [G loss: 0.644769]\n",
      "1100 [D loss: 0.758578, acc.: 50.00%] [G loss: 0.704206]\n",
      "1200 [D loss: 0.696809, acc.: 49.00%] [G loss: 0.786539]\n",
      "1300 [D loss: 0.653385, acc.: 59.00%] [G loss: 0.767821]\n",
      "1400 [D loss: 0.487984, acc.: 92.50%] [G loss: 0.868770]\n",
      "1500 [D loss: 0.838662, acc.: 21.00%] [G loss: 0.600395]\n",
      "1600 [D loss: 0.533594, acc.: 78.00%] [G loss: 0.992828]\n",
      "1700 [D loss: 0.769054, acc.: 49.00%] [G loss: 0.747299]\n",
      "1800 [D loss: 0.690456, acc.: 61.00%] [G loss: 0.702443]\n",
      "1900 [D loss: 0.600874, acc.: 79.50%] [G loss: 0.842692]\n",
      "2000 [D loss: 0.699641, acc.: 49.00%] [G loss: 0.761117]\n",
      "2100 [D loss: 0.595723, acc.: 72.50%] [G loss: 0.973029]\n",
      "2200 [D loss: 0.659448, acc.: 61.50%] [G loss: 0.889145]\n",
      "2300 [D loss: 0.309268, acc.: 93.50%] [G loss: 1.519360]\n",
      "2400 [D loss: 0.690365, acc.: 70.50%] [G loss: 0.987688]\n",
      "2500 [D loss: 0.723713, acc.: 57.00%] [G loss: 0.993211]\n",
      "2600 [D loss: 0.603249, acc.: 78.00%] [G loss: 0.977227]\n",
      "2700 [D loss: 0.683673, acc.: 73.50%] [G loss: 1.039953]\n",
      "2800 [D loss: 0.942036, acc.: 42.00%] [G loss: 0.501823]\n",
      "2900 [D loss: 0.527146, acc.: 73.00%] [G loss: 0.996489]\n",
      "3000 [D loss: 0.661730, acc.: 69.00%] [G loss: 0.801262]\n",
      "3100 [D loss: 0.619074, acc.: 70.00%] [G loss: 0.988508]\n",
      "3200 [D loss: 0.691540, acc.: 50.00%] [G loss: 0.778825]\n",
      "3300 [D loss: 0.879503, acc.: 47.50%] [G loss: 0.633322]\n",
      "3400 [D loss: 0.661093, acc.: 65.50%] [G loss: 1.041259]\n",
      "3500 [D loss: 0.783248, acc.: 46.50%] [G loss: 0.760221]\n",
      "3600 [D loss: 0.603279, acc.: 76.00%] [G loss: 0.911331]\n",
      "3700 [D loss: 0.595115, acc.: 76.00%] [G loss: 0.883299]\n",
      "3800 [D loss: 0.644781, acc.: 61.00%] [G loss: 0.713329]\n",
      "3900 [D loss: 0.624690, acc.: 59.00%] [G loss: 0.931338]\n",
      "4000 [D loss: 0.821671, acc.: 46.00%] [G loss: 0.553088]\n",
      "4100 [D loss: 0.723666, acc.: 61.00%] [G loss: 0.885393]\n",
      "4200 [D loss: 0.762476, acc.: 39.50%] [G loss: 0.742808]\n",
      "4300 [D loss: 0.725989, acc.: 55.00%] [G loss: 0.744546]\n",
      "4400 [D loss: 0.588965, acc.: 75.50%] [G loss: 0.971436]\n",
      "4500 [D loss: 0.735361, acc.: 50.50%] [G loss: 0.826188]\n",
      "4600 [D loss: 0.569663, acc.: 76.00%] [G loss: 0.845888]\n",
      "4700 [D loss: 0.729227, acc.: 42.50%] [G loss: 0.771514]\n",
      "4800 [D loss: 0.609302, acc.: 71.00%] [G loss: 0.820187]\n",
      "4900 [D loss: 0.545118, acc.: 76.00%] [G loss: 1.107805]\n",
      "5000 [D loss: 0.401983, acc.: 84.50%] [G loss: 1.351093]\n",
      "5100 [D loss: 0.857716, acc.: 46.50%] [G loss: 0.965711]\n",
      "5200 [D loss: 0.692243, acc.: 54.00%] [G loss: 0.926671]\n",
      "5300 [D loss: 0.550815, acc.: 74.50%] [G loss: 0.961367]\n",
      "5400 [D loss: 0.699792, acc.: 59.00%] [G loss: 0.756295]\n",
      "5500 [D loss: 0.639881, acc.: 70.00%] [G loss: 0.864320]\n",
      "5600 [D loss: 0.719950, acc.: 59.00%] [G loss: 0.927007]\n",
      "5700 [D loss: 0.594950, acc.: 66.50%] [G loss: 0.998893]\n",
      "5800 [D loss: 0.673630, acc.: 59.50%] [G loss: 0.934327]\n",
      "5900 [D loss: 0.585169, acc.: 71.50%] [G loss: 1.014042]\n",
      "6000 [D loss: 0.687134, acc.: 50.00%] [G loss: 0.683722]\n",
      "6100 [D loss: 0.593002, acc.: 64.00%] [G loss: 1.018985]\n",
      "6200 [D loss: 0.521945, acc.: 85.50%] [G loss: 1.032946]\n",
      "6300 [D loss: 0.712714, acc.: 60.50%] [G loss: 0.944369]\n",
      "6400 [D loss: 0.636084, acc.: 59.00%] [G loss: 0.941353]\n",
      "6500 [D loss: 0.657531, acc.: 57.00%] [G loss: 0.928325]\n",
      "6600 [D loss: 0.741319, acc.: 53.50%] [G loss: 0.873762]\n",
      "6700 [D loss: 0.637749, acc.: 61.00%] [G loss: 0.927317]\n",
      "6800 [D loss: 1.068777, acc.: 35.50%] [G loss: 0.493603]\n",
      "6900 [D loss: 0.387829, acc.: 82.50%] [G loss: 1.701060]\n",
      "7000 [D loss: 0.641443, acc.: 61.00%] [G loss: 1.152063]\n",
      "7100 [D loss: 0.795767, acc.: 52.50%] [G loss: 0.861889]\n",
      "7200 [D loss: 0.505449, acc.: 81.00%] [G loss: 1.449244]\n",
      "7300 [D loss: 0.786778, acc.: 48.00%] [G loss: 0.845319]\n",
      "7400 [D loss: 0.784685, acc.: 51.50%] [G loss: 0.935993]\n",
      "7500 [D loss: 0.830621, acc.: 53.00%] [G loss: 0.781109]\n",
      "7600 [D loss: 0.384087, acc.: 90.50%] [G loss: 1.203737]\n",
      "7700 [D loss: 0.920268, acc.: 55.00%] [G loss: 0.994540]\n",
      "7800 [D loss: 0.647816, acc.: 77.00%] [G loss: 1.015107]\n",
      "7900 [D loss: 0.486406, acc.: 84.50%] [G loss: 1.042540]\n",
      "8000 [D loss: 0.564579, acc.: 78.00%] [G loss: 0.873154]\n",
      "8100 [D loss: 0.762127, acc.: 43.00%] [G loss: 0.551745]\n",
      "8200 [D loss: 0.784362, acc.: 45.50%] [G loss: 0.825812]\n",
      "8300 [D loss: 0.690734, acc.: 49.00%] [G loss: 0.848639]\n",
      "8400 [D loss: 0.767418, acc.: 43.50%] [G loss: 0.792946]\n",
      "8500 [D loss: 0.605096, acc.: 70.00%] [G loss: 1.307509]\n",
      "8600 [D loss: 0.931295, acc.: 36.50%] [G loss: 0.701407]\n",
      "8700 [D loss: 0.580658, acc.: 71.50%] [G loss: 1.043546]\n",
      "8800 [D loss: 0.924339, acc.: 46.00%] [G loss: 0.912251]\n",
      "8900 [D loss: 0.585508, acc.: 73.00%] [G loss: 1.133235]\n",
      "9000 [D loss: 0.657279, acc.: 55.50%] [G loss: 0.924510]\n",
      "9100 [D loss: 0.605350, acc.: 58.00%] [G loss: 0.883206]\n",
      "9200 [D loss: 0.691069, acc.: 58.00%] [G loss: 1.215470]\n",
      "9300 [D loss: 0.571740, acc.: 71.50%] [G loss: 1.322759]\n",
      "9400 [D loss: 0.653412, acc.: 62.50%] [G loss: 1.052151]\n",
      "9500 [D loss: 0.578176, acc.: 66.50%] [G loss: 1.170417]\n",
      "9600 [D loss: 0.621912, acc.: 65.50%] [G loss: 1.217743]\n",
      "9700 [D loss: 0.509755, acc.: 75.50%] [G loss: 1.459422]\n",
      "9800 [D loss: 0.837783, acc.: 54.50%] [G loss: 1.037166]\n",
      "9900 [D loss: 0.403474, acc.: 85.00%] [G loss: 1.422933]\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1/20 synthetic data sets.\n",
      "Created 2/20 synthetic data sets.\n",
      "Created 3/20 synthetic data sets.\n",
      "Created 4/20 synthetic data sets.\n",
      "Created 5/20 synthetic data sets.\n",
      "Created 6/20 synthetic data sets.\n",
      "Created 7/20 synthetic data sets.\n",
      "Created 8/20 synthetic data sets.\n",
      "Created 9/20 synthetic data sets.\n",
      "Created 10/20 synthetic data sets.\n",
      "Created 11/20 synthetic data sets.\n",
      "Created 12/20 synthetic data sets.\n",
      "Created 13/20 synthetic data sets.\n",
      "Created 14/20 synthetic data sets.\n",
      "Created 15/20 synthetic data sets.\n",
      "Created 16/20 synthetic data sets.\n",
      "Created 17/20 synthetic data sets.\n",
      "Created 18/20 synthetic data sets.\n",
      "Created 19/20 synthetic data sets.\n",
      "Created 20/20 synthetic data sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.712377, acc.: 37.50%] [G loss: 0.687657]\n",
      "100 [D loss: 0.769662, acc.: 38.00%] [G loss: 0.587318]\n",
      "200 [D loss: 0.692474, acc.: 45.50%] [G loss: 0.623007]\n",
      "300 [D loss: 0.539679, acc.: 88.00%] [G loss: 0.889849]\n",
      "400 [D loss: 0.644386, acc.: 59.50%] [G loss: 0.884852]\n",
      "500 [D loss: 0.606272, acc.: 74.50%] [G loss: 0.910845]\n",
      "600 [D loss: 1.081889, acc.: 27.00%] [G loss: 0.471749]\n",
      "700 [D loss: 0.536413, acc.: 85.00%] [G loss: 0.927639]\n",
      "800 [D loss: 0.720845, acc.: 63.00%] [G loss: 0.889657]\n",
      "900 [D loss: 0.709634, acc.: 54.50%] [G loss: 0.679301]\n",
      "1000 [D loss: 0.706853, acc.: 61.50%] [G loss: 0.737439]\n",
      "1100 [D loss: 0.711189, acc.: 60.50%] [G loss: 0.752824]\n",
      "1200 [D loss: 0.784429, acc.: 37.50%] [G loss: 0.699633]\n",
      "1300 [D loss: 0.600872, acc.: 71.00%] [G loss: 0.718808]\n",
      "1400 [D loss: 0.667406, acc.: 60.00%] [G loss: 0.775057]\n",
      "1500 [D loss: 0.672801, acc.: 66.00%] [G loss: 0.884597]\n",
      "1600 [D loss: 0.815819, acc.: 46.50%] [G loss: 0.727503]\n",
      "1700 [D loss: 0.749009, acc.: 44.00%] [G loss: 0.564938]\n",
      "1800 [D loss: 0.700989, acc.: 55.00%] [G loss: 0.667219]\n",
      "1900 [D loss: 0.665589, acc.: 70.50%] [G loss: 0.764673]\n",
      "2000 [D loss: 0.742740, acc.: 55.50%] [G loss: 0.793985]\n",
      "2100 [D loss: 0.645933, acc.: 62.50%] [G loss: 0.816110]\n",
      "2200 [D loss: 0.776188, acc.: 52.50%] [G loss: 0.722428]\n",
      "2300 [D loss: 0.696462, acc.: 50.50%] [G loss: 0.844549]\n",
      "2400 [D loss: 0.412895, acc.: 90.00%] [G loss: 1.254311]\n",
      "2500 [D loss: 0.637126, acc.: 64.50%] [G loss: 0.740490]\n",
      "2600 [D loss: 0.572412, acc.: 89.50%] [G loss: 0.870005]\n",
      "2700 [D loss: 0.755402, acc.: 34.50%] [G loss: 0.709968]\n",
      "2800 [D loss: 0.645869, acc.: 59.00%] [G loss: 0.864368]\n",
      "2900 [D loss: 0.649235, acc.: 65.50%] [G loss: 0.808850]\n",
      "3000 [D loss: 0.709183, acc.: 50.00%] [G loss: 0.683307]\n",
      "3100 [D loss: 0.538147, acc.: 72.00%] [G loss: 1.086860]\n",
      "3200 [D loss: 0.653675, acc.: 72.50%] [G loss: 0.724953]\n",
      "3300 [D loss: 0.692314, acc.: 64.50%] [G loss: 0.792695]\n",
      "3400 [D loss: 0.659177, acc.: 68.00%] [G loss: 0.791634]\n",
      "3500 [D loss: 0.634018, acc.: 60.50%] [G loss: 0.757246]\n",
      "3600 [D loss: 0.605315, acc.: 72.50%] [G loss: 0.853215]\n",
      "3700 [D loss: 0.715078, acc.: 59.00%] [G loss: 0.730730]\n",
      "3800 [D loss: 0.667117, acc.: 63.50%] [G loss: 0.724069]\n",
      "3900 [D loss: 0.641206, acc.: 58.50%] [G loss: 0.758761]\n",
      "4000 [D loss: 0.644667, acc.: 59.00%] [G loss: 0.831097]\n",
      "4100 [D loss: 0.729083, acc.: 50.00%] [G loss: 0.830340]\n",
      "4200 [D loss: 0.667546, acc.: 54.00%] [G loss: 0.709994]\n",
      "4300 [D loss: 0.746322, acc.: 40.50%] [G loss: 0.682812]\n",
      "4400 [D loss: 0.649255, acc.: 59.00%] [G loss: 0.917197]\n",
      "4500 [D loss: 0.669482, acc.: 57.50%] [G loss: 0.773552]\n",
      "4600 [D loss: 0.619812, acc.: 62.50%] [G loss: 0.869706]\n",
      "4700 [D loss: 0.676394, acc.: 53.50%] [G loss: 0.839643]\n",
      "4800 [D loss: 0.849192, acc.: 47.50%] [G loss: 0.767861]\n",
      "4900 [D loss: 0.747454, acc.: 57.00%] [G loss: 0.856761]\n",
      "5000 [D loss: 0.964193, acc.: 23.00%] [G loss: 0.491396]\n",
      "5100 [D loss: 0.685815, acc.: 62.50%] [G loss: 0.782458]\n",
      "5200 [D loss: 0.622647, acc.: 60.50%] [G loss: 0.950782]\n",
      "5300 [D loss: 0.571779, acc.: 71.50%] [G loss: 1.054162]\n",
      "5400 [D loss: 0.642259, acc.: 59.00%] [G loss: 0.803915]\n",
      "5500 [D loss: 0.624142, acc.: 60.50%] [G loss: 0.857072]\n",
      "5600 [D loss: 0.706436, acc.: 61.00%] [G loss: 0.800862]\n",
      "5700 [D loss: 0.646546, acc.: 63.00%] [G loss: 0.841396]\n",
      "5800 [D loss: 0.745097, acc.: 54.50%] [G loss: 0.820339]\n",
      "5900 [D loss: 0.729732, acc.: 58.00%] [G loss: 0.896252]\n",
      "6000 [D loss: 0.650195, acc.: 59.00%] [G loss: 0.900149]\n",
      "6100 [D loss: 0.655821, acc.: 63.00%] [G loss: 0.905707]\n",
      "6200 [D loss: 0.629460, acc.: 55.50%] [G loss: 0.897145]\n",
      "6300 [D loss: 0.651043, acc.: 58.00%] [G loss: 1.013982]\n",
      "6400 [D loss: 0.522319, acc.: 69.00%] [G loss: 1.083341]\n",
      "6500 [D loss: 0.536303, acc.: 69.00%] [G loss: 0.860250]\n",
      "6600 [D loss: 0.717731, acc.: 63.50%] [G loss: 1.160360]\n",
      "6700 [D loss: 0.891955, acc.: 36.00%] [G loss: 0.552568]\n",
      "6800 [D loss: 0.720753, acc.: 65.50%] [G loss: 1.186561]\n",
      "6900 [D loss: 0.992929, acc.: 34.50%] [G loss: 0.411065]\n",
      "7000 [D loss: 0.675228, acc.: 61.00%] [G loss: 0.897013]\n",
      "7100 [D loss: 0.679659, acc.: 57.50%] [G loss: 0.985055]\n",
      "7200 [D loss: 0.623541, acc.: 69.50%] [G loss: 1.230621]\n",
      "7300 [D loss: 0.737334, acc.: 50.00%] [G loss: 0.868872]\n",
      "7400 [D loss: 0.643011, acc.: 59.50%] [G loss: 0.697317]\n",
      "7500 [D loss: 0.737485, acc.: 56.50%] [G loss: 0.892310]\n",
      "7600 [D loss: 0.503981, acc.: 75.50%] [G loss: 1.246961]\n",
      "7700 [D loss: 0.539938, acc.: 72.00%] [G loss: 0.927622]\n",
      "7800 [D loss: 0.691156, acc.: 70.00%] [G loss: 0.968088]\n",
      "7900 [D loss: 0.724325, acc.: 47.00%] [G loss: 0.729075]\n",
      "8000 [D loss: 0.643942, acc.: 71.00%] [G loss: 0.987962]\n",
      "8100 [D loss: 0.912457, acc.: 42.50%] [G loss: 0.783104]\n",
      "8200 [D loss: 0.758774, acc.: 53.50%] [G loss: 0.926407]\n",
      "8300 [D loss: 0.697631, acc.: 63.00%] [G loss: 1.131768]\n",
      "8400 [D loss: 0.805694, acc.: 44.50%] [G loss: 0.935600]\n",
      "8500 [D loss: 0.747624, acc.: 49.00%] [G loss: 0.600013]\n",
      "8600 [D loss: 0.656165, acc.: 63.50%] [G loss: 0.883306]\n",
      "8700 [D loss: 0.615650, acc.: 68.00%] [G loss: 1.186052]\n",
      "8800 [D loss: 0.763658, acc.: 55.50%] [G loss: 0.948641]\n",
      "8900 [D loss: 0.595215, acc.: 63.00%] [G loss: 0.788165]\n",
      "9000 [D loss: 0.652795, acc.: 59.50%] [G loss: 1.156608]\n",
      "9100 [D loss: 0.770508, acc.: 46.00%] [G loss: 0.816901]\n",
      "9200 [D loss: 0.565641, acc.: 69.00%] [G loss: 0.760709]\n",
      "9300 [D loss: 0.756334, acc.: 54.00%] [G loss: 0.720617]\n",
      "9400 [D loss: 0.745141, acc.: 49.00%] [G loss: 0.903917]\n",
      "9500 [D loss: 0.663055, acc.: 57.50%] [G loss: 1.041675]\n",
      "9600 [D loss: 0.658570, acc.: 66.00%] [G loss: 0.964506]\n",
      "9700 [D loss: 0.560721, acc.: 78.50%] [G loss: 1.104939]\n",
      "9800 [D loss: 0.448015, acc.: 75.50%] [G loss: 1.644797]\n",
      "9900 [D loss: 0.375256, acc.: 84.00%] [G loss: 1.775376]\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1/20 synthetic data sets.\n",
      "Created 2/20 synthetic data sets.\n",
      "Created 3/20 synthetic data sets.\n",
      "Created 4/20 synthetic data sets.\n",
      "Created 5/20 synthetic data sets.\n",
      "Created 6/20 synthetic data sets.\n",
      "Created 7/20 synthetic data sets.\n",
      "Created 8/20 synthetic data sets.\n",
      "Created 9/20 synthetic data sets.\n",
      "Created 10/20 synthetic data sets.\n",
      "Created 11/20 synthetic data sets.\n",
      "Created 12/20 synthetic data sets.\n",
      "Created 13/20 synthetic data sets.\n",
      "Created 14/20 synthetic data sets.\n",
      "Created 15/20 synthetic data sets.\n",
      "Created 16/20 synthetic data sets.\n",
      "Created 17/20 synthetic data sets.\n",
      "Created 18/20 synthetic data sets.\n",
      "Created 19/20 synthetic data sets.\n",
      "Created 20/20 synthetic data sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.496136, acc.: 76.50%] [G loss: 0.698081]\n",
      "100 [D loss: 0.753981, acc.: 42.50%] [G loss: 0.549714]\n",
      "200 [D loss: 0.699537, acc.: 50.50%] [G loss: 0.689856]\n",
      "300 [D loss: 0.667475, acc.: 75.50%] [G loss: 0.832108]\n",
      "400 [D loss: 0.409153, acc.: 93.50%] [G loss: 1.164187]\n",
      "500 [D loss: 0.515485, acc.: 82.00%] [G loss: 1.062199]\n",
      "600 [D loss: 0.864151, acc.: 37.50%] [G loss: 0.713775]\n",
      "700 [D loss: 0.573642, acc.: 79.00%] [G loss: 0.820848]\n",
      "800 [D loss: 0.758965, acc.: 66.50%] [G loss: 0.899719]\n",
      "900 [D loss: 0.490529, acc.: 85.00%] [G loss: 0.874714]\n",
      "1000 [D loss: 0.771344, acc.: 32.50%] [G loss: 0.658767]\n",
      "1100 [D loss: 0.701541, acc.: 62.50%] [G loss: 0.751809]\n",
      "1200 [D loss: 0.873072, acc.: 43.50%] [G loss: 0.750356]\n",
      "1300 [D loss: 0.508142, acc.: 77.00%] [G loss: 0.913487]\n",
      "1400 [D loss: 0.548437, acc.: 84.50%] [G loss: 0.845491]\n",
      "1500 [D loss: 0.790354, acc.: 57.50%] [G loss: 0.819828]\n",
      "1600 [D loss: 0.673702, acc.: 78.00%] [G loss: 0.947754]\n",
      "1700 [D loss: 0.707852, acc.: 61.50%] [G loss: 0.789741]\n",
      "1800 [D loss: 0.679868, acc.: 59.50%] [G loss: 0.936782]\n",
      "1900 [D loss: 0.670551, acc.: 62.50%] [G loss: 0.695857]\n",
      "2000 [D loss: 0.702348, acc.: 50.00%] [G loss: 0.692311]\n",
      "2100 [D loss: 0.612409, acc.: 65.50%] [G loss: 0.967391]\n",
      "2200 [D loss: 0.498453, acc.: 78.50%] [G loss: 1.179055]\n",
      "2300 [D loss: 0.779297, acc.: 45.50%] [G loss: 0.637979]\n",
      "2400 [D loss: 0.792394, acc.: 57.00%] [G loss: 0.678336]\n",
      "2500 [D loss: 0.678560, acc.: 59.00%] [G loss: 0.727485]\n",
      "2600 [D loss: 0.714598, acc.: 54.00%] [G loss: 0.852582]\n",
      "2700 [D loss: 0.674900, acc.: 61.00%] [G loss: 0.791809]\n",
      "2800 [D loss: 0.772485, acc.: 38.50%] [G loss: 0.779916]\n",
      "2900 [D loss: 0.578780, acc.: 71.50%] [G loss: 0.885576]\n",
      "3000 [D loss: 0.706748, acc.: 53.00%] [G loss: 0.787538]\n",
      "3100 [D loss: 0.682497, acc.: 51.00%] [G loss: 0.751674]\n",
      "3200 [D loss: 0.667514, acc.: 59.00%] [G loss: 0.870375]\n",
      "3300 [D loss: 0.696477, acc.: 57.50%] [G loss: 0.890349]\n",
      "3400 [D loss: 0.600756, acc.: 70.00%] [G loss: 0.880276]\n",
      "3500 [D loss: 0.765283, acc.: 40.00%] [G loss: 0.799860]\n",
      "3600 [D loss: 0.640080, acc.: 61.00%] [G loss: 0.708599]\n",
      "3700 [D loss: 0.594478, acc.: 66.00%] [G loss: 0.912752]\n",
      "3800 [D loss: 0.534093, acc.: 83.00%] [G loss: 1.005700]\n",
      "3900 [D loss: 0.661005, acc.: 60.50%] [G loss: 0.965146]\n",
      "4000 [D loss: 0.688366, acc.: 55.50%] [G loss: 0.745723]\n",
      "4100 [D loss: 0.734782, acc.: 50.00%] [G loss: 0.818005]\n",
      "4200 [D loss: 0.842866, acc.: 45.00%] [G loss: 0.834875]\n",
      "4300 [D loss: 0.644883, acc.: 56.50%] [G loss: 0.889410]\n",
      "4400 [D loss: 0.544285, acc.: 78.00%] [G loss: 1.077186]\n",
      "4500 [D loss: 0.696545, acc.: 58.50%] [G loss: 0.812779]\n",
      "4600 [D loss: 0.632062, acc.: 61.50%] [G loss: 1.023114]\n",
      "4700 [D loss: 0.793537, acc.: 31.50%] [G loss: 0.715423]\n",
      "4800 [D loss: 0.631112, acc.: 65.00%] [G loss: 0.792812]\n",
      "4900 [D loss: 0.678254, acc.: 56.50%] [G loss: 0.723562]\n",
      "5000 [D loss: 0.683272, acc.: 58.00%] [G loss: 0.856420]\n",
      "5100 [D loss: 0.671224, acc.: 70.50%] [G loss: 0.859402]\n",
      "5200 [D loss: 0.676025, acc.: 57.50%] [G loss: 0.811054]\n",
      "5300 [D loss: 0.662762, acc.: 55.00%] [G loss: 0.848033]\n",
      "5400 [D loss: 0.669148, acc.: 60.00%] [G loss: 0.878510]\n",
      "5500 [D loss: 0.617257, acc.: 60.50%] [G loss: 1.016056]\n",
      "5600 [D loss: 0.580037, acc.: 68.50%] [G loss: 0.990617]\n",
      "5700 [D loss: 0.679499, acc.: 57.50%] [G loss: 1.097108]\n",
      "5800 [D loss: 0.661737, acc.: 59.00%] [G loss: 1.090238]\n",
      "5900 [D loss: 0.596954, acc.: 65.50%] [G loss: 0.973434]\n",
      "6000 [D loss: 0.525605, acc.: 76.50%] [G loss: 1.073466]\n",
      "6100 [D loss: 0.677961, acc.: 53.50%] [G loss: 0.685777]\n",
      "6200 [D loss: 0.649468, acc.: 62.50%] [G loss: 0.861017]\n",
      "6300 [D loss: 0.841949, acc.: 41.00%] [G loss: 0.725041]\n",
      "6400 [D loss: 0.610906, acc.: 70.00%] [G loss: 1.156525]\n",
      "6500 [D loss: 0.335261, acc.: 93.00%] [G loss: 1.380221]\n",
      "6600 [D loss: 0.873930, acc.: 43.50%] [G loss: 0.850929]\n",
      "6700 [D loss: 0.608516, acc.: 71.00%] [G loss: 0.969559]\n",
      "6800 [D loss: 0.929143, acc.: 45.50%] [G loss: 1.037130]\n",
      "6900 [D loss: 0.567844, acc.: 73.00%] [G loss: 1.070742]\n",
      "7000 [D loss: 0.648647, acc.: 65.00%] [G loss: 1.132267]\n",
      "7100 [D loss: 0.583523, acc.: 72.00%] [G loss: 0.861562]\n",
      "7200 [D loss: 0.854345, acc.: 37.50%] [G loss: 0.630915]\n",
      "7300 [D loss: 0.556460, acc.: 77.50%] [G loss: 0.978886]\n",
      "7400 [D loss: 0.692132, acc.: 60.50%] [G loss: 1.037442]\n",
      "7500 [D loss: 0.763619, acc.: 62.00%] [G loss: 0.889435]\n",
      "7600 [D loss: 0.402668, acc.: 86.00%] [G loss: 1.327078]\n",
      "7700 [D loss: 0.750307, acc.: 59.00%] [G loss: 1.005270]\n",
      "7800 [D loss: 0.742832, acc.: 57.50%] [G loss: 0.806034]\n",
      "7900 [D loss: 0.557457, acc.: 72.00%] [G loss: 1.318510]\n",
      "8000 [D loss: 0.647777, acc.: 58.00%] [G loss: 1.189964]\n",
      "8100 [D loss: 0.748259, acc.: 68.50%] [G loss: 1.069967]\n",
      "8200 [D loss: 0.589713, acc.: 69.50%] [G loss: 0.740838]\n",
      "8300 [D loss: 0.472972, acc.: 88.00%] [G loss: 1.184313]\n",
      "8400 [D loss: 0.805296, acc.: 63.00%] [G loss: 0.932043]\n",
      "8500 [D loss: 0.347457, acc.: 87.00%] [G loss: 1.371828]\n",
      "8600 [D loss: 0.698692, acc.: 68.00%] [G loss: 1.311392]\n",
      "8700 [D loss: 0.571166, acc.: 69.50%] [G loss: 1.346415]\n",
      "8800 [D loss: 0.699575, acc.: 57.50%] [G loss: 1.286631]\n",
      "8900 [D loss: 0.850452, acc.: 43.00%] [G loss: 0.738043]\n",
      "9000 [D loss: 0.527375, acc.: 80.50%] [G loss: 1.818501]\n",
      "9100 [D loss: 0.534683, acc.: 73.00%] [G loss: 1.070480]\n",
      "9200 [D loss: 0.473555, acc.: 84.50%] [G loss: 1.207503]\n",
      "9300 [D loss: 0.665244, acc.: 70.00%] [G loss: 0.985984]\n",
      "9400 [D loss: 0.833287, acc.: 67.50%] [G loss: 1.082825]\n",
      "9500 [D loss: 0.689478, acc.: 63.00%] [G loss: 0.745065]\n",
      "9600 [D loss: 0.715553, acc.: 49.00%] [G loss: 0.881294]\n",
      "9700 [D loss: 0.606096, acc.: 76.50%] [G loss: 0.927279]\n",
      "9800 [D loss: 0.806377, acc.: 55.50%] [G loss: 0.785280]\n",
      "9900 [D loss: 0.654886, acc.: 59.50%] [G loss: 0.725127]\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1/20 synthetic data sets.\n",
      "Created 2/20 synthetic data sets.\n",
      "Created 3/20 synthetic data sets.\n",
      "Created 4/20 synthetic data sets.\n",
      "Created 5/20 synthetic data sets.\n",
      "Created 6/20 synthetic data sets.\n",
      "Created 7/20 synthetic data sets.\n",
      "Created 8/20 synthetic data sets.\n",
      "Created 9/20 synthetic data sets.\n",
      "Created 10/20 synthetic data sets.\n",
      "Created 11/20 synthetic data sets.\n",
      "Created 12/20 synthetic data sets.\n",
      "Created 13/20 synthetic data sets.\n",
      "Created 14/20 synthetic data sets.\n",
      "Created 15/20 synthetic data sets.\n",
      "Created 16/20 synthetic data sets.\n",
      "Created 17/20 synthetic data sets.\n",
      "Created 18/20 synthetic data sets.\n",
      "Created 19/20 synthetic data sets.\n",
      "Created 20/20 synthetic data sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.888432, acc.: 41.50%] [G loss: 0.786200]\n",
      "100 [D loss: 0.709028, acc.: 30.50%] [G loss: 0.588364]\n",
      "200 [D loss: 0.476131, acc.: 98.00%] [G loss: 0.940437]\n",
      "300 [D loss: 0.660891, acc.: 53.50%] [G loss: 0.703461]\n",
      "400 [D loss: 0.851783, acc.: 5.00%] [G loss: 0.606419]\n",
      "500 [D loss: 0.738596, acc.: 40.00%] [G loss: 0.607260]\n",
      "600 [D loss: 0.657953, acc.: 59.00%] [G loss: 0.692789]\n",
      "700 [D loss: 0.718753, acc.: 46.50%] [G loss: 0.656974]\n",
      "800 [D loss: 0.600192, acc.: 84.00%] [G loss: 0.816476]\n",
      "900 [D loss: 0.688377, acc.: 56.50%] [G loss: 0.707166]\n",
      "1000 [D loss: 0.734971, acc.: 52.50%] [G loss: 0.650678]\n",
      "1100 [D loss: 0.660259, acc.: 50.00%] [G loss: 0.832685]\n",
      "1200 [D loss: 0.676727, acc.: 59.00%] [G loss: 0.715964]\n",
      "1300 [D loss: 0.673043, acc.: 57.50%] [G loss: 0.761562]\n",
      "1400 [D loss: 0.736519, acc.: 31.50%] [G loss: 0.655225]\n",
      "1500 [D loss: 0.576020, acc.: 75.50%] [G loss: 0.807433]\n",
      "1600 [D loss: 0.725970, acc.: 43.00%] [G loss: 0.693479]\n",
      "1700 [D loss: 0.597486, acc.: 82.00%] [G loss: 0.889555]\n",
      "1800 [D loss: 0.594137, acc.: 90.00%] [G loss: 0.825712]\n",
      "1900 [D loss: 0.728862, acc.: 50.00%] [G loss: 0.744267]\n",
      "2000 [D loss: 0.570112, acc.: 83.50%] [G loss: 0.878063]\n",
      "2100 [D loss: 0.704265, acc.: 48.00%] [G loss: 0.697361]\n",
      "2200 [D loss: 0.672798, acc.: 55.50%] [G loss: 0.722791]\n",
      "2300 [D loss: 0.715541, acc.: 45.00%] [G loss: 0.739687]\n",
      "2400 [D loss: 0.721329, acc.: 46.50%] [G loss: 0.753545]\n",
      "2500 [D loss: 0.795723, acc.: 44.50%] [G loss: 0.722125]\n",
      "2600 [D loss: 0.731695, acc.: 59.00%] [G loss: 0.784592]\n",
      "2700 [D loss: 0.325258, acc.: 92.50%] [G loss: 1.428769]\n",
      "2800 [D loss: 0.782111, acc.: 42.50%] [G loss: 0.623909]\n",
      "2900 [D loss: 0.715786, acc.: 55.50%] [G loss: 0.729933]\n",
      "3000 [D loss: 0.597309, acc.: 74.50%] [G loss: 0.865549]\n",
      "3100 [D loss: 0.609522, acc.: 77.00%] [G loss: 0.778351]\n",
      "3200 [D loss: 0.599451, acc.: 72.50%] [G loss: 0.938396]\n",
      "3300 [D loss: 0.546970, acc.: 75.50%] [G loss: 0.971512]\n",
      "3400 [D loss: 0.734776, acc.: 36.00%] [G loss: 0.707244]\n",
      "3500 [D loss: 0.588392, acc.: 68.50%] [G loss: 0.934140]\n",
      "3600 [D loss: 0.777927, acc.: 39.50%] [G loss: 0.638712]\n",
      "3700 [D loss: 0.650464, acc.: 52.50%] [G loss: 0.861313]\n",
      "3800 [D loss: 0.805620, acc.: 41.00%] [G loss: 0.681768]\n",
      "3900 [D loss: 0.708620, acc.: 53.00%] [G loss: 0.856015]\n",
      "4000 [D loss: 0.622935, acc.: 68.00%] [G loss: 0.860482]\n",
      "4100 [D loss: 0.667084, acc.: 59.50%] [G loss: 0.874128]\n",
      "4200 [D loss: 0.733129, acc.: 53.50%] [G loss: 0.722436]\n",
      "4300 [D loss: 0.622328, acc.: 67.50%] [G loss: 0.799067]\n",
      "4400 [D loss: 0.654391, acc.: 54.50%] [G loss: 0.842256]\n",
      "4500 [D loss: 0.502474, acc.: 83.50%] [G loss: 0.901939]\n",
      "4600 [D loss: 0.681096, acc.: 53.50%] [G loss: 0.923285]\n",
      "4700 [D loss: 0.659335, acc.: 52.50%] [G loss: 0.817160]\n",
      "4800 [D loss: 0.805649, acc.: 45.50%] [G loss: 0.750096]\n",
      "4900 [D loss: 0.647787, acc.: 63.00%] [G loss: 0.637640]\n",
      "5000 [D loss: 0.754435, acc.: 47.00%] [G loss: 0.768835]\n",
      "5100 [D loss: 0.676495, acc.: 58.00%] [G loss: 0.854602]\n",
      "5200 [D loss: 0.644984, acc.: 61.00%] [G loss: 0.889475]\n",
      "5300 [D loss: 0.616280, acc.: 61.00%] [G loss: 1.020374]\n",
      "5400 [D loss: 0.627849, acc.: 65.00%] [G loss: 0.858739]\n",
      "5500 [D loss: 0.670305, acc.: 37.00%] [G loss: 0.815498]\n",
      "5600 [D loss: 0.587719, acc.: 62.50%] [G loss: 0.878985]\n",
      "5700 [D loss: 1.193554, acc.: 43.50%] [G loss: 0.259876]\n",
      "5800 [D loss: 0.780549, acc.: 49.00%] [G loss: 0.691147]\n",
      "5900 [D loss: 0.724201, acc.: 55.00%] [G loss: 0.984174]\n",
      "6000 [D loss: 0.585296, acc.: 80.00%] [G loss: 1.132825]\n",
      "6100 [D loss: 0.551729, acc.: 75.50%] [G loss: 1.211522]\n",
      "6200 [D loss: 0.694565, acc.: 57.50%] [G loss: 1.201263]\n",
      "6300 [D loss: 0.577859, acc.: 83.00%] [G loss: 1.146230]\n",
      "6400 [D loss: 0.339040, acc.: 97.00%] [G loss: 1.328556]\n",
      "6500 [D loss: 0.937560, acc.: 67.50%] [G loss: 0.856505]\n",
      "6600 [D loss: 0.824954, acc.: 45.00%] [G loss: 0.748680]\n",
      "6700 [D loss: 0.811273, acc.: 42.00%] [G loss: 0.718486]\n",
      "6800 [D loss: 0.640374, acc.: 76.00%] [G loss: 0.951452]\n",
      "6900 [D loss: 0.520831, acc.: 79.00%] [G loss: 1.147944]\n",
      "7000 [D loss: 0.769792, acc.: 59.50%] [G loss: 0.886095]\n",
      "7100 [D loss: 0.531232, acc.: 72.00%] [G loss: 1.230630]\n",
      "7200 [D loss: 0.685031, acc.: 60.50%] [G loss: 0.772892]\n",
      "7300 [D loss: 0.680077, acc.: 56.50%] [G loss: 0.809300]\n",
      "7400 [D loss: 0.865514, acc.: 50.50%] [G loss: 0.789077]\n",
      "7500 [D loss: 0.518285, acc.: 82.00%] [G loss: 1.114524]\n",
      "7600 [D loss: 0.863849, acc.: 49.50%] [G loss: 0.735134]\n",
      "7700 [D loss: 0.514730, acc.: 78.00%] [G loss: 1.107276]\n",
      "7800 [D loss: 0.740377, acc.: 63.00%] [G loss: 1.063209]\n",
      "7900 [D loss: 0.606775, acc.: 76.50%] [G loss: 1.128640]\n",
      "8000 [D loss: 0.773074, acc.: 56.50%] [G loss: 1.020325]\n",
      "8100 [D loss: 0.610222, acc.: 66.50%] [G loss: 1.150571]\n",
      "8200 [D loss: 0.558258, acc.: 70.50%] [G loss: 1.025835]\n",
      "8300 [D loss: 0.614843, acc.: 60.00%] [G loss: 1.263600]\n",
      "8400 [D loss: 0.627012, acc.: 56.00%] [G loss: 0.834911]\n",
      "8500 [D loss: 0.708819, acc.: 60.50%] [G loss: 1.142154]\n",
      "8600 [D loss: 0.776970, acc.: 57.50%] [G loss: 1.145912]\n",
      "8700 [D loss: 0.592561, acc.: 61.00%] [G loss: 1.176716]\n",
      "8800 [D loss: 0.757642, acc.: 60.50%] [G loss: 1.095359]\n",
      "8900 [D loss: 0.561046, acc.: 66.50%] [G loss: 1.127531]\n",
      "9000 [D loss: 0.651512, acc.: 61.00%] [G loss: 1.069500]\n",
      "9100 [D loss: 0.643493, acc.: 62.50%] [G loss: 0.907528]\n",
      "9200 [D loss: 0.567093, acc.: 68.00%] [G loss: 1.173245]\n",
      "9300 [D loss: 0.614003, acc.: 66.00%] [G loss: 0.983936]\n",
      "9400 [D loss: 0.706449, acc.: 53.00%] [G loss: 0.825355]\n",
      "9500 [D loss: 0.445249, acc.: 93.50%] [G loss: 1.283071]\n",
      "9600 [D loss: 0.573733, acc.: 79.00%] [G loss: 1.876341]\n",
      "9700 [D loss: 1.122485, acc.: 51.50%] [G loss: 1.098725]\n",
      "9800 [D loss: 0.669392, acc.: 63.50%] [G loss: 1.173742]\n",
      "9900 [D loss: 0.530989, acc.: 80.00%] [G loss: 0.941767]\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1/20 synthetic data sets.\n",
      "Created 2/20 synthetic data sets.\n",
      "Created 3/20 synthetic data sets.\n",
      "Created 4/20 synthetic data sets.\n",
      "Created 5/20 synthetic data sets.\n",
      "Created 6/20 synthetic data sets.\n",
      "Created 7/20 synthetic data sets.\n",
      "Created 8/20 synthetic data sets.\n",
      "Created 9/20 synthetic data sets.\n",
      "Created 10/20 synthetic data sets.\n",
      "Created 11/20 synthetic data sets.\n",
      "Created 12/20 synthetic data sets.\n",
      "Created 13/20 synthetic data sets.\n",
      "Created 14/20 synthetic data sets.\n",
      "Created 15/20 synthetic data sets.\n",
      "Created 16/20 synthetic data sets.\n",
      "Created 17/20 synthetic data sets.\n",
      "Created 18/20 synthetic data sets.\n",
      "Created 19/20 synthetic data sets.\n",
      "Created 20/20 synthetic data sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.522558, acc.: 55.00%] [G loss: 0.577350]\n",
      "100 [D loss: 0.729107, acc.: 42.50%] [G loss: 0.553248]\n",
      "200 [D loss: 0.613371, acc.: 62.00%] [G loss: 0.635321]\n",
      "300 [D loss: 0.629466, acc.: 80.00%] [G loss: 0.782421]\n",
      "400 [D loss: 0.580263, acc.: 80.50%] [G loss: 0.860062]\n",
      "500 [D loss: 0.709631, acc.: 28.50%] [G loss: 0.676202]\n",
      "600 [D loss: 0.693739, acc.: 60.50%] [G loss: 0.711263]\n",
      "700 [D loss: 0.732820, acc.: 38.50%] [G loss: 0.649911]\n",
      "800 [D loss: 0.737175, acc.: 36.00%] [G loss: 0.631370]\n",
      "900 [D loss: 0.655101, acc.: 61.00%] [G loss: 0.826390]\n",
      "1000 [D loss: 0.655214, acc.: 63.00%] [G loss: 0.810637]\n",
      "1100 [D loss: 0.719417, acc.: 38.50%] [G loss: 0.639939]\n",
      "1200 [D loss: 0.670894, acc.: 68.50%] [G loss: 0.693543]\n",
      "1300 [D loss: 0.716413, acc.: 49.50%] [G loss: 0.693027]\n",
      "1400 [D loss: 0.694930, acc.: 47.50%] [G loss: 0.687037]\n",
      "1500 [D loss: 0.705417, acc.: 47.50%] [G loss: 0.682033]\n",
      "1600 [D loss: 0.699486, acc.: 45.50%] [G loss: 0.691056]\n",
      "1700 [D loss: 0.643723, acc.: 80.00%] [G loss: 0.753588]\n",
      "1800 [D loss: 0.736162, acc.: 53.50%] [G loss: 0.706898]\n",
      "1900 [D loss: 0.724908, acc.: 16.50%] [G loss: 0.663894]\n",
      "2000 [D loss: 0.705641, acc.: 46.50%] [G loss: 0.695275]\n",
      "2100 [D loss: 0.653809, acc.: 61.00%] [G loss: 0.709936]\n",
      "2200 [D loss: 0.687224, acc.: 51.50%] [G loss: 0.685094]\n",
      "2300 [D loss: 0.713430, acc.: 53.50%] [G loss: 0.626299]\n",
      "2400 [D loss: 0.654080, acc.: 64.50%] [G loss: 0.686379]\n",
      "2500 [D loss: 0.697188, acc.: 52.50%] [G loss: 0.722720]\n",
      "2600 [D loss: 0.668911, acc.: 69.50%] [G loss: 0.766032]\n",
      "2700 [D loss: 0.631560, acc.: 92.00%] [G loss: 0.764130]\n",
      "2800 [D loss: 0.706165, acc.: 51.00%] [G loss: 0.657652]\n",
      "2900 [D loss: 0.672729, acc.: 72.00%] [G loss: 0.754965]\n",
      "3000 [D loss: 0.711873, acc.: 44.50%] [G loss: 0.668947]\n",
      "3100 [D loss: 0.627082, acc.: 79.50%] [G loss: 0.750844]\n",
      "3200 [D loss: 0.629843, acc.: 60.50%] [G loss: 0.671948]\n",
      "3300 [D loss: 0.728971, acc.: 30.00%] [G loss: 0.663097]\n",
      "3400 [D loss: 0.715903, acc.: 46.50%] [G loss: 0.700881]\n",
      "3500 [D loss: 0.691043, acc.: 49.00%] [G loss: 0.714361]\n",
      "3600 [D loss: 0.671399, acc.: 49.50%] [G loss: 0.728143]\n",
      "3700 [D loss: 0.699119, acc.: 64.50%] [G loss: 0.721551]\n",
      "3800 [D loss: 0.723044, acc.: 44.00%] [G loss: 0.665170]\n",
      "3900 [D loss: 0.648042, acc.: 61.00%] [G loss: 0.725792]\n",
      "4000 [D loss: 0.674948, acc.: 61.00%] [G loss: 0.778412]\n",
      "4100 [D loss: 0.696638, acc.: 46.00%] [G loss: 0.631601]\n",
      "4200 [D loss: 0.675998, acc.: 58.00%] [G loss: 0.684255]\n",
      "4300 [D loss: 0.697801, acc.: 50.00%] [G loss: 0.732276]\n",
      "4400 [D loss: 0.627632, acc.: 73.00%] [G loss: 0.832543]\n",
      "4500 [D loss: 0.649594, acc.: 72.50%] [G loss: 0.705087]\n",
      "4600 [D loss: 0.754684, acc.: 47.00%] [G loss: 0.641012]\n",
      "4700 [D loss: 0.684059, acc.: 53.50%] [G loss: 0.690076]\n",
      "4800 [D loss: 0.641151, acc.: 71.50%] [G loss: 0.732624]\n",
      "4900 [D loss: 0.633639, acc.: 73.50%] [G loss: 0.784256]\n",
      "5000 [D loss: 0.672565, acc.: 59.50%] [G loss: 0.702185]\n",
      "5100 [D loss: 0.640320, acc.: 69.00%] [G loss: 0.850048]\n",
      "5200 [D loss: 0.654675, acc.: 59.50%] [G loss: 0.699129]\n",
      "5300 [D loss: 0.723333, acc.: 27.50%] [G loss: 0.667740]\n",
      "5400 [D loss: 0.571946, acc.: 79.00%] [G loss: 0.928321]\n",
      "5500 [D loss: 0.714793, acc.: 52.50%] [G loss: 0.705088]\n",
      "5600 [D loss: 0.719637, acc.: 50.00%] [G loss: 0.772738]\n",
      "5700 [D loss: 0.697468, acc.: 61.50%] [G loss: 0.725522]\n",
      "5800 [D loss: 0.710585, acc.: 43.00%] [G loss: 0.674762]\n",
      "5900 [D loss: 0.694526, acc.: 44.50%] [G loss: 0.665805]\n",
      "6000 [D loss: 0.800224, acc.: 9.50%] [G loss: 0.617744]\n",
      "6100 [D loss: 0.611374, acc.: 73.00%] [G loss: 0.752048]\n",
      "6200 [D loss: 0.764038, acc.: 22.00%] [G loss: 0.688154]\n",
      "6300 [D loss: 0.767414, acc.: 33.50%] [G loss: 0.627082]\n",
      "6400 [D loss: 0.685657, acc.: 62.50%] [G loss: 0.755390]\n",
      "6500 [D loss: 0.644396, acc.: 73.50%] [G loss: 0.835800]\n",
      "6600 [D loss: 0.687588, acc.: 50.50%] [G loss: 0.708775]\n",
      "6700 [D loss: 0.707755, acc.: 51.00%] [G loss: 0.683382]\n",
      "6800 [D loss: 0.729268, acc.: 41.00%] [G loss: 0.669648]\n",
      "6900 [D loss: 0.692028, acc.: 48.00%] [G loss: 0.713786]\n",
      "7000 [D loss: 0.685801, acc.: 51.50%] [G loss: 0.654824]\n",
      "7100 [D loss: 0.644133, acc.: 64.50%] [G loss: 0.801935]\n",
      "7200 [D loss: 0.712368, acc.: 44.50%] [G loss: 0.698389]\n",
      "7300 [D loss: 0.669777, acc.: 69.00%] [G loss: 0.721965]\n",
      "7400 [D loss: 0.683340, acc.: 52.00%] [G loss: 0.700725]\n",
      "7500 [D loss: 0.711227, acc.: 51.00%] [G loss: 0.742303]\n",
      "7600 [D loss: 0.691276, acc.: 56.00%] [G loss: 0.707693]\n",
      "7700 [D loss: 0.695709, acc.: 55.00%] [G loss: 0.722829]\n",
      "7800 [D loss: 0.673286, acc.: 63.50%] [G loss: 0.729144]\n",
      "7900 [D loss: 0.695348, acc.: 46.00%] [G loss: 0.724860]\n",
      "8000 [D loss: 0.664668, acc.: 57.00%] [G loss: 0.681867]\n",
      "8100 [D loss: 0.656168, acc.: 72.00%] [G loss: 0.878660]\n",
      "8200 [D loss: 0.656950, acc.: 73.00%] [G loss: 0.758250]\n",
      "8300 [D loss: 0.563249, acc.: 68.00%] [G loss: 0.892126]\n",
      "8400 [D loss: 0.631335, acc.: 78.50%] [G loss: 0.789896]\n",
      "8500 [D loss: 0.764863, acc.: 38.50%] [G loss: 0.673210]\n",
      "8600 [D loss: 0.707160, acc.: 47.00%] [G loss: 0.719615]\n",
      "8700 [D loss: 0.659491, acc.: 59.50%] [G loss: 0.815694]\n",
      "8800 [D loss: 0.686752, acc.: 44.00%] [G loss: 0.735662]\n",
      "8900 [D loss: 0.651632, acc.: 62.50%] [G loss: 0.742650]\n",
      "9000 [D loss: 0.716804, acc.: 55.00%] [G loss: 0.599234]\n",
      "9100 [D loss: 0.599356, acc.: 69.50%] [G loss: 0.917885]\n",
      "9200 [D loss: 0.743871, acc.: 43.00%] [G loss: 0.652713]\n",
      "9300 [D loss: 0.669107, acc.: 57.00%] [G loss: 0.734200]\n",
      "9400 [D loss: 0.708770, acc.: 53.50%] [G loss: 0.702528]\n",
      "9500 [D loss: 0.693639, acc.: 46.00%] [G loss: 0.695687]\n",
      "9600 [D loss: 0.718064, acc.: 52.50%] [G loss: 0.643799]\n",
      "9700 [D loss: 0.664801, acc.: 56.00%] [G loss: 0.729975]\n",
      "9800 [D loss: 0.685958, acc.: 53.00%] [G loss: 0.714331]\n",
      "9900 [D loss: 0.627595, acc.: 74.00%] [G loss: 0.841048]\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cam\\anaconda3\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1/20 synthetic data sets.\n",
      "Created 2/20 synthetic data sets.\n",
      "Created 3/20 synthetic data sets.\n",
      "Created 4/20 synthetic data sets.\n",
      "Created 5/20 synthetic data sets.\n",
      "Created 6/20 synthetic data sets.\n",
      "Created 7/20 synthetic data sets.\n",
      "Created 8/20 synthetic data sets.\n",
      "Created 9/20 synthetic data sets.\n",
      "Created 10/20 synthetic data sets.\n",
      "Created 11/20 synthetic data sets.\n",
      "Created 12/20 synthetic data sets.\n",
      "Created 13/20 synthetic data sets.\n",
      "Created 14/20 synthetic data sets.\n",
      "Created 15/20 synthetic data sets.\n",
      "Created 16/20 synthetic data sets.\n",
      "Created 17/20 synthetic data sets.\n",
      "Created 18/20 synthetic data sets.\n",
      "Created 19/20 synthetic data sets.\n",
      "Created 20/20 synthetic data sets.\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import os\n",
    "# import logging\n",
    "# import tensorflow as tf\n",
    "# from absl import logging as absl_logging\n",
    "\n",
    "# # Suppress low-level TF C++ logs (0=all, 1=INFO, 2=WARNING, 3=ERROR)\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# # Suppress Python-level TF warnings\n",
    "# tf.get_logger().setLevel(logging.ERROR)\n",
    "# logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "# absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "\n",
    "all_synthetic_datasets = {}\n",
    "\n",
    "\"\"\"iteraties en batch size hetzelfde houden.\"\"\"\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# scale data for GAN training\n",
    "scaler0 = MinMaxScaler(feature_range = (-1, 1))\n",
    "scaler0 = scaler0.fit(train_data)\n",
    "train_GAN_real = scaler0.transform(train_data)\n",
    "train_GAN_real = pd.DataFrame(train_GAN_real)\n",
    "\n",
    "# we vary the noise multipliers here, train a GAN and generate multiple synthetic data sets for each noise multiplier\n",
    "for iter, noise_multiplier in enumerate(noise_multipliers): \n",
    "  random.seed(iter)\n",
    "  np.random.seed(iter)\n",
    "  tf.random.set_seed(iter)\n",
    "\n",
    "  # train GAN on train data\n",
    "  gan_train = GAN(privacy = True)\n",
    "  gan_train.train(data = np.array(train_GAN_real), iterations=iterations, batch_size=batch_size, sample_interval=((iterations-1)/10), model_name = \"train_1.h5\")\n",
    "\n",
    "  print('Model trained.')\n",
    "\n",
    "  # list to store synthetic data sets\n",
    "  synthetic_datasets = []\n",
    "  # load model\n",
    "  generator = load_model('train_1.h5')\n",
    "  # for 20 iterations\n",
    "  for data_num in range(20):\n",
    "    # set random seeds\n",
    "    random.seed(data_num)\n",
    "    np.random.seed(data_num)\n",
    "    tf.random.set_seed(data_num)\n",
    "    # generate a synthetic data set\n",
    "    synthetic_datasets.append(generator.predict(np.random.normal(0, 1, (samples, 16)), verbose = False))\n",
    "    print('Created ' + str(data_num+1) + \"/\" + \"20 synthetic data sets.\")\n",
    "  # invert the min-max transformation\n",
    "  synthetic_datasets = [scaler0.inverse_transform(X) for X in synthetic_datasets]\n",
    "  # reshape to correct size\n",
    "  synthetic_datasets = [pd.DataFrame(X.reshape(samples, 16)) for X in synthetic_datasets]\n",
    "  # replace column names and round categorical variables\n",
    "  for X in synthetic_datasets:\n",
    "    # replace column names\n",
    "    X.columns = train_data.columns.values\n",
    "    ####################################################\n",
    "    # round the values of categorical variables, as done by Ponte et al.\n",
    "    ####################################################\n",
    "    X['treatment'] = X['treatment'].round()\n",
    "    X['conversion'] = X['conversion'].round()\n",
    "    X['visit'] = X['visit'].round()\n",
    "    X['exposure'] = X['exposure'].round()\n",
    "\n",
    "  all_synthetic_datasets[str(noise_multiplier)] = synthetic_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13fa63",
   "metadata": {},
   "source": [
    "Save synthetic data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c92900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_path = \"../../Data/Criteo/\"\n",
    "epsilons = [\"13\", \"3\", \"1\", \"05\", \"005\"]\n",
    "\n",
    "for e, item in enumerate(all_synthetic_datasets.items()):\n",
    "    sXs = item[1]\n",
    "    if not os.path.exists(synthetic_data_path):\n",
    "        os.makedirs(synthetic_data_path)\n",
    "    for i, X in enumerate(sXs):\n",
    "        X.to_csv(synthetic_data_path + \"dpgan_\" + epsilons[e] + \"_\" + str(i) + \"_\" + data_set + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57ed56",
   "metadata": {},
   "source": [
    "End of file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
