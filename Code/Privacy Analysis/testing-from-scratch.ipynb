{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34994e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from __future__ import print_function, division\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import io\n",
    "from keras.models import load_model\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, LeakyReLU\n",
    "from keras.layers import UpSampling2D, Conv2D, Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2aa9ef",
   "metadata": {},
   "source": [
    "Above works okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de181182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13436424411240122\n"
     ]
    }
   ],
   "source": [
    "# set global seeds\n",
    "seed=1\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# For working on GPUs from \"TensorFlow Determinism\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8323858",
   "metadata": {},
   "source": [
    "Above works okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255fc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, privacy):\n",
    "      self.img_rows = 1\n",
    "      self.img_cols = 9\n",
    "      self.img_shape = (self.img_cols,)\n",
    "      self.latent_dim = (9)\n",
    "      lr = 0.001\n",
    "\n",
    "      optimizer = keras.optimizers.Adam()\n",
    "      self.discriminator = self.build_discriminator()\n",
    "      self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                 optimizer=optimizer,\n",
    "                                 metrics=['accuracy'])\n",
    "      if privacy == True:\n",
    "        print(noise_multiplier)\n",
    "        print(\"using differential privacy\")\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(optimizer=DPKerasAdamOptimizer(\n",
    "            l2_norm_clip=4,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=lr),\n",
    "            loss= tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE), metrics=['accuracy'])\n",
    "\n",
    "      # Build the generator\n",
    "      self.generator = self.build_generator()\n",
    "\n",
    "      # The generator takes noise as input and generates imgs\n",
    "      z = Input(shape=(self.latent_dim,))\n",
    "      img = self.generator(z)\n",
    "\n",
    "      # For the combined model we will only train the generator\n",
    "      self.discriminator.trainable = False\n",
    "\n",
    "      # The discriminator takes generated images as input and determines validity\n",
    "      valid = self.discriminator(img)\n",
    "\n",
    "      # The combined model  (stacked generator and discriminator)\n",
    "      # Trains the generator to fool the discriminator\n",
    "      self.combined = Model(z, valid)\n",
    "      self.combined.compile(loss='binary_crossentropy', optimizer= optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "      model = Sequential()\n",
    "      model.add(Dense(self.latent_dim, input_dim=self.latent_dim))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(64, input_shape=self.img_shape))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(self.latent_dim))\n",
    "      model.add(Activation(\"tanh\"))\n",
    "\n",
    "      #model.summary()\n",
    "\n",
    "      noise = Input(shape=(self.latent_dim,))\n",
    "      img = model(noise)\n",
    "      return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, data, iterations, batch_size, sample_interval, model_name, generator_losses = [], discriminator_acc = [], correlations = [], accuracy = [], MAPD_collect = [],MSE_collect = [], MAE_collect = []):\n",
    "      # Adversarial ground truths\n",
    "      valid = np.ones((batch_size, 1))\n",
    "      fake = np.zeros((batch_size, 1))\n",
    "      corr = 0\n",
    "      MAPD = 0\n",
    "      MSE = 0\n",
    "      MAE = 0\n",
    "      #fake += 0.05 * np.random.random(fake.shape)\n",
    "      #valid += 0.05 * np.random.random(valid.shape)\n",
    "\n",
    "      for epoch in range(iterations):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            imgs = data[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise, verbose = False)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            if (epoch % 100) == 0:\n",
    "              print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "      print(\"save model\")\n",
    "      self.generator.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eef69b",
   "metadata": {},
   "source": [
    "Above works okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6211837",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_privacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_dp_sgd_privacy\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdp_optimizer_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DPKerasSGDOptimizer, DPKerasAdamOptimizer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute_dp_sgd_privacy_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_dp_sgd_privacy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_privacy'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732b1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
