{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b365d8d1",
   "metadata": {},
   "source": [
    "### Assess empirical epsilon for Anand & Lee (2022) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9751875",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\cdbale\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\cdbale\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from __future__ import print_function, division\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import io\n",
    "from keras.models import load_model\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set global seeds\n",
    "seed=1\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# For working on GPUs from \"TensorFlow Determinism\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "print(random.random())\n",
    "\n",
    "# # define utility\n",
    "# def utility(real_data, protected_data):\n",
    "#   from sklearn.linear_model import LinearRegression\n",
    "#   from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "#   reg = LinearRegression()\n",
    "#   reg.fit(np.array(real_data)[:,1:9],np.array(real_data)[:,0])\n",
    "#   reg_protect = LinearRegression()\n",
    "#   reg_protect.fit(np.array(protected_data)[:,1:9],np.array(protected_data)[:,0])\n",
    "#   MAPD = mean_absolute_percentage_error(reg.coef_, reg_protect.coef_)*100\n",
    "#   MAE = mean_absolute_error(reg.coef_, reg_protect.coef_)\n",
    "#   MSE = mean_squared_error(reg.coef_, reg_protect.coef_)\n",
    "#   return MAPD, MAE, MSE\n",
    "\n",
    "\"\"\"# Anand and lee (2022)\"\"\"\n",
    "\n",
    "## in the paper we had the following optimal settings:\n",
    "N = 1262423\n",
    "samples = N\n",
    "iterations = (100000)+1\n",
    "batch_size = 128\n",
    "\n",
    "epochs = iterations/(N/batch_size)\n",
    "print(epochs)\n",
    "\n",
    "#### Keeping these parameters as in Ponte et al.\n",
    "\n",
    "## will use N*3 as reference for desired sample sizes (i.e., 100, 3000, 10000)\n",
    "\n",
    "\n",
    "\n",
    "# therefore we want to have the same number of epochs for smaller sample sizes\n",
    "N = 10000\n",
    "samples = int(N*3)\n",
    "iterations = 1000\n",
    "batch_size = 100\n",
    "epochs = iterations/(N/batch_size)\n",
    "print(epochs)\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, privacy):\n",
    "      self.img_rows = 1\n",
    "      self.img_cols = 1\n",
    "      self.img_shape = (self.img_cols,)\n",
    "      self.latent_dim = (1)\n",
    "\n",
    "      optimizer = keras.optimizers.Adam()\n",
    "      self.discriminator = self.build_discriminator()\n",
    "      self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                 optimizer=optimizer,\n",
    "                                 metrics=['accuracy'])\n",
    "      if privacy == True:\n",
    "        print(\"using differential privacy\")\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(optimizer=DPKerasAdamOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=lr),\n",
    "            loss= tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE), metrics=['accuracy'])\n",
    "\n",
    "      # Build the generator\n",
    "      self.generator = self.build_generator()\n",
    "\n",
    "      # The generator takes noise as input and generates imgs\n",
    "      z = Input(shape=(self.latent_dim,))\n",
    "      img = self.generator(z)\n",
    "\n",
    "      # For the combined model we will only train the generator\n",
    "      self.discriminator.trainable = False\n",
    "\n",
    "      # The discriminator takes generated images as input and determines validity\n",
    "      valid = self.discriminator(img)\n",
    "\n",
    "      # The combined model  (stacked generator and discriminator)\n",
    "      # Trains the generator to fool the discriminator\n",
    "      self.combined = Model(z, valid)\n",
    "      self.combined.compile(loss='binary_crossentropy', optimizer= optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "      model = Sequential()\n",
    "      model.add(Dense(self.latent_dim, input_dim=self.latent_dim))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(1024, input_shape=self.img_shape))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(self.latent_dim))\n",
    "      model.add(Activation(\"tanh\"))\n",
    "\n",
    "      #model.summary()\n",
    "\n",
    "      noise = Input(shape=(self.latent_dim,))\n",
    "      img = model(noise)\n",
    "      return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(1024, input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, data, iterations, batch_size, model_name, generator_losses = [], discriminator_acc = [], correlations = [], accuracy = [], MAPD_col = [],MSE_col = [], MAE_col = []):\n",
    "      # Adversarial ground truths\n",
    "\n",
    "      valid = np.ones((batch_size, 1))\n",
    "      fake = np.zeros((batch_size, 1))\n",
    "      corr = 0\n",
    "      MAPD = 0\n",
    "      MSE = 0\n",
    "      MAE = 0\n",
    "      #fake += 0.05 * np.random.random(fake.shape)\n",
    "      #valid += 0.05 * np.random.random(valid.shape)\n",
    "\n",
    "      for epoch in range(iterations):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            imgs = data[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise, verbose = False)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # collect losses\n",
    "            discriminator_acc = np.append(discriminator_acc, 100*d_loss[1])\n",
    "            generator_losses = np.append(generator_losses, g_loss)\n",
    "      self.generator.save(model_name)\n",
    "              #print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f, corr: %f, MAPD: %f, MSE: %f, MAE: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss, corr, MAPD, MSE, MAE))\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "epsilons = np.array([])\n",
    "MAPD_col = np.array([])\n",
    "MAE_col = np.array([])\n",
    "MSE_col = np.array([])\n",
    "\n",
    "for iter in range(0,100):\n",
    "  random.seed(iter)\n",
    "  np.random.seed(iter)\n",
    "  tf.random.set_seed(iter)\n",
    "  churn = pd.read_csv('data.csv', sep = ',', na_values=['(NA)']).fillna(0)\n",
    "  churn = pd.DataFrame.drop_duplicates(churn)\n",
    "  churn, evaluation_outside_training = train_test_split(churn, train_size = int(samples*2/3), test_size = int(30000), stratify = churn['Churn'])\n",
    "  train_original, adversary_training = train_test_split(churn, train_size = int(samples*1/3), stratify= churn['Churn'])\n",
    "  N = len(train_original)/10\n",
    "\n",
    "  train_outcome = train_original[['Tenure']]\n",
    "  train_covariates = train_original.drop('Tenure', axis=1)\n",
    "\n",
    "  adversary_training_outcome = adversary_training[['Tenure']]\n",
    "  adversary_training_covariates = adversary_training.drop('Tenure', axis=1)\n",
    "\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "  scaler0 = MinMaxScaler(feature_range= (-1, 1))\n",
    "  scaler0 = scaler0.fit(train_outcome)\n",
    "  train_outcome = scaler0.transform(train_outcome)\n",
    "  train_outcome = pd.DataFrame(train_outcome)\n",
    "\n",
    "  print(\"start train set training\")\n",
    "  gan_train = GAN(privacy = False)\n",
    "  gan_train.train(data = np.array(train_outcome), iterations=iterations, batch_size=batch_size, model_name = \"train_anand.h5\")\n",
    "\n",
    "  # Generate a batch of new customers\n",
    "  generator = load_model('train_anand.h5', compile = True)\n",
    "  noise = np.random.normal(0, 1, (len(train_outcome), 1))\n",
    "  gen_imgs = generator.predict(noise, verbose = False)\n",
    "  gen_imgs = scaler0.inverse_transform(gen_imgs)\n",
    "  gen_imgs = gen_imgs.reshape(len(train_outcome), 1)\n",
    "  train_GAN = pd.DataFrame(gen_imgs)\n",
    "\n",
    "  # adversary has access to the model and samples another adversary_sample\n",
    "  print(\"start adversary set training\")\n",
    "\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "  scaler1 = MinMaxScaler(feature_range= (-1, 1))\n",
    "  scaler1 = scaler1.fit(adversary_training_outcome)\n",
    "  adversary_training_outcome = scaler1.transform(adversary_training_outcome)\n",
    "  adversary_training_outcome = pd.DataFrame(adversary_training_outcome)\n",
    "\n",
    "  gan_adv = GAN(privacy = False)\n",
    "  gan_adv.train(data = np.array(adversary_training_outcome), iterations=iterations, batch_size=batch_size, model_name = \"adversary_anand.h5\")\n",
    "\n",
    "  generator = load_model('adversary_anand.h5', compile = True)\n",
    "  generated_data = []\n",
    "\n",
    "  noise = np.random.normal(0, 1, (len(adversary_training_outcome), 1))\n",
    "  # Generate a batch of new images\n",
    "  gen_imgs = generator.predict(noise, verbose = False)\n",
    "  gen_imgs = scaler1.inverse_transform(gen_imgs)\n",
    "  gen_imgs = gen_imgs.reshape(len(adversary_training_outcome), 1)\n",
    "  adversary_training_GAN = pd.DataFrame(gen_imgs)\n",
    "\n",
    "  # combine one protected variable with other\n",
    "  train = pd.concat([train_covariates.reset_index(drop = True), train_GAN], axis=1)\n",
    "  adversary = pd.concat([adversary_training_covariates.reset_index(drop = True), adversary_training_GAN], axis=1)\n",
    "\n",
    "  # stap 1, 2\n",
    "  train.rename(columns = {0:'Tenure'}, inplace = True)\n",
    "  adversary.rename(columns = {0:'Tenure'}, inplace = True)\n",
    "  params = {\"bandwidth\": np.logspace(-1, 1, 20)}\n",
    "  grid_train = GridSearchCV(KernelDensity(), params, n_jobs = -1)\n",
    "  grid_train.fit(train)\n",
    "  kde_train = grid_train.best_estimator_\n",
    "\n",
    "  params = {\"bandwidth\": np.logspace(-1, 1, 20)}\n",
    "  grid = GridSearchCV(KernelDensity(), params, n_jobs = -1)\n",
    "  grid.fit(adversary)\n",
    "  kde_adversary = grid.best_estimator_\n",
    "  evaluation_outside_training = evaluation_outside_training[['Churn','Sex', 'Age', 'Contact', 'Household_size', 'Social_class', 'Income', 'Ethnicity', 'Tenure']]\n",
    "\n",
    "  # stap 3\n",
    "  density_train = kde_train.score_samples(train) # f1\n",
    "  density_adversary = kde_adversary.score_samples(train) # f2\n",
    "  #print(density_train > density_adversary)  # f1 > f2\n",
    "  TPR = sum(density_train > density_adversary)/len(density_train) # all training!\n",
    "\n",
    "  # stap 4\n",
    "  density_train_new = kde_train.score_samples(evaluation_outside_training) # f1\n",
    "  density_adversary_new = kde_adversary.score_samples(evaluation_outside_training) # f2\n",
    "  #density_train_new > density_adversary_new  # f1 > f2\n",
    "  #print(density_train_new > density_adversary_new)  # f1 > f2\n",
    "  FPR = sum(density_train_new > density_adversary_new)/len(density_train_new) # random!\n",
    "  TNR = 1 - FPR\n",
    "  FNR = 1 - TPR\n",
    "  print(\"FPR is \" + str(FPR))\n",
    "  print(\"FNR is \" + str(FNR))\n",
    "  print(\"TPR is \" + str(TPR))\n",
    "  print(\"TNR is \" + str(TNR))\n",
    "  try:\n",
    "    epsilons = np.append(epsilons,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "    print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "  except:\n",
    "    epsilons = np.append(epsilons, math.log((1 - (1/N) - FPR)/FNR))\n",
    "    print(\"empirical epsilon = \" + str(math.log((1 - (1/N) - FPR)/FNR)))\n",
    "\n",
    "  # utility\n",
    "  MAPD_train, MAE_train, MSE_train = utility(real_data = train, protected_data = train_GAN)\n",
    "  MAPD_adv, MAE_adv, MSE_adv = utility(real_data = train, protected_data = adversary_training_GAN)\n",
    "  MAPD_col = np.append(MAPD_col, ((MAPD_train+MAPD_adv)/2))\n",
    "  MAE_col = np.append(MAE_col, ((MAE_train+MAE_adv)/2))\n",
    "  MSE_col = np.append(MSE_col, ((MSE_train+MSE_adv)/2))\n",
    "  print(\"MAPD train = \" + str(MAPD_train))\n",
    "  print(\"MAPD adversary = \" + str(MAPD_adv))\n",
    "\n",
    "np.savetxt(\"epsilons_anand_30000.csv\", epsilons, delimiter=\",\")\n",
    "np.savetxt(\"MAPD_anand_30000.csv\", MAPD_col, delimiter=\",\")\n",
    "np.savetxt(\"MAE_anand_30000.csv\", MAE_col, delimiter=\",\")\n",
    "np.savetxt(\"MSE_anand_30000.csv\", MSE_col, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ponte-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
