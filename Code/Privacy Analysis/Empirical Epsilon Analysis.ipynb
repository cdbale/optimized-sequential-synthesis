{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Assessing Privacy Risk Using the Attack from Ponte et al. (2024)\n",
    "\n",
    "https://github.com/GilianPonte/whereswaldoIJRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "%matplotlib inline\n",
    "\n",
    "# Add the parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Then import\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a subset of the Criteo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../Data/Criteo/cleaned_criteo_small.gz\",\n",
    "                         compression='gzip', \n",
    "                         sep='\\,',\n",
    "                         header=0,\n",
    "                         engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates and reset index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame.drop_duplicates(train_data)\n",
    "# reset dataframe index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Attack Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are as follows:\n",
    "\n",
    "- Split the data three ways:\n",
    "    - Marketer training data\n",
    "    - Adversary training data\n",
    "    - Outside data\n",
    "- Train Marketer and Adversary synthesis models\n",
    "- Compute predictions for distribution membership for outside data and compute empirical epsilon\n",
    "- Repeat the above steps many times (100 iterations in Ponte et al. 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split into train and test sets while ensuring that the training data has an even number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_even(X, train_size, random_state=None):\n",
    "    # Split the data normally\n",
    "    X_train, X_test = train_test_split(\n",
    "        X, train_size=train_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # If train set has odd number of rows\n",
    "    if len(X_train) % 2 != 0:\n",
    "        # Move the last row from train to test\n",
    "        X_test = pd.concat([X_test, X_train[-1:]], axis=0)\n",
    "        X_train = X_train[:-1]\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for synthesis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesis steps\n",
    "# written as a list of tuples (features, model)\n",
    "synthesis_steps = [\n",
    "    ([\"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\", \"f10\", \"f11\"], 'gmm'),\n",
    "    ('treatment', 'multinomial'),\n",
    "    ('exposure', 'multinomial'),\n",
    "    ('visit', 'multinomial'),\n",
    "    ('conversion', 'multinomial')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter bounds\n",
    "param_bounds = {\n",
    "    'gmm': {\n",
    "        'num_components': (10, 50.99),\n",
    "    },\n",
    "    'multinomial': {\n",
    "        'treatment': {'C': (0.001, 3)},\n",
    "        'exposure': {'C': (0.001, 3)},\n",
    "        'visit': {'C': (0.001, 3)},\n",
    "        'conversion': {'C': (0.001, 3)}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_synthetic_datasets = 10\n",
    "num_iter_optimization = 25\n",
    "num_init_optimization = 5\n",
    "random_states = [1006, 428]\n",
    "poly_degree_mnl = 2\n",
    "poly_degree_pmse = 2\n",
    "interaction_only = True\n",
    "gmm_n_init = 3\n",
    "covariance_type = \"diag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Everything Up in a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = [300, 3000, 30000]\n",
    "num_simulations = 100\n",
    "epsilons = {}\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are synthesizing variables out of the order in which they appear in the data, you need to re-order the initial training data to match that order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order training data variables based on synthesis order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = []\n",
    "for item in synthesis_steps:\n",
    "    name = item[0]\n",
    "    if type(name) == list:\n",
    "        for i in name:\n",
    "            var_names.append(i)\n",
    "    else:\n",
    "        var_names.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[var_names]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelized Version of attack (will run simulations in parallel to speed up processing). Still loops over values of N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_single_simulation(n, i, current_data_sample, seed, random_states, **params):\n",
    "    # Unpack all parameters from params\n",
    "    (number_synthetic_datasets, synthesis_steps, param_bounds, poly_degree_mnl, \n",
    "     poly_degree_pmse, interaction_only, covariance_type, gmm_n_init, \n",
    "     num_iter_optimization, num_init_optimization) = params.values()\n",
    "    \n",
    "    # Split data\n",
    "    internal_data, external_data = train_test_split_even(\n",
    "        current_data_sample, train_size=0.67, random_state=seed+i\n",
    "    )\n",
    "    marketer_train, adversary_train = train_test_split(internal_data, train_size=0.5)\n",
    "\n",
    "    N = len(marketer_train)/10\n",
    "    \n",
    "    def optimize_models_wrapper(data_to_synthesize, random_states):\n",
    "        return [\n",
    "            optimize_models(\n",
    "                train_data=data_to_synthesize,\n",
    "                number_synthetic_datasets=number_synthetic_datasets,\n",
    "                synthesis_steps=synthesis_steps,\n",
    "                param_bounds=param_bounds,\n",
    "                poly_degree_mnl=poly_degree_mnl,\n",
    "                poly_degree_pmse=poly_degree_pmse,\n",
    "                interaction_only=interaction_only,\n",
    "                covariance_type=covariance_type,\n",
    "                gmm_n_init=gmm_n_init,\n",
    "                random_state=r,\n",
    "                num_iter_optimization=num_iter_optimization,\n",
    "                num_init_optimization=num_init_optimization\n",
    "            ) for r in random_states\n",
    "        ]\n",
    "    \n",
    "    # Parallelize model optimization\n",
    "    marketer_results = optimize_models_wrapper(marketer_train, random_states)\n",
    "    adversary_results = optimize_models_wrapper(adversary_train, random_states)\n",
    "    \n",
    "    # store best params\n",
    "    best_marketer_params = marketer_results[np.argmin([x['best_score'] for x in marketer_results])]['best_params']\n",
    "    best_adversary_params = adversary_results[np.argmin([x['best_score'] for x in adversary_results])]['best_params']\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    # train and generate with best params\n",
    "    _, marketer_sXs = perform_synthesis(train_data=marketer_train,\n",
    "                                        number_synthetic_datasets=2,\n",
    "                                        poly_degree_mnl=poly_degree_mnl,\n",
    "                                        poly_degree_pmse=poly_degree_pmse,\n",
    "                                        interaction_only=interaction_only,\n",
    "                                        covariance_type=covariance_type,\n",
    "                                        gmm_n_init=gmm_n_init,\n",
    "                                        synthesis_steps=synthesis_steps,\n",
    "                                        param_values=best_marketer_params)\n",
    "\n",
    "    _, adversary_sXs = perform_synthesis(train_data=adversary_train,\n",
    "                                        number_synthetic_datasets=2,\n",
    "                                        poly_degree_mnl=poly_degree_mnl,\n",
    "                                        poly_degree_pmse=poly_degree_pmse,\n",
    "                                        interaction_only=interaction_only,\n",
    "                                        covariance_type=covariance_type,\n",
    "                                        gmm_n_init=gmm_n_init,\n",
    "                                        synthesis_steps=synthesis_steps,\n",
    "                                        param_values=best_adversary_params)\n",
    "\n",
    "    marketer_synthetic = marketer_sXs[0]\n",
    "    adversary_synthetic = adversary_sXs[0]\n",
    "\n",
    "    ### below code borrowed from Ponte et al. (2024)\n",
    "\n",
    "    # step 1, 2 from paper\n",
    "    bw_params = {\"bandwidth\": np.logspace(-1, 1, 20)} # vary the bandwith\n",
    "    grid_marketer = GridSearchCV(KernelDensity(), bw_params, n_jobs = 1) # cross validate for bandwiths\n",
    "    grid_marketer.fit(marketer_synthetic) # estimate pdf from train data.\n",
    "    marketer_kde = grid_marketer.best_estimator_ # get best estimator\n",
    "\n",
    "    grid_adversary = GridSearchCV(KernelDensity(), bw_params, n_jobs = 1) # cross validate (CV)\n",
    "    grid_adversary.fit(adversary_synthetic) # estimate pdf from adversary data\n",
    "    adversary_kde = grid_adversary.best_estimator_ # get best estimator from CV\n",
    "\n",
    "    density_marketer = marketer_kde.score_samples(marketer_train) # score train examples from train on pdf_train\n",
    "    density_adversary = adversary_kde.score_samples(marketer_train) # score train examples from train on pdf_adversary\n",
    "    TPR = sum(density_marketer > density_adversary)/len(density_marketer) # calculate TPR\n",
    "\n",
    "    density_marketer_new = marketer_kde.score_samples(external_data) # score eval_outside examples on train density\n",
    "    density_adversary_new = adversary_kde.score_samples(external_data) # score eval_outside examples on adversary density\n",
    "    FPR = sum(density_marketer_new > density_adversary_new)/len(density_marketer_new) # calculate FPR\n",
    "    TNR = 1 - FPR\n",
    "    FNR = 1 - TPR\n",
    "    \n",
    "    risk_vals = [(1 - (1/N) - FPR)/FNR, (1 - (1/N) - FNR)/FPR]\n",
    "    \n",
    "    return math.log(risk_vals[np.argmax(risk_vals)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "def process_n(n):\n",
    "    current_data_sample, _ = train_test_split(\n",
    "        train_data, \n",
    "        train_size=n, \n",
    "        stratify=train_data['conversion'], \n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Prepare parameters\n",
    "    params = {\n",
    "        'number_synthetic_datasets': number_synthetic_datasets,\n",
    "        'synthesis_steps': synthesis_steps,\n",
    "        'param_bounds': param_bounds,\n",
    "        'poly_degree_mnl': poly_degree_mnl,\n",
    "        'poly_degree_pmse': poly_degree_pmse,\n",
    "        'interaction_only': interaction_only,\n",
    "        'covariance_type': covariance_type,\n",
    "        'gmm_n_init': gmm_n_init,\n",
    "        'num_iter_optimization': num_iter_optimization,\n",
    "        'num_init_optimization': num_init_optimization\n",
    "    }\n",
    "    \n",
    "    # Process simulations in parallel\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_single_simulation)(\n",
    "            n, i, current_data_sample, seed, random_states, **params\n",
    "        ) for i in range(num_simulations)\n",
    "    )\n",
    "    \n",
    "    return n, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run attack simulation. Save 100 empirical epsilon results for each value of n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [process_n(n) for n in num_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to the original format\n",
    "epsilons = {n: results[i][1] for i, n in enumerate(num_obs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_results = pd.DataFrame.from_dict(epsilons)\n",
    "epsilon_results.to_csv('empirical_epsilon_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter('ignore')\n",
    "\n",
    "#     for n in num_obs:\n",
    "\n",
    "#         current_data_sample, _ = train_test_split(train_data, train_size = n, stratify = train_data['conversion'], random_state=seed)\n",
    "\n",
    "#         epsilons[n] = []\n",
    "\n",
    "#         for i in range(num_simulations):\n",
    "\n",
    "#             # split data\n",
    "#             internal_data, external_data = train_test_split_even(current_data_sample, train_size=0.67, random_state=seed+i)\n",
    "#             marketer_train, adversary_train = train_test_split(internal_data, train_size=0.5)\n",
    "\n",
    "#             # marketer synthesis\n",
    "#             marketer_optimization_results = [optimize_models(train_data=marketer_train, \n",
    "#                                                              number_synthetic_datasets=number_synthetic_datasets,\n",
    "#                                                              synthesis_steps=synthesis_steps,\n",
    "#                                                              param_bounds=param_bounds,\n",
    "#                                                              poly_degree_mnl=poly_degree_mnl,\n",
    "#                                                              poly_degree_pmse=poly_degree_pmse,\n",
    "#                                                              interaction_only=interaction_only,\n",
    "#                                                              covariance_type=covariance_type,\n",
    "#                                                              gmm_n_init=gmm_n_init,\n",
    "#                                                              random_state=r,\n",
    "#                                                              num_iter_optimization=num_iter_optimization,\n",
    "#                                                              num_init_optimization=num_init_optimization) for r in random_states]\n",
    "\n",
    "#             # adversary synthesis\n",
    "#             adversary_optimization_results = [optimize_models(train_data=adversary_train, \n",
    "#                                                               number_synthetic_datasets=number_synthetic_datasets,\n",
    "#                                                               synthesis_steps=synthesis_steps,\n",
    "#                                                               param_bounds=param_bounds,\n",
    "#                                                               poly_degree_mnl=poly_degree_mnl,\n",
    "#                                                               poly_degree_pmse=poly_degree_pmse,\n",
    "#                                                               interaction_only=interaction_only,\n",
    "#                                                               covariance_type=covariance_type,\n",
    "#                                                               gmm_n_init=gmm_n_init,\n",
    "#                                                               random_state=r,\n",
    "#                                                               num_iter_optimization=num_iter_optimization,\n",
    "#                                                               num_init_optimization=num_init_optimization) for r in random_states]\n",
    "\n",
    "#             # store best params\n",
    "#             best_marketer_params = marketer_optimization_results[np.argmin([x['best_score'] for x in marketer_optimization_results])]['best_params']\n",
    "#             best_adversary_params = adversary_optimization_results[np.argmin([x['best_score'] for x in adversary_optimization_results])]['best_params']\n",
    "\n",
    "#             # train and generate with best params\n",
    "#             marketer_ratios, marketer_sXs = perform_synthesis(train_data=marketer_train,\n",
    "#                                                               number_synthetic_datasets=2,\n",
    "#                                                               poly_degree_mnl=poly_degree_mnl,\n",
    "#                                                               poly_degree_pmse=poly_degree_pmse,\n",
    "#                                                               interaction_only=interaction_only,\n",
    "#                                                               covariance_type=covariance_type,\n",
    "#                                                               gmm_n_init=gmm_n_init,\n",
    "#                                                               synthesis_steps=synthesis_steps,\n",
    "#                                                               param_values=best_marketer_params)\n",
    "\n",
    "#             adversary_ratios, adversary_sXs = perform_synthesis(train_data=adversary_train,\n",
    "#                                                                 number_synthetic_datasets=2,\n",
    "#                                                                 poly_degree_mnl=poly_degree_mnl,\n",
    "#                                                                 poly_degree_pmse=poly_degree_pmse,\n",
    "#                                                                 interaction_only=interaction_only,\n",
    "#                                                                 covariance_type=covariance_type,\n",
    "#                                                                 gmm_n_init=gmm_n_init,\n",
    "#                                                                 synthesis_steps=synthesis_steps,\n",
    "#                                                                 param_values=best_adversary_params)\n",
    "\n",
    "#             marketer_synthetic = marketer_sXs[0]\n",
    "#             adversary_synthetic = adversary_sXs[0]\n",
    "\n",
    "#             ### below code borrowed from Ponte et al. (2024)\n",
    "#             # to prevent a naive model\n",
    "#             N = len(marketer_train)/10\n",
    "\n",
    "#             # step 1, 2 from paper\n",
    "#             params = {\"bandwidth\": np.logspace(-1, 1, 20)} # vary the bandwith\n",
    "#             grid_marketer = GridSearchCV(KernelDensity(), params, n_jobs = -1) # cross validate for bandwiths\n",
    "#             grid_marketer.fit(marketer_synthetic) # estimate pdf from train data.\n",
    "#             marketer_kde = grid_marketer.best_estimator_ # get best estimator\n",
    "\n",
    "#             grid_adversary = GridSearchCV(KernelDensity(), params, n_jobs = -1) # cross validate (CV)\n",
    "#             grid_adversary.fit(adversary_synthetic) # estimate pdf from adversary data\n",
    "#             adversary_kde = grid_adversary.best_estimator_ # get best estimator from CV\n",
    "\n",
    "#             density_marketer = marketer_kde.score_samples(marketer_train) # score train examples from train on pdf_train\n",
    "#             density_adversary = adversary_kde.score_samples(marketer_train) # score train examples from train on pdf_adversary\n",
    "#             TPR = sum(density_marketer > density_adversary)/len(density_marketer) # calculate TPR\n",
    "\n",
    "#             density_marketer_new = marketer_kde.score_samples(external_data) # score eval_outside examples on train density\n",
    "#             density_adversary_new = adversary_kde.score_samples(external_data) # score eval_outside examples on adversary density\n",
    "#             FPR = sum(density_marketer_new > density_adversary_new)/len(density_marketer_new) # calculate FPR\n",
    "#             TNR = 1 - FPR\n",
    "#             FNR = 1 - TPR\n",
    "  \n",
    "#             risk_vals = [(1 - (1/N) - FPR)/FNR, (1 - (1/N) - FNR)/FPR]\n",
    "#             epsilons[n].append(math.log(risk_vals[np.argmax(risk_vals)])) # append resulting epsilon to epsilons\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
