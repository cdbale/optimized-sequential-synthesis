{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Assessing Privacy Risk Using the Attack from Ponte et al. (2024)\n",
    "\n",
    "https://github.com/GilianPonte/whereswaldoIJRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "%matplotlib inline\n",
    "\n",
    "# Add the parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Then import\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the oversampled subset of the Criteo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../Data/Criteo/cleaned_criteo_os.gz\",\n",
    "                         compression='gzip', \n",
    "                         sep='\\,',\n",
    "                         header=0,\n",
    "                         engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates and reset index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>treatment</th>\n",
       "      <th>conversion</th>\n",
       "      <th>visit</th>\n",
       "      <th>exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.609851</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>3.359763</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-4.595460</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.681893</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.365028</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-1.288207</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.822599</td>\n",
       "      <td>29.642144</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.884354</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.943716</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.885354</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.533054</td>\n",
       "      <td>-0.205137</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-12.422866</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.827510</td>\n",
       "      <td>23.570168</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.589325</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>2.934780</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-3.282109</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447093</th>\n",
       "      <td>13.680284</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.325934</td>\n",
       "      <td>-0.600592</td>\n",
       "      <td>11.029584</td>\n",
       "      <td>1.128518</td>\n",
       "      <td>-13.045950</td>\n",
       "      <td>10.885556</td>\n",
       "      <td>3.758296</td>\n",
       "      <td>44.784329</td>\n",
       "      <td>5.844038</td>\n",
       "      <td>-0.267350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447094</th>\n",
       "      <td>14.251906</td>\n",
       "      <td>13.579750</td>\n",
       "      <td>8.303577</td>\n",
       "      <td>-2.272900</td>\n",
       "      <td>12.594889</td>\n",
       "      <td>-4.636110</td>\n",
       "      <td>-19.328059</td>\n",
       "      <td>5.621479</td>\n",
       "      <td>3.755250</td>\n",
       "      <td>42.018683</td>\n",
       "      <td>6.141586</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447095</th>\n",
       "      <td>20.711370</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.290111</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-6.359690</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.813849</td>\n",
       "      <td>26.606156</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447096</th>\n",
       "      <td>23.767207</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.283185</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-3.282109</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.767224</td>\n",
       "      <td>46.714867</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447097</th>\n",
       "      <td>23.752643</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.306093</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-15.877431</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.803969</td>\n",
       "      <td>40.811160</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447098 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               f0         f1        f2        f3         f4        f5         f6         f7        f8         f9       f10       f11  treatment  conversion  visit  exposure\n",
       "0       19.609851  10.059654  8.214383  3.359763  10.280525  4.115453  -4.595460   4.833815  3.971858  13.190056  5.300375 -0.168679          1           0      0         0\n",
       "1       26.681893  10.059654  8.365028  4.679882  10.280525  4.115453  -1.288207   4.833815  3.822599  29.642144  5.300375 -0.168679          1           0      1         0\n",
       "2       12.616365  10.059654  8.884354  4.679882  10.280525  4.115453   0.294443   4.833815  3.943716  13.190056  5.300375 -0.168679          1           0      0         0\n",
       "3       17.885354  10.059654  8.533054 -0.205137  10.280525  4.115453 -12.422866   4.833815  3.827510  23.570168  5.300375 -0.168679          1           0      0         0\n",
       "4       17.589325  10.059654  8.214383  2.934780  10.280525  4.115453  -3.282109   4.833815  3.971858  13.190056  5.300375 -0.168679          1           0      0         0\n",
       "...           ...        ...       ...       ...        ...       ...        ...        ...       ...        ...       ...       ...        ...         ...    ...       ...\n",
       "447093  13.680284  10.059654  8.325934 -0.600592  11.029584  1.128518 -13.045950  10.885556  3.758296  44.784329  5.844038 -0.267350          1           1      1         1\n",
       "447094  14.251906  13.579750  8.303577 -2.272900  12.594889 -4.636110 -19.328059   5.621479  3.755250  42.018683  6.141586 -0.168679          1           1      1         1\n",
       "447095  20.711370  10.059654  8.290111  4.679882  10.280525  4.115453  -6.359690   4.833815  3.813849  26.606156  5.300375 -0.168679          1           1      1         1\n",
       "447096  23.767207  10.059654  8.283185  4.679882  10.280525  4.115453  -3.282109   4.833815  3.767224  46.714867  5.300375 -0.168679          1           1      1         0\n",
       "447097  23.752643  10.059654  8.306093  4.679882  10.280525  4.115453 -15.877431   4.833815  3.803969  40.811160  5.300375 -0.168679          1           1      1         1\n",
       "\n",
       "[447098 rows x 16 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame.drop_duplicates(train_data)\n",
    "# reset dataframe index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates, we still have approximately 10% observations with conversion = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 2.23664110e-06],\n",
       "       [9.08800959e-01, 9.11968043e-02]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_counts = np.unique(train_data.conversion, return_counts = True)\n",
    "conversion_counts/np.sum(conversion_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Attack Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are as follows:\n",
    "\n",
    "- Split the data three ways:\n",
    "    - Marketer training data\n",
    "    - Adversary training data\n",
    "    - Outside data\n",
    "- Train Marketer and Adversary synthesis models\n",
    "- Compute predictions for distribution membership for outside data and compute empirical epsilon\n",
    "- Repeat the above steps many times (100 iterations in Ponte et al. 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split into train and test sets while ensuring that the training data has an even number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_even(X, train_size, strat_var, random_state=None):\n",
    "    \n",
    "    # Split the data normally\n",
    "    # stratify based on strat var, if it exists\n",
    "    if strat_var:\n",
    "        X_train, X_test = train_test_split(\n",
    "            X, train_size=train_size, stratify=X[strat_var], random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(\n",
    "            X, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    # If train set has odd number of rows\n",
    "    if len(X_train) % 2 != 0:\n",
    "        # Move the last row from train to test\n",
    "        X_test = pd.concat([X_test, X_train[-1:]], axis=0)\n",
    "        X_train = X_train[:-1]\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate utility of synthetic data based on the mean-absolute percentage error (MAPE), mean-absolute error, and mean-squared error between regression coefficients estimated on the real and synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define utility\n",
    "def utility(real_data, protected_data):\n",
    "    \n",
    "    # import error metrics\n",
    "    from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "\n",
    "    # estimate logistic regression coefficients for real_data\n",
    "    logit_params_original = logit_params(X = real_data.drop('conversion', axis=1), y = real_data['conversion'])\n",
    "\n",
    "    # estimate logistic regression coefficients for protected_data\n",
    "    logit_params_protected = logit_params(X = protected_data.drop('conversion', axis=1), y = protected_data['conversion'])\n",
    "\n",
    "    # compute error metrics\n",
    "    MAPE = mean_absolute_percentage_error(logit_params_original, logit_params_protected)*100\n",
    "    MAE = mean_absolute_error(logit_params_original, logit_params_protected)\n",
    "    MSE = mean_squared_error(logit_params_original, logit_params_protected)\n",
    "    return MAPE, MAE, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for synthesis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some default bounds for leaf values. These will be appropriately filled in during the loop below\n",
    "\n",
    "param_bounds = {\n",
    "    'tree': {\n",
    "        'f0': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f1': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f2': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f3': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f4': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f5': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f6': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f7': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f8': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [4063, 10000]  # [min, max] bounds\n",
    "        },\n",
    "        'f9': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f10': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f11': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_synthetic_datasets = 10\n",
    "# number_synthetic_datasets = 5\n",
    "num_iter_optimization = 25\n",
    "# num_iter_optimization = 5\n",
    "num_init_optimization = 5\n",
    "random_states = [1006, 428]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Everything Up in a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = [300, 3000, 30000]\n",
    "# num_obs = [300, 400]\n",
    "num_simulations = 100\n",
    "# num_simulations = 5\n",
    "epsilons = {}\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are synthesizing variables out of the order in which they appear in the data, you need to re-order the initial training data to match that order. This is done in the loop below already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelized Version of attack (will run simulations in parallel to speed up processing). Still loops over values of N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_single_simulation(n, i, current_data_sample, strat_var, seed, random_states, **params):\n",
    "    # Unpack all parameters from params\n",
    "    (number_synthetic_datasets, param_bounds, num_iter_optimization, num_init_optimization) = params.values()\n",
    "    \n",
    "    # Split data into training data and external data (which won't be included in marketer or adversary training data)\n",
    "    internal_data, external_data = train_test_split_even(\n",
    "        current_data_sample, train_size=2/3, strat_var=strat_var, random_state=seed+i)\n",
    "\n",
    "    # define training sets for the marketer and the adversary\n",
    "    marketer_train, adversary_train = train_test_split(internal_data, train_size=0.5, stratify=internal_data[strat_var])\n",
    "\n",
    "    #### Define synthesis inputs for marketer ####\n",
    "\n",
    "    # define order of synthesis and the bounds of synthesis for the marketer\n",
    "    marketer_cols, marketer_steps, marketer_bounds = define_synthesis_steps(marketer_train, param_bounds)\n",
    "\n",
    "    # reorder the columns in the training data to match the synthesis order\n",
    "    marketer_train = marketer_train[marketer_cols]\n",
    "\n",
    "    # define the target variable for the user model\n",
    "    # we use the same target variable as stratification variable\n",
    "    # define the exogenous variables for the user model\n",
    "    marketer_exog_variables = list(marketer_train.drop(strat_var, axis=1).columns)\n",
    "\n",
    "    # parameter values from the training data\n",
    "    marketer_target_params = logit_params(X = marketer_train[marketer_exog_variables], y = marketer_train[strat_var])\n",
    "\n",
    "    #### Define synthesis inputs for adversary ####\n",
    "    \n",
    "    # define order of synthesis and the bounds of synthesis for the marketer\n",
    "    adversary_cols, adversary_steps, adversary_bounds = define_synthesis_steps(adversary_train, param_bounds)\n",
    "\n",
    "    # reorder the columns in the training data to match the synthesis order\n",
    "    adversary_train = adversary_train[adversary_cols]\n",
    "\n",
    "    # define the exogenous variables for the user model\n",
    "    adversary_exog_variables = list(adversary_train.drop(strat_var, axis=1).columns)\n",
    "\n",
    "    # parameter values from the training data\n",
    "    adversary_target_params = logit_params(X = adversary_train[adversary_exog_variables], y = adversary_train[strat_var])\n",
    "\n",
    "    N = len(marketer_train)/10\n",
    "    \n",
    "    def optimize_models_wrapper(data_to_synthesize, steps_to_follow, bounds_to_use, params_to_target, x_variables, random_states):\n",
    "        return [\n",
    "            optimize_models_with_param_target(train_data=data_to_synthesize,\n",
    "                                              number_synthetic_datasets=number_synthetic_datasets,\n",
    "                                              synthesis_steps=steps_to_follow,\n",
    "                                              param_bounds=bounds_to_use,\n",
    "                                              random_state=r,\n",
    "                                              target_params=params_to_target,\n",
    "                                              target_variable=strat_var,\n",
    "                                              exog_variables=x_variables,\n",
    "                                              n_iter=num_iter_optimization,\n",
    "                                              n_init=num_init_optimization) for r in random_states\n",
    "        ]\n",
    "    \n",
    "    # Parallelize model optimization\n",
    "    marketer_results = optimize_models_wrapper(marketer_train, marketer_steps, marketer_bounds, marketer_target_params, marketer_exog_variables, random_states)\n",
    "    adversary_results = optimize_models_wrapper(adversary_train, adversary_steps, adversary_bounds, adversary_target_params, adversary_exog_variables, random_states)\n",
    "    \n",
    "    # store best params\n",
    "    best_marketer_params = marketer_results[np.argmin([x['best_score'] for x in marketer_results])]['best_params']\n",
    "    best_adversary_params = adversary_results[np.argmin([x['best_score'] for x in adversary_results])]['best_params']\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    # train and generate with best params\n",
    "    _, marketer_sXs = perform_synthesis_with_param_target(\n",
    "        train_data=marketer_train,\n",
    "        number_synthetic_datasets=2,\n",
    "        synthesis_steps=marketer_steps,\n",
    "        target_params=marketer_target_params,\n",
    "        target_variable=strat_var,\n",
    "        exog_variables=marketer_exog_variables,\n",
    "        param_values=best_marketer_params)\n",
    "\n",
    "    _, adversary_sXs = perform_synthesis_with_param_target(\n",
    "        train_data=adversary_train,\n",
    "        number_synthetic_datasets=2,\n",
    "        synthesis_steps=adversary_steps,\n",
    "        target_params=adversary_target_params,\n",
    "        target_variable=strat_var,\n",
    "        exog_variables=adversary_exog_variables,\n",
    "        param_values=best_adversary_params)\n",
    "\n",
    "    marketer_synthetic = marketer_sXs[0]\n",
    "    adversary_synthetic = adversary_sXs[0]\n",
    "\n",
    "    # for consistent evaluation below, ensure that column orderings are the same in all data sets\n",
    "    # we haven't touched the external data yet, so we know it preserves the original column order\n",
    "    marketer_train = marketer_train[external_data.columns]\n",
    "    adversary_train = adversary_train[external_data.columns]\n",
    "    marketer_synthetic = marketer_synthetic[external_data.columns]\n",
    "    adversary_synthetic = adversary_synthetic[external_data.columns]\n",
    "\n",
    "    # evaluate utility of logistic regression coefficients\n",
    "    # the columns have been consistently reordered so coefficient orders will match\n",
    "    marketer_mape, marketer_mae, marketer_mse = utility(marketer_train, marketer_synthetic)\n",
    "    adversary_mape, adversary_mae, adversary_mse = utility(adversary_train, adversary_synthetic)\n",
    "\n",
    "    # average utility measures\n",
    "    MAPE = (marketer_mape + adversary_mape)/2\n",
    "    MAE = (marketer_mae + adversary_mae)/2\n",
    "    MSE = (marketer_mse + adversary_mse)/2\n",
    "\n",
    "    ### below code borrowed from Ponte et al. (2024)\n",
    "\n",
    "    # step 1, 2 from paper\n",
    "    bw_params = {\"bandwidth\": np.logspace(-1, 1, 20)} # vary the bandwith\n",
    "    grid_marketer = GridSearchCV(KernelDensity(), bw_params, n_jobs = 1) # cross validate for bandwiths\n",
    "    grid_marketer.fit(marketer_synthetic) # estimate pdf from train data.\n",
    "    marketer_kde = grid_marketer.best_estimator_ # get best estimator\n",
    "\n",
    "    grid_adversary = GridSearchCV(KernelDensity(), bw_params, n_jobs = 1) # cross validate (CV)\n",
    "    grid_adversary.fit(adversary_synthetic) # estimate pdf from adversary data\n",
    "    adversary_kde = grid_adversary.best_estimator_ # get best estimator from CV\n",
    "\n",
    "    density_marketer = marketer_kde.score_samples(marketer_train) # score train examples from train on pdf_train\n",
    "    density_adversary = adversary_kde.score_samples(marketer_train) # score train examples from train on pdf_adversary\n",
    "    TPR = sum(density_marketer > density_adversary)/len(density_marketer) # calculate TPR\n",
    "\n",
    "    density_marketer_new = marketer_kde.score_samples(external_data) # score eval_outside examples on train density\n",
    "    density_adversary_new = adversary_kde.score_samples(external_data) # score eval_outside examples on adversary density\n",
    "    FPR = sum(density_marketer_new > density_adversary_new)/len(density_marketer_new) # calculate FPR\n",
    "    TNR = 1 - FPR\n",
    "    FNR = 1 - TPR\n",
    "    \n",
    "    risk_vals = [(1 - (1/N) - FPR)/FNR, (1 - (1/N) - FNR)/FPR]\n",
    "    \n",
    "    return {'epsilon': math.log(risk_vals[np.argmax(risk_vals)]), 'MAPE': MAPE, 'MAE': MAE, 'MSE': MSE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "def process_n(n, strat_var):\n",
    "\n",
    "    # create current_data_sample by splitting the original data, stratified by 'conversion'\n",
    "    current_data_sample, _ = train_test_split(\n",
    "        train_data, \n",
    "        train_size=n, \n",
    "        stratify=train_data[strat_var], \n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Prepare parameters\n",
    "    params = {\n",
    "        'number_synthetic_datasets': number_synthetic_datasets,\n",
    "        'param_bounds': param_bounds,\n",
    "        'num_iter_optimization': num_iter_optimization,\n",
    "        'num_init_optimization': num_init_optimization\n",
    "    }\n",
    "    \n",
    "    # Process simulations in parallel\n",
    "    # using -5 to leave a few cores free to use computer while code is running\n",
    "    results = Parallel(n_jobs=-5, verbose=10)(\n",
    "        delayed(process_single_simulation)(\n",
    "            n, i, current_data_sample, strat_var, seed, random_states, **params\n",
    "        ) for i in range(num_simulations)\n",
    "    )\n",
    "    \n",
    "    return n, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run attack simulation. Save 100 empirical epsilon results for each value of n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-5)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=-5)]: Done   5 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-5)]: Done  16 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-5)]: Done  29 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-5)]: Done  42 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-5)]: Done  56 out of 100 | elapsed:  3.2min remaining:  2.5min\n",
      "[Parallel(n_jobs=-5)]: Done  67 out of 100 | elapsed:  3.8min remaining:  1.9min\n",
      "[Parallel(n_jobs=-5)]: Done  78 out of 100 | elapsed:  4.3min remaining:  1.2min\n",
      "[Parallel(n_jobs=-5)]: Done  89 out of 100 | elapsed:  4.9min remaining:   36.3s\n",
      "[Parallel(n_jobs=-5)]: Done 100 out of 100 | elapsed:  5.7min finished\n",
      "[Parallel(n_jobs=-5)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=-5)]: Done   5 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-5)]: Done  16 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-5)]: Done  29 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-5)]: Done  42 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-5)]: Done  56 out of 100 | elapsed:  3.6min remaining:  2.8min\n",
      "[Parallel(n_jobs=-5)]: Done  67 out of 100 | elapsed:  4.8min remaining:  2.4min\n",
      "[Parallel(n_jobs=-5)]: Done  78 out of 100 | elapsed:  4.9min remaining:  1.4min\n",
      "[Parallel(n_jobs=-5)]: Done  89 out of 100 | elapsed:  5.9min remaining:   44.0s\n",
      "[Parallel(n_jobs=-5)]: Done 100 out of 100 | elapsed:  6.3min finished\n",
      "[Parallel(n_jobs=-5)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=-5)]: Done   5 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-5)]: Done  16 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-5)]: Done  29 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-5)]: Done  42 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-5)]: Done  56 out of 100 | elapsed: 12.3min remaining:  9.6min\n",
      "[Parallel(n_jobs=-5)]: Done  67 out of 100 | elapsed: 17.8min remaining:  8.8min\n",
      "[Parallel(n_jobs=-5)]: Done  78 out of 100 | elapsed: 17.9min remaining:  5.1min\n",
      "[Parallel(n_jobs=-5)]: Done  89 out of 100 | elapsed: 22.8min remaining:  2.8min\n",
      "[Parallel(n_jobs=-5)]: Done 100 out of 100 | elapsed: 23.1min finished\n"
     ]
    }
   ],
   "source": [
    "results = [process_n(n, strat_var='conversion') for n in num_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eps_vals = [[x['epsilon'] for x in y[1]] for y in results]\n",
    "all_mape_vals = [[x['MAPE'] for x in y[1]] for y in results]\n",
    "all_mae_vals = [[x['MAE'] for x in y[1]] for y in results]\n",
    "all_mse_vals = [[x['MSE'] for x in y[1]] for y in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponte et al. display the median MSE vs. the maximum empirical privacy risk epsilon. We compute the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epsilons = [np.max(x) for x in all_eps_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_mape = [np.median(x) for x in all_mape_vals]\n",
    "med_mae = [np.median(x) for x in all_mae_vals]\n",
    "med_mse = [np.median(x) for x in all_mse_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(3268.4149815231017),\n",
       " np.float64(26.466382227022606),\n",
       " np.float64(4.628194753413067)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place results in a dataframe and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_Size</th>\n",
       "      <th>Max_Epsilon</th>\n",
       "      <th>Median_MAPE</th>\n",
       "      <th>Median_MAE</th>\n",
       "      <th>Median_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0.356675</td>\n",
       "      <td>480.830599</td>\n",
       "      <td>32.027167</td>\n",
       "      <td>3268.414982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.164755</td>\n",
       "      <td>200.978114</td>\n",
       "      <td>2.606830</td>\n",
       "      <td>26.466382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30000</td>\n",
       "      <td>0.080382</td>\n",
       "      <td>171.811690</td>\n",
       "      <td>1.072529</td>\n",
       "      <td>4.628195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample_Size  Max_Epsilon  Median_MAPE  Median_MAE   Median_MSE\n",
       "0          300     0.356675   480.830599   32.027167  3268.414982\n",
       "1         3000     0.164755   200.978114    2.606830    26.466382\n",
       "2        30000     0.080382   171.811690    1.072529     4.628195"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee_results = pd.DataFrame({'Sample_Size' : num_obs, 'Max_Epsilon': max_epsilons, 'Median_MAPE': med_mape, 'Median_MAE': med_mae, 'Median_MSE': med_mse})\n",
    "ee_results.to_csv('empirical_epsilon_results.csv', index=False)\n",
    "ee_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
