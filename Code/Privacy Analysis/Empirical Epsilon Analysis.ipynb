{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Assessing Privacy Risk Using the Attack from Ponte et al. (2024)\n",
    "\n",
    "https://github.com/GilianPonte/whereswaldoIJRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "%matplotlib inline\n",
    "\n",
    "# Add the parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Then import\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the oversampled subset of the Criteo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../Data/Criteo/cleaned_criteo_os.gz\",\n",
    "                         compression='gzip', \n",
    "                         sep='\\,',\n",
    "                         header=0,\n",
    "                         engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates and reset index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>treatment</th>\n",
       "      <th>conversion</th>\n",
       "      <th>visit</th>\n",
       "      <th>exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.609851</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>3.359763</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-4.595460</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.681893</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.365028</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-1.288207</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.822599</td>\n",
       "      <td>29.642144</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.884354</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.943716</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.885354</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.533054</td>\n",
       "      <td>-0.205137</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-12.422866</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.827510</td>\n",
       "      <td>23.570168</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.589325</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>2.934780</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-3.282109</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447093</th>\n",
       "      <td>13.680284</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.325934</td>\n",
       "      <td>-0.600592</td>\n",
       "      <td>11.029584</td>\n",
       "      <td>1.128518</td>\n",
       "      <td>-13.045950</td>\n",
       "      <td>10.885556</td>\n",
       "      <td>3.758296</td>\n",
       "      <td>44.784329</td>\n",
       "      <td>5.844038</td>\n",
       "      <td>-0.267350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447094</th>\n",
       "      <td>14.251906</td>\n",
       "      <td>13.579750</td>\n",
       "      <td>8.303577</td>\n",
       "      <td>-2.272900</td>\n",
       "      <td>12.594889</td>\n",
       "      <td>-4.636110</td>\n",
       "      <td>-19.328059</td>\n",
       "      <td>5.621479</td>\n",
       "      <td>3.755250</td>\n",
       "      <td>42.018683</td>\n",
       "      <td>6.141586</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447095</th>\n",
       "      <td>20.711370</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.290111</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-6.359690</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.813849</td>\n",
       "      <td>26.606156</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447096</th>\n",
       "      <td>23.767207</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.283185</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-3.282109</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.767224</td>\n",
       "      <td>46.714867</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447097</th>\n",
       "      <td>23.752643</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.306093</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-15.877431</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.803969</td>\n",
       "      <td>40.811160</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447098 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               f0         f1        f2        f3         f4        f5         f6         f7        f8         f9       f10       f11  treatment  conversion  visit  exposure\n",
       "0       19.609851  10.059654  8.214383  3.359763  10.280525  4.115453  -4.595460   4.833815  3.971858  13.190056  5.300375 -0.168679          1           0      0         0\n",
       "1       26.681893  10.059654  8.365028  4.679882  10.280525  4.115453  -1.288207   4.833815  3.822599  29.642144  5.300375 -0.168679          1           0      1         0\n",
       "2       12.616365  10.059654  8.884354  4.679882  10.280525  4.115453   0.294443   4.833815  3.943716  13.190056  5.300375 -0.168679          1           0      0         0\n",
       "3       17.885354  10.059654  8.533054 -0.205137  10.280525  4.115453 -12.422866   4.833815  3.827510  23.570168  5.300375 -0.168679          1           0      0         0\n",
       "4       17.589325  10.059654  8.214383  2.934780  10.280525  4.115453  -3.282109   4.833815  3.971858  13.190056  5.300375 -0.168679          1           0      0         0\n",
       "...           ...        ...       ...       ...        ...       ...        ...        ...       ...        ...       ...       ...        ...         ...    ...       ...\n",
       "447093  13.680284  10.059654  8.325934 -0.600592  11.029584  1.128518 -13.045950  10.885556  3.758296  44.784329  5.844038 -0.267350          1           1      1         1\n",
       "447094  14.251906  13.579750  8.303577 -2.272900  12.594889 -4.636110 -19.328059   5.621479  3.755250  42.018683  6.141586 -0.168679          1           1      1         1\n",
       "447095  20.711370  10.059654  8.290111  4.679882  10.280525  4.115453  -6.359690   4.833815  3.813849  26.606156  5.300375 -0.168679          1           1      1         1\n",
       "447096  23.767207  10.059654  8.283185  4.679882  10.280525  4.115453  -3.282109   4.833815  3.767224  46.714867  5.300375 -0.168679          1           1      1         0\n",
       "447097  23.752643  10.059654  8.306093  4.679882  10.280525  4.115453 -15.877431   4.833815  3.803969  40.811160  5.300375 -0.168679          1           1      1         1\n",
       "\n",
       "[447098 rows x 16 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame.drop_duplicates(train_data)\n",
    "# reset dataframe index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates, we still have approximately 10% observations with conversion = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 2.23664110e-06],\n",
       "       [9.08800959e-01, 9.11968043e-02]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_counts = np.unique(train_data.conversion, return_counts = True)\n",
    "conversion_counts/np.sum(conversion_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Attack Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are as follows:\n",
    "\n",
    "- Split the data three ways:\n",
    "    - Marketer training data\n",
    "    - Adversary training data\n",
    "    - Outside data\n",
    "- Train Marketer and Adversary synthesis models\n",
    "- Compute predictions for distribution membership for outside data and compute empirical epsilon\n",
    "- Repeat the above steps many times (100 iterations in Ponte et al. 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split into train and test sets while ensuring that the training data has an even number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_even(X, train_size, strat_var, random_state=None):\n",
    "    \n",
    "    # Split the data normally\n",
    "    # stratify based on strat var, if it exists\n",
    "    if strat_var:\n",
    "        X_train, X_test = train_test_split(\n",
    "            X, train_size=train_size, stratify=X[strat_var], random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(\n",
    "            X, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    # If train set has odd number of rows\n",
    "    if len(X_train) % 2 != 0:\n",
    "        # Move the last row from train to test\n",
    "        X_test = pd.concat([X_test, X_train[-1:]], axis=0)\n",
    "        X_train = X_train[:-1]\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for synthesis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute correlation matrix\n",
    "# correlation_matrix = train_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_synthesis_order = list(np.abs(correlation_matrix).sum()[:12].sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_synthesis_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # synthesis steps\n",
    "# # written as a list of tuples (features, model)\n",
    "\n",
    "# # synthesis steps\n",
    "# # written as a list of tuples (features, model)\n",
    "# synthesis_steps = [\n",
    "#     (['treatment', 'exposure', 'visit', 'conversion'], 'joint_categorical'),\n",
    "#     (tree_synthesis_order, 'tree'),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a list to contain all column names in order of synthesis\n",
    "# all_cols = synthesis_steps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the remaining column names (the 'f' variables)\n",
    "# [all_cols.append(synthesis_steps[1][0][i]) for i in range(len(synthesis_steps[1][0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# all_leaf_sizes = []\n",
    "# transformed_targets = []\n",
    "\n",
    "# # loop over column indices from f0 to f11, with the categorical variables placed as the first four variables\n",
    "# for id in range(4, len(all_cols)):\n",
    "    \n",
    "#     # define X variables\n",
    "#     covariates = train_data.loc[:, all_cols[:id]]\n",
    "\n",
    "#     # define Y variable\n",
    "#     target = train_data.loc[:, all_cols[id]]\n",
    "\n",
    "#     # transform target with Yeo-johnson\n",
    "#     pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "#     target = pt.fit_transform(target.to_numpy().reshape(-1, 1)).ravel()\n",
    "\n",
    "#     transformed_targets.append(target)\n",
    "\n",
    "#     # define and fit CART model\n",
    "#     # use min_samples_leaf = 1 for a tree unconstrained on leaf size\n",
    "#     tree = DecisionTreeRegressor(min_samples_leaf=1)\n",
    "#     tree.fit(covariates, target)\n",
    "\n",
    "#     # compute leaf assignments\n",
    "#     # obtain leaf assignments for training data\n",
    "#     train_leaves = tree.apply(covariates)\n",
    "#     # count the number of samples in each leaf\n",
    "#     all_leaf_sizes.append(np.unique(train_leaves, return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_leaf_sizes = pd.Series([np.min(x) for x in all_leaf_sizes], index=all_cols[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_leaf_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, j in zip(min_leaf_sizes.index, min_leaf_sizes):\n",
    "#     if j == 1:\n",
    "#         print('full tree')\n",
    "#         param_bounds['tree'][i]['min_samples_leaf'] = [10, int(train_data.shape[0]/2)]\n",
    "#     elif j == train_data.shape[0]:\n",
    "#         param_bounds['tree'][i]['min_samples_leaf'] = [1, 1]\n",
    "#         print('root node only')\n",
    "#     else:\n",
    "#         if j < int(train_data.shape[0]/2):\n",
    "#             param_bounds['tree'][i]['min_samples_leaf'] = [j, int(train_data.shape[0]/2)]\n",
    "#         else:\n",
    "#             param_bounds['tree'][i]['min_samples_leaf'] = [j, train_data.shape[0]]\n",
    "#         print('custom value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some default bounds for leaf values. These will be appropriately filled in during the loop below\n",
    "\n",
    "param_bounds = {\n",
    "    'tree': {\n",
    "        'f0': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f1': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f2': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f3': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f4': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f5': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f6': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        },\n",
    "        'f7': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f8': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [4063, 10000]  # [min, max] bounds\n",
    "        },\n",
    "        'f9': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f10': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [5, 5]  # [min, max] bounds\n",
    "        },\n",
    "        'f11': {  # Applies to all tree-synthesized variables\n",
    "            'min_samples_leaf': [10, 1000]  # [min, max] bounds\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_synthetic_datasets = 10\n",
    "num_iter_optimization = 25\n",
    "num_init_optimization = 5\n",
    "random_states = [1006, 428]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reorder the columns in the training data to match the synthesis order\n",
    "# train_data = train_data[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_variable = 'conversion'\n",
    "# exog_variables = list(train_data.drop(target_variable, axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_params = logit_params(X = train_data[exog_variables], y = train_data[target_variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Everything Up in a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = [900, 9000, 90000]\n",
    "# using 3X the desired training data size, which gets split into thirds (marketer_train, adversary_train, external_data)\n",
    "# num_obs = [900]\n",
    "num_simulations = 100\n",
    "# num_simulations = 5\n",
    "epsilons = {}\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are synthesizing variables out of the order in which they appear in the data, you need to re-order the initial training data to match that order. This is done in the loop below already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelized Version of attack (will run simulations in parallel to speed up processing). Still loops over values of N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_single_simulation(n, i, current_data_sample, strat_var, seed, random_states, **params):\n",
    "    # Unpack all parameters from params\n",
    "    (number_synthetic_datasets, param_bounds, num_iter_optimization, num_init_optimization) = params.values()\n",
    "    \n",
    "    # Split data into training data and external data (which won't be included in marketer or adversary training data)\n",
    "    internal_data, external_data = train_test_split_even(\n",
    "        current_data_sample, train_size=2/3, strat_var=strat_var, random_state=seed+i)\n",
    "\n",
    "    # define training sets for the marketer and the adversary\n",
    "    marketer_train, adversary_train = train_test_split(internal_data, train_size=0.5, stratify=internal_data[strat_var])\n",
    "\n",
    "    #### Define synthesis inputs for marketer ####\n",
    "\n",
    "    # define order of synthesis and the bounds of synthesis for the marketer\n",
    "    marketer_cols, marketer_steps, marketer_bounds = define_synthesis_steps(marketer_train, param_bounds)\n",
    "\n",
    "    # reorder the columns in the training data to match the synthesis order\n",
    "    marketer_train = marketer_train[marketer_cols]\n",
    "\n",
    "    # define the target variable for the user model\n",
    "    # we use the same target variable as stratification variable\n",
    "    # define the exogenous variables for the user model\n",
    "    marketer_exog_variables = list(marketer_train.drop(strat_var, axis=1).columns)\n",
    "\n",
    "    # parameter values from the training data\n",
    "    marketer_target_params = logit_params(X = marketer_train[marketer_exog_variables], y = marketer_train[strat_var])\n",
    "\n",
    "    #### Define synthesis inputs for adversary ####\n",
    "    \n",
    "    # define order of synthesis and the bounds of synthesis for the marketer\n",
    "    adversary_cols, adversary_steps, adversary_bounds = define_synthesis_steps(adversary_train, param_bounds)\n",
    "\n",
    "    # reorder the columns in the training data to match the synthesis order\n",
    "    adversary_train = adversary_train[adversary_cols]\n",
    "\n",
    "    # define the exogenous variables for the user model\n",
    "    adversary_exog_variables = list(adversary_train.drop(strat_var, axis=1).columns)\n",
    "\n",
    "    # parameter values from the training data\n",
    "    adversary_target_params = logit_params(X = adversary_train[adversary_exog_variables], y = adversary_train[strat_var])\n",
    "\n",
    "    N = len(marketer_train)/10\n",
    "    \n",
    "    def optimize_models_wrapper(data_to_synthesize, steps_to_follow, bounds_to_use, params_to_target, x_variables, random_states):\n",
    "        return [\n",
    "            optimize_models_with_param_target(train_data=data_to_synthesize,\n",
    "                                              number_synthetic_datasets=number_synthetic_datasets,\n",
    "                                              synthesis_steps=steps_to_follow,\n",
    "                                              param_bounds=bounds_to_use,\n",
    "                                              random_state=r,\n",
    "                                              target_params=params_to_target,\n",
    "                                              target_variable=strat_var,\n",
    "                                              exog_variables=x_variables,\n",
    "                                              n_iter=num_iter_optimization,\n",
    "                                              n_init=num_init_optimization) for r in random_states\n",
    "        ]\n",
    "    \n",
    "    # Parallelize model optimization\n",
    "    marketer_results = optimize_models_wrapper(marketer_train, marketer_steps, marketer_bounds, marketer_target_params, marketer_exog_variables, random_states)\n",
    "    adversary_results = optimize_models_wrapper(adversary_train, adversary_steps, adversary_bounds, adversary_target_params, adversary_exog_variables, random_states)\n",
    "    \n",
    "    # store best params\n",
    "    best_marketer_params = marketer_results[np.argmin([x['best_score'] for x in marketer_results])]['best_params']\n",
    "    best_adversary_params = adversary_results[np.argmin([x['best_score'] for x in adversary_results])]['best_params']\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    # train and generate with best params\n",
    "    _, marketer_sXs = perform_synthesis_with_param_target(\n",
    "        train_data=marketer_train,\n",
    "        number_synthetic_datasets=2,\n",
    "        synthesis_steps=marketer_steps,\n",
    "        target_params=marketer_target_params,\n",
    "        target_variable=strat_var,\n",
    "        exog_variables=marketer_exog_variables,\n",
    "        param_values=best_marketer_params)\n",
    "\n",
    "    _, adversary_sXs = perform_synthesis_with_param_target(\n",
    "        train_data=adversary_train,\n",
    "        number_synthetic_datasets=2,\n",
    "        synthesis_steps=adversary_steps,\n",
    "        target_params=adversary_target_params,\n",
    "        target_variable=strat_var,\n",
    "        exog_variables=adversary_exog_variables,\n",
    "        param_values=best_adversary_params)\n",
    "\n",
    "    marketer_synthetic = marketer_sXs[0]\n",
    "    adversary_synthetic = adversary_sXs[0]\n",
    "\n",
    "    # for consistent evaluation below, ensure that column orderings are the same in all data sets\n",
    "    # we haven't touched the external data yet, so we know it preserves the original column order\n",
    "    marketer_train = marketer_train[external_data.columns]\n",
    "    adversary_train = adversary_train[external_data.columns]\n",
    "    marketer_synthetic = marketer_synthetic[external_data.columns]\n",
    "    adversary_synthetic = adversary_synthetic[external_data.columns]\n",
    "\n",
    "    ### below code borrowed from Ponte et al. (2024)\n",
    "\n",
    "    # step 1, 2 from paper\n",
    "    bw_params = {\"bandwidth\": np.logspace(-1, 1, 20)} # vary the bandwith\n",
    "    grid_marketer = GridSearchCV(KernelDensity(), bw_params, n_jobs = 1) # cross validate for bandwiths\n",
    "    grid_marketer.fit(marketer_synthetic) # estimate pdf from train data.\n",
    "    marketer_kde = grid_marketer.best_estimator_ # get best estimator\n",
    "\n",
    "    grid_adversary = GridSearchCV(KernelDensity(), bw_params, n_jobs = 1) # cross validate (CV)\n",
    "    grid_adversary.fit(adversary_synthetic) # estimate pdf from adversary data\n",
    "    adversary_kde = grid_adversary.best_estimator_ # get best estimator from CV\n",
    "\n",
    "    density_marketer = marketer_kde.score_samples(marketer_train) # score train examples from train on pdf_train\n",
    "    density_adversary = adversary_kde.score_samples(marketer_train) # score train examples from train on pdf_adversary\n",
    "    TPR = sum(density_marketer > density_adversary)/len(density_marketer) # calculate TPR\n",
    "\n",
    "    density_marketer_new = marketer_kde.score_samples(external_data) # score eval_outside examples on train density\n",
    "    density_adversary_new = adversary_kde.score_samples(external_data) # score eval_outside examples on adversary density\n",
    "    FPR = sum(density_marketer_new > density_adversary_new)/len(density_marketer_new) # calculate FPR\n",
    "    TNR = 1 - FPR\n",
    "    FNR = 1 - TPR\n",
    "    \n",
    "    risk_vals = [(1 - (1/N) - FPR)/FNR, (1 - (1/N) - FNR)/FPR]\n",
    "    \n",
    "    return math.log(risk_vals[np.argmax(risk_vals)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `process_single_simulation` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = num_obs[0]\n",
    "# strat_var = 'conversion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_data_sample, _ = train_test_split(\n",
    "#         train_data, \n",
    "#         train_size=n, \n",
    "#         stratify=train_data[strat_var], \n",
    "#         random_state=seed\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         'number_synthetic_datasets': number_synthetic_datasets,\n",
    "#         'param_bounds': param_bounds,\n",
    "#         'num_iter_optimization': num_iter_optimization,\n",
    "#         'num_init_optimization': num_init_optimization\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_single_simulation(num_obs[0], \n",
    "#                           0, \n",
    "#                           current_data_sample, \n",
    "#                           strat_var, \n",
    "#                           seed,\n",
    "#                           random_states,\n",
    "#                           **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "def process_n(n, strat_var):\n",
    "\n",
    "    # create current_data_sample by splitting the original data, stratified by 'conversion'\n",
    "    current_data_sample, _ = train_test_split(\n",
    "        train_data, \n",
    "        train_size=n, \n",
    "        stratify=train_data[strat_var], \n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Prepare parameters\n",
    "    params = {\n",
    "        'number_synthetic_datasets': number_synthetic_datasets,\n",
    "        'param_bounds': param_bounds,\n",
    "        'num_iter_optimization': num_iter_optimization,\n",
    "        'num_init_optimization': num_init_optimization\n",
    "    }\n",
    "    \n",
    "    # Process simulations in parallel\n",
    "    # using -5 to leave a few cores free to use computer while code is running\n",
    "    results = Parallel(n_jobs=-5, verbose=10)(\n",
    "        delayed(process_single_simulation)(\n",
    "            n, i, current_data_sample, strat_var, seed, random_states, **params\n",
    "        ) for i in range(num_simulations)\n",
    "    )\n",
    "    \n",
    "    return n, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run attack simulation. Save 100 empirical epsilon results for each value of n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-5)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=-5)]: Done   5 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-5)]: Done  16 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-5)]: Done  29 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-5)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-5)]: Done  56 out of 100 | elapsed:  3.1min remaining:  2.4min\n",
      "[Parallel(n_jobs=-5)]: Done  67 out of 100 | elapsed:  3.5min remaining:  1.7min\n",
      "[Parallel(n_jobs=-5)]: Done  78 out of 100 | elapsed:  3.8min remaining:  1.1min\n",
      "[Parallel(n_jobs=-5)]: Done  89 out of 100 | elapsed:  4.3min remaining:   32.0s\n",
      "[Parallel(n_jobs=-5)]: Done 100 out of 100 | elapsed:  4.8min finished\n",
      "[Parallel(n_jobs=-5)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=-5)]: Done   5 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-5)]: Done  16 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-5)]: Done  29 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-5)]: Done  42 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-5)]: Done  56 out of 100 | elapsed:  4.9min remaining:  3.8min\n",
      "[Parallel(n_jobs=-5)]: Done  67 out of 100 | elapsed:  6.5min remaining:  3.2min\n",
      "[Parallel(n_jobs=-5)]: Done  78 out of 100 | elapsed:  6.9min remaining:  1.9min\n",
      "[Parallel(n_jobs=-5)]: Done  89 out of 100 | elapsed:  8.4min remaining:  1.0min\n",
      "[Parallel(n_jobs=-5)]: Done 100 out of 100 | elapsed:  8.7min finished\n",
      "[Parallel(n_jobs=-5)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=-5)]: Done   5 tasks      | elapsed: 52.1min\n",
      "[Parallel(n_jobs=-5)]: Done  16 tasks      | elapsed: 53.1min\n",
      "[Parallel(n_jobs=-5)]: Done  29 tasks      | elapsed: 100.2min\n",
      "[Parallel(n_jobs=-5)]: Done  42 tasks      | elapsed: 102.7min\n",
      "[Parallel(n_jobs=-5)]: Done  56 out of 100 | elapsed: 107.0min remaining: 84.1min\n",
      "[Parallel(n_jobs=-5)]: Done  67 out of 100 | elapsed: 153.6min remaining: 75.6min\n",
      "[Parallel(n_jobs=-5)]: Done  78 out of 100 | elapsed: 154.6min remaining: 43.6min\n",
      "[Parallel(n_jobs=-5)]: Done  89 out of 100 | elapsed: 182.7min remaining: 22.6min\n",
      "[Parallel(n_jobs=-5)]: Done 100 out of 100 | elapsed: 183.5min finished\n"
     ]
    }
   ],
   "source": [
    "results = [process_n(n, strat_var='conversion') for n in num_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to the original format\n",
    "epsilons = {n: results[i][1] for i, n in enumerate(num_obs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_results = pd.DataFrame.from_dict(epsilons)\n",
    "epsilon_results.to_csv('empirical_epsilon_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900      0.252496\n",
       "9000     0.129666\n",
       "90000    0.061733\n",
       "dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon_results.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
