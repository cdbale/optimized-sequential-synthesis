{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c595ee9",
   "metadata": {},
   "source": [
    "# Empirical Epsilon Calculation - DP-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520b219",
   "metadata": {},
   "source": [
    "Note that we use the following python modules:\n",
    "\n",
    "- tensorflow==2.15.0\n",
    "- keras==2.15.0\n",
    "- tensorflow-estimator==2.15.0\n",
    "- tensorflow-privacy==0.9.0\n",
    "- numpy==1.26.4\n",
    "- pandas==2.2.2\n",
    "- scikit-learn==1.4.2\n",
    "- scipy==1.11.4\n",
    "- absl-py==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb632c33",
   "metadata": {},
   "source": [
    "The following statments can be used to install the required python modules:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow==2.15.0\n",
    "pip install keras==2.15.0\n",
    "pip install tensorflow-estimator==2.15.0\n",
    "pip install tensorflow-privacy==0.9.0\n",
    "pip install numpy==1.26.4\n",
    "pip install pandas==2.2.2\n",
    "pip install scikit-learn==1.4.2\n",
    "pip install scipy==1.11.4\n",
    "pip install absl-py==1.4.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ca35e",
   "metadata": {},
   "source": [
    "Perform a quick check that `tensorflow`, `keras` and `tensorflow_privacy` are installed and importable. Also check versions of `NumPy`, `Pandas`, and `Scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0806f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "TF: 2.15.0\n",
      "Keras: 2.15.0\n",
      "NumPy: 1.26.4\n",
      "Pandas: 2.2.2\n",
      "Sklearn: 1.4.2\n",
      "DP optimizer OK\n"
     ]
    }
   ],
   "source": [
    "# sanity check the environment\n",
    "import tensorflow as tf, keras, numpy as np, pandas as pd, sklearn\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "\n",
    "print(\"TF:\", tf.__version__)              # 2.15.0\n",
    "print(\"Keras:\", keras.__version__)        # 2.15.0\n",
    "print(\"NumPy:\", np.__version__)           # 1.26.4\n",
    "print(\"Pandas:\", pd.__version__)          # 2.2.2\n",
    "print(\"Sklearn:\", sklearn.__version__)    # 1.4.2\n",
    "_ = DPKerasAdamOptimizer(l2_norm_clip=1.0, noise_multiplier=0.5,\n",
    "                         num_microbatches=1, learning_rate=1e-3)\n",
    "print(\"DP optimizer OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe62801",
   "metadata": {},
   "source": [
    "Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661c3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import io\n",
    "from keras.models import load_model\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, LeakyReLU\n",
    "from keras.layers import UpSampling2D, Conv2D, Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from keras.models import load_model\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902dcd2",
   "metadata": {},
   "source": [
    "Include path to where data is saved, import data, and remove duplicates. Note that we are using the oversampled version of the Criteo data (oversampled to make the prevalence of conversion about 10%) so that we have positive values in all data subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e1ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to data to synthesize\n",
    "file_path = '../../Data/Criteo/'\n",
    "# import the confidential data to synthesize\n",
    "# using 'churn' as name for compatibility with code from Ponte et al.\n",
    "churn = pd.read_csv(file_path + \"cleaned_criteo_os.gz\",\n",
    "                         compression='gzip',\n",
    "                         sep='\\,',\n",
    "                         header=0,\n",
    "                         engine='python')\n",
    "churn = pd.DataFrame.drop_duplicates(churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22715eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>treatment</th>\n",
       "      <th>conversion</th>\n",
       "      <th>visit</th>\n",
       "      <th>exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>9.051023</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.955396</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.846971</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.334264</td>\n",
       "      <td>-4.109746</td>\n",
       "      <td>11.561050</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-17.719730</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.837301</td>\n",
       "      <td>38.005580</td>\n",
       "      <td>5.900432</td>\n",
       "      <td>-0.337358</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.823921</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-10.764422</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.616365</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.943488</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>0.294443</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.920995</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.942159</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.214383</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-3.993764</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.971858</td>\n",
       "      <td>13.190056</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81543</th>\n",
       "      <td>13.680284</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.325934</td>\n",
       "      <td>-0.600592</td>\n",
       "      <td>11.029584</td>\n",
       "      <td>1.128518</td>\n",
       "      <td>-13.045950</td>\n",
       "      <td>10.885556</td>\n",
       "      <td>3.758296</td>\n",
       "      <td>44.784329</td>\n",
       "      <td>5.844038</td>\n",
       "      <td>-0.267350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81544</th>\n",
       "      <td>14.251906</td>\n",
       "      <td>13.579750</td>\n",
       "      <td>8.303577</td>\n",
       "      <td>-2.272900</td>\n",
       "      <td>12.594889</td>\n",
       "      <td>-4.636110</td>\n",
       "      <td>-19.328059</td>\n",
       "      <td>5.621479</td>\n",
       "      <td>3.755250</td>\n",
       "      <td>42.018683</td>\n",
       "      <td>6.141586</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81545</th>\n",
       "      <td>20.711370</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.290111</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-6.359690</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.813849</td>\n",
       "      <td>26.606156</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81546</th>\n",
       "      <td>23.767207</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.283185</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-3.282109</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.767224</td>\n",
       "      <td>46.714867</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81547</th>\n",
       "      <td>23.752643</td>\n",
       "      <td>10.059654</td>\n",
       "      <td>8.306093</td>\n",
       "      <td>4.679882</td>\n",
       "      <td>10.280525</td>\n",
       "      <td>4.115453</td>\n",
       "      <td>-15.877431</td>\n",
       "      <td>4.833815</td>\n",
       "      <td>3.803969</td>\n",
       "      <td>40.811160</td>\n",
       "      <td>5.300375</td>\n",
       "      <td>-0.168679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81537 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              f0         f1        f2        f3         f4        f5  \\\n",
       "0      12.616365  10.059654  9.051023  4.679882  10.280525  4.115453   \n",
       "1      12.846971  10.059654  8.334264 -4.109746  11.561050  4.115453   \n",
       "2      25.823921  10.059654  8.214383  4.679882  10.280525  4.115453   \n",
       "3      12.616365  10.059654  8.943488  4.679882  10.280525  4.115453   \n",
       "4      21.942159  10.059654  8.214383  4.679882  10.280525  4.115453   \n",
       "...          ...        ...       ...       ...        ...       ...   \n",
       "81543  13.680284  10.059654  8.325934 -0.600592  11.029584  1.128518   \n",
       "81544  14.251906  13.579750  8.303577 -2.272900  12.594889 -4.636110   \n",
       "81545  20.711370  10.059654  8.290111  4.679882  10.280525  4.115453   \n",
       "81546  23.767207  10.059654  8.283185  4.679882  10.280525  4.115453   \n",
       "81547  23.752643  10.059654  8.306093  4.679882  10.280525  4.115453   \n",
       "\n",
       "              f6         f7        f8         f9       f10       f11  \\\n",
       "0       0.294443   4.833815  3.955396  13.190056  5.300375 -0.168679   \n",
       "1     -17.719730   4.833815  3.837301  38.005580  5.900432 -0.337358   \n",
       "2     -10.764422   4.833815  3.971858  13.190056  5.300375 -0.168679   \n",
       "3       0.294443   4.833815  3.920995  13.190056  5.300375 -0.168679   \n",
       "4      -3.993764   4.833815  3.971858  13.190056  5.300375 -0.168679   \n",
       "...          ...        ...       ...        ...       ...       ...   \n",
       "81543 -13.045950  10.885556  3.758296  44.784329  5.844038 -0.267350   \n",
       "81544 -19.328059   5.621479  3.755250  42.018683  6.141586 -0.168679   \n",
       "81545  -6.359690   4.833815  3.813849  26.606156  5.300375 -0.168679   \n",
       "81546  -3.282109   4.833815  3.767224  46.714867  5.300375 -0.168679   \n",
       "81547 -15.877431   4.833815  3.803969  40.811160  5.300375 -0.168679   \n",
       "\n",
       "       treatment  conversion  visit  exposure  \n",
       "0              0           0      0         0  \n",
       "1              1           0      0         0  \n",
       "2              1           0      0         0  \n",
       "3              1           0      0         0  \n",
       "4              1           0      0         0  \n",
       "...          ...         ...    ...       ...  \n",
       "81543          1           1      1         1  \n",
       "81544          1           1      1         1  \n",
       "81545          1           1      1         1  \n",
       "81546          1           1      1         0  \n",
       "81547          1           1      1         1  \n",
       "\n",
       "[81537 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e52215",
   "metadata": {},
   "source": [
    "Create dictionary of noise multipliers used for each data set size, and a list of data sizes to use for privacy analysis. Note that the size of the `marketer`, `adversary`, and `external` data are all `training_data_size/3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98301796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary containing noise multipliers used for each data set size\n",
    "all_noise_multipliers = {\n",
    "      '300': [1.011, 2.98, 6.96, 11.85, 60],\n",
    "      '3000': [0.6785, 1.43, 3.1, 5.4, 38.5],\n",
    "      '30000': [0.502, 0.81, 1.35, 2.23, 15.5]}\n",
    "\n",
    "# defining data sizes that are 3X those of the actual training data\n",
    "# (i.e., the marketer, adversary, and external data have size training_data_size/3)\n",
    "data_sizes = [300, 3000, 30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3e160",
   "metadata": {},
   "source": [
    "Define a class for estimating a GAN with differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ee16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# GANs with differential privacy\"\"\"\n",
    "class GAN():\n",
    "    def __init__(self, privacy):\n",
    "      self.img_rows = 1\n",
    "      self.img_cols = 16\n",
    "      self.img_shape = (self.img_cols,)\n",
    "      self.latent_dim = (16)\n",
    "      lr = 0.001\n",
    "\n",
    "      optimizer = keras.optimizers.Adam()\n",
    "      self.discriminator = self.build_discriminator()\n",
    "      self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                 optimizer=optimizer,\n",
    "                                 metrics=['accuracy'])\n",
    "      if privacy == True:\n",
    "        # print(noise_multiplier)\n",
    "        # print(\"using differential privacy\")\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(optimizer=DPKerasAdamOptimizer(\n",
    "            l2_norm_clip=4,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=lr),\n",
    "            loss= tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE), metrics=['accuracy'])\n",
    "\n",
    "      # Build the generator\n",
    "      self.generator = self.build_generator()\n",
    "\n",
    "      # The generator takes noise as input and generates imgs\n",
    "      z = Input(shape=(self.latent_dim,))\n",
    "      img = self.generator(z)\n",
    "\n",
    "      # For the combined model we will only train the generator\n",
    "      self.discriminator.trainable = False\n",
    "\n",
    "      # The discriminator takes generated images as input and determines validity\n",
    "      valid = self.discriminator(img)\n",
    "\n",
    "      # The combined model  (stacked generator and discriminator)\n",
    "      # Trains the generator to fool the discriminator\n",
    "      self.combined = Model(z, valid)\n",
    "      self.combined.compile(loss='binary_crossentropy', optimizer= optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "      model = Sequential()\n",
    "      model.add(Dense(self.latent_dim, input_dim=self.latent_dim))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(64, input_shape=self.img_shape))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Dense(self.latent_dim))\n",
    "      model.add(Activation(\"tanh\"))\n",
    "\n",
    "      #model.summary()\n",
    "\n",
    "      noise = Input(shape=(self.latent_dim,))\n",
    "      img = model(noise)\n",
    "      return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, data, iterations, batch_size, sample_interval, model_name, generator_losses = [], discriminator_acc = [], correlations = [], accuracy = [], MAPD_collect = [],MSE_collect = [], MAE_collect = []):\n",
    "      # Adversarial ground truths\n",
    "      valid = np.ones((batch_size, 1))\n",
    "      fake = np.zeros((batch_size, 1))\n",
    "      corr = 0\n",
    "      MAPD = 0\n",
    "      MSE = 0\n",
    "      MAE = 0\n",
    "      #fake += 0.05 * np.random.random(fake.shape)\n",
    "      #valid += 0.05 * np.random.random(valid.shape)\n",
    "\n",
    "      for epoch in range(iterations):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            imgs = data[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise, verbose = False)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            if (epoch % 100) == 0:\n",
    "              print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "      self.generator.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0859b17",
   "metadata": {},
   "source": [
    "Utility function (adapted from Ponte et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a4b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(real_data, protected_data):\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "  reg = LogisticRegression(penalty=None, max_iter=1000, solver='lbfgs')\n",
    "  reg.fit(X = real_data.drop('conversion', axis=1), y = real_data['conversion'])\n",
    "  reg_protect = LogisticRegression(penalty=None, max_iter=1000, solver='lbfgs')\n",
    "  reg_protect.fit(protected_data.drop('conversion', axis=1), y = protected_data['conversion'])\n",
    "\n",
    "  conf_coefs = np.concatenate([[reg.intercept_[0]], reg.coef_.flatten()])\n",
    "  prot_coefs = np.concatenate([[reg_protect.intercept_[0]], reg_protect.coef_.flatten()])\n",
    "\n",
    "  MAPD = mean_absolute_percentage_error(conf_coefs, prot_coefs)*100\n",
    "  MAE = mean_absolute_error(conf_coefs, prot_coefs)\n",
    "  MSE = mean_squared_error(conf_coefs, prot_coefs)\n",
    "  \n",
    "  return MAPD, MAE, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d65b8f",
   "metadata": {},
   "source": [
    "#### Results for $N = 300$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebce59",
   "metadata": {},
   "source": [
    "Including old version of code. We believe to have found some logical errors in the code of Ponte et al., and have addressed these in the new version. The errors were as follows:\n",
    "\n",
    "1. Inside the inner loop over `noise_multipliers`, you overwrite `train_GAN` and `adversary_training_GAN` with newly generated synthetic data:\n",
    "    - Initial values before the loop:\n",
    "        - `train_GAN = scaler0.transform(train)` and `adversary_training_GAN = scaler1.transform(adversary_training)` (both derived from confidential data).\n",
    "    - After training and sampling for the first noise level, you do:\n",
    "        - `train_GAN = pd.DataFrame(gen_imgs...)` and `adversary_training_GAN = pd.DataFrame(gen_imgs...)`.\n",
    "    - On the next noise level in the same outer iter, the GANs are trained on these synthetic datasets instead of the original confidential data.\n",
    "    - Result: for noise_multipliers[1] and beyond (within the same iter), GANs are indeed fit to synthetic data produced by the previous noise level’s GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe27838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import os\n",
    "# import logging\n",
    "# import tensorflow as tf\n",
    "# from absl import logging as absl_logging\n",
    "\n",
    "# # Suppress low-level TF C++ logs (0=all, 1=INFO, 2=WARNING, 3=ERROR)\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# # Suppress Python-level TF warnings\n",
    "# tf.get_logger().setLevel(logging.ERROR)\n",
    "# logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "# absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "\n",
    "# noise_multipliers = all_noise_multipliers['300']\n",
    "# samples = int(data_sizes[0])\n",
    "\n",
    "# \"\"\"iteraties en batch size hetzelfde houden.\"\"\"\n",
    "# random.seed(1)\n",
    "# np.random.seed(1)\n",
    "# tf.random.set_seed(1)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# epsilons_13 = np.array([])\n",
    "# MAPD_col_13 = np.array([])\n",
    "# MAE_col_13 = np.array([])\n",
    "# MSE_col_13 = np.array([])\n",
    "\n",
    "# epsilons_3 = np.array([])\n",
    "# MAPD_col_3 = np.array([])\n",
    "# MAE_col_3 = np.array([])\n",
    "# MSE_col_3 = np.array([])\n",
    "\n",
    "# epsilons_1 = np.array([])\n",
    "# MAPD_col_1 = np.array([])\n",
    "# MAE_col_1 = np.array([])\n",
    "# MSE_col_1 = np.array([])\n",
    "\n",
    "# epsilons_05 = np.array([])\n",
    "# MAPD_col_05 = np.array([])\n",
    "# MAE_col_05 = np.array([])\n",
    "# MSE_col_05 = np.array([])\n",
    "\n",
    "# epsilons_005 = np.array([])\n",
    "# MAPD_col_005 = np.array([])\n",
    "# MAE_col_005 = np.array([])\n",
    "# MSE_col_005 = np.array([])\n",
    "\n",
    "# epsilons_001 = np.array([])\n",
    "# MAPD_col_001 = np.array([])\n",
    "# MAE_col_001 = np.array([])\n",
    "# MSE_col_001 = np.array([])\n",
    "\n",
    "# TPR_col = np.array([])\n",
    "# FPR_col = np.array([])\n",
    "# TNR_col = np.array([])\n",
    "# FNR_col = np.array([])\n",
    "\n",
    "# for iter in range(100):\n",
    "#   random.seed(iter)\n",
    "#   np.random.seed(iter)\n",
    "#   tf.random.set_seed(iter)\n",
    "#   print(\"iteration is \" + str(iter))\n",
    "#   sampled_churn = churn.sample(frac = 1, random_state = iter)\n",
    "#   both_train, evaluation_outside_training = train_test_split(sampled_churn, train_size = int(samples*2/3), test_size = int(samples*1/3), stratify = sampled_churn['conversion'])\n",
    "#   train, adversary_training = train_test_split(both_train, train_size = int(samples*1/3), stratify=both_train['conversion'])\n",
    "\n",
    "#   scaler0 = MinMaxScaler(feature_range= (-1, 1))\n",
    "#   scaler0 = scaler0.fit(train)\n",
    "#   train_GAN = scaler0.transform(train)\n",
    "#   train_GAN = pd.DataFrame(train_GAN)\n",
    "\n",
    "#   scaler1 = MinMaxScaler(feature_range= (-1, 1))\n",
    "#   scaler1 = scaler1.fit(adversary_training)\n",
    "#   adversary_training_GAN = scaler1.transform(adversary_training)\n",
    "#   adversary_training_GAN = pd.DataFrame(adversary_training_GAN)\n",
    "\n",
    "#   for noise in noise_multipliers: # we vary the noise multipliers here\n",
    "#     random.seed(iter)\n",
    "#     np.random.seed(iter)\n",
    "#     tf.random.set_seed(iter)\n",
    "\n",
    "#     # setting epsilon\n",
    "#     N = len(train)\n",
    "#     batch_size = 100\n",
    "#     ### change for different data sizes\n",
    "#     iterations = 10\n",
    "#     epochs = iterations/(N/batch_size) # should be 10\n",
    "\n",
    "#     # the noise_multiplier is not directly passed to the GAN, but the GAN code reads it from the global environment\n",
    "#     noise_multiplier = noise\n",
    "#     l2_norm_clip = 4 # see paper in validation section.\n",
    "#     delta= 1/N # should be 1/N\n",
    "#     theor_epsilon = compute_dp_sgd_privacy(N, batch_size, noise_multiplier,\n",
    "#                           epochs, delta) # calculate the theoretical bound of epsilon\n",
    "#     N = len(train)/10 # to prevent naive model\n",
    "#     num_microbatches = batch_size # see validation section paper.\n",
    "#     # print(\"theoretical epsilon = \" + str(round(theor_epsilon[0],2))) # print epsilon\n",
    "\n",
    "#     # train GAN on train data\n",
    "#     gan_train = GAN(privacy = True)\n",
    "#     gan_train.train(data = np.array(train_GAN), iterations=iterations, batch_size=batch_size, sample_interval=((iterations-1)/10), model_name = \"train_1.h5\")\n",
    "\n",
    "#     # Generate a batch of new customers\n",
    "#     generator = load_model('train_1.h5')\n",
    "#     noise = np.random.normal(0, 1, (int(samples*1/3), 16))\n",
    "#     gen_imgs = generator.predict(noise, verbose = False)\n",
    "#     gen_imgs = scaler0.inverse_transform(gen_imgs)\n",
    "#     train_GAN = pd.DataFrame(gen_imgs.reshape(int(samples*1/3), 16))\n",
    "#     train_GAN.columns = train.columns.values\n",
    "\n",
    "#     ####################################################\n",
    "#     # round the values of categorical variables, as done by Ponte et al.\n",
    "#     ####################################################\n",
    "#     train_GAN['treatment'] = train_GAN['treatment'].round()\n",
    "#     train_GAN['conversion'] = train_GAN['conversion'].round()\n",
    "#     train_GAN['visit'] = train_GAN['visit'].round()\n",
    "#     train_GAN['exposure'] = train_GAN['exposure'].round()\n",
    "    \n",
    "#     # adversary has access to the model and samples another adversary_sample\n",
    "#     gan_adv = GAN(privacy = True)\n",
    "#     gan_adv.train(data = np.array(adversary_training_GAN), iterations=iterations, batch_size=batch_size, sample_interval=((iterations-1)/10), model_name = \"adversary_1.h5\")\n",
    "\n",
    "#     # Generate a batch of new images\n",
    "#     generator = load_model('adversary_1.h5')\n",
    "#     noise = np.random.normal(0, 1, (int(samples*1/3), 16))\n",
    "#     gen_imgs = generator.predict(noise, verbose = False)\n",
    "#     gen_imgs = scaler1.inverse_transform(gen_imgs)\n",
    "#     adversary_training_GAN = pd.DataFrame(gen_imgs.reshape(int(samples*1/3), 16))\n",
    "#     adversary_training_GAN.columns = adversary_training.columns.values\n",
    "\n",
    "#     ####################################################\n",
    "#     # round the values of categorical variables, as done by Ponte et al.\n",
    "#     ####################################################\n",
    "#     adversary_training_GAN['treatment'] = adversary_training_GAN['treatment'].round()\n",
    "#     adversary_training_GAN['conversion'] = adversary_training_GAN['conversion'].round()\n",
    "#     adversary_training_GAN['visit'] = adversary_training_GAN['visit'].round()\n",
    "#     adversary_training_GAN['exposure'] = adversary_training_GAN['exposure'].round()\n",
    "\n",
    "#     # stap 1, 2\n",
    "#     params = {\"bandwidth\": np.logspace(-1, 1, 20)}\n",
    "#     grid_train = GridSearchCV(KernelDensity(), params, n_jobs = -1)\n",
    "#     grid_train.fit(train_GAN)\n",
    "#     # print(grid_train.best_estimator_)\n",
    "#     kde_train = grid_train.best_estimator_\n",
    "\n",
    "#     grid = GridSearchCV(KernelDensity(), params, n_jobs = -1)\n",
    "#     grid.fit(adversary_training_GAN)\n",
    "#     # print(grid.best_estimator_)\n",
    "#     kde_adversary = grid.best_estimator_\n",
    "\n",
    "#     # stap 3\n",
    "#     density_train = kde_train.score_samples(train)\n",
    "#     density_adversary = kde_adversary.score_samples(train)\n",
    "#     TPR = sum(density_train > density_adversary)/len(density_train)\n",
    "\n",
    "#     # stap 4\n",
    "#     density_train_new = kde_train.score_samples(evaluation_outside_training)\n",
    "#     density_adversary_new = kde_adversary.score_samples(evaluation_outside_training)\n",
    "#     FPR = sum(density_train_new > density_adversary_new)/len(density_train_new)\n",
    "#     TNR = 1 - FPR\n",
    "#     FNR = 1 - TPR\n",
    "#     print(\"FPR is \" + str(FPR))\n",
    "#     print(\"FNR is \" + str(FNR))\n",
    "#     print(\"TPR is \" + str(TPR))\n",
    "#     print(\"TNR is \" + str(TNR))\n",
    "\n",
    "#     TPR_col = np.append(TPR_col, TPR)\n",
    "#     FPR_col = np.append(FPR_col, FPR)\n",
    "#     TNR_col = np.append(TNR_col, TNR)\n",
    "#     FNR_col = np.append(FNR_col, FNR)\n",
    "\n",
    "#     # utility\n",
    "#     MAPD_train, MAE_train, MSE_train = utility(real_data = train, protected_data = train_GAN)\n",
    "#     MAPD_adv, MAE_adv, MSE_adv = utility(real_data = train, protected_data = adversary_training_GAN)\n",
    "#     MAPD = (MAPD_train+MAPD_adv)/2\n",
    "#     MAE = (MAE_train+MAE_adv)/2\n",
    "#     MSE = (MSE_train+MSE_adv)/2\n",
    "#     # print(\"MAPD\" + str(MAPD))\n",
    "\n",
    "#     ## to save the results per epsilon (a bit lazy admittedly).\n",
    "#     if noise_multiplier == noise_multipliers[0]:\n",
    "#       try:\n",
    "#         epsilons_13 = np.append(epsilons_13,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "#         print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "#         MAPD_col_13 = np.append(MAPD_col_13, MAPD)\n",
    "#         MAE_col_13 = np.append(MAE_col_13, MAE)\n",
    "#         MSE_col_13 = np.append(MSE_col_13, MSE)\n",
    "#       except:\n",
    "#         print(\"undefined privacy risk\")\n",
    "#         epsilons_13 = np.append(epsilons_13, 0)\n",
    "#         print(\"empirical epsilon = \" + str(0))\n",
    "#         MAPD_col_13 = np.append(MAPD_col_13, MAPD)\n",
    "#         MAE_col_13 = np.append(MAE_col_13, MAE)\n",
    "#         MSE_col_13 = np.append(MSE_col_13, MSE)\n",
    "\n",
    "#     if noise_multiplier == noise_multipliers[1]:\n",
    "#       try:\n",
    "#         epsilons_3 = np.append(epsilons_3,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "#         print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "#         MAPD_col_3 = np.append(MAPD_col_3, MAPD)\n",
    "#         MAE_col_3 = np.append(MAE_col_3, MAE)\n",
    "#         MSE_col_3 = np.append(MSE_col_3, MSE)\n",
    "#       except:\n",
    "#         print(\"undefined privacy risk\")\n",
    "#         epsilons_3 = np.append(epsilons_3, 0)\n",
    "#         print(\"empirical epsilon = \" + str(0))\n",
    "#         MAPD_col_3 = np.append(MAPD_col_3, MAPD)\n",
    "#         MAE_col_3 = np.append(MAE_col_3, MAE)\n",
    "#         MSE_col_3 = np.append(MSE_col_3, MSE)\n",
    "\n",
    "#     if noise_multiplier == noise_multipliers[2]:\n",
    "#       try:\n",
    "#         epsilons_1 = np.append(epsilons_1,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "#         print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "#         MAPD_col_1 = np.append(MAPD_col_1, MAPD)\n",
    "#         MAE_col_1 = np.append(MAE_col_1, MAE)\n",
    "#         MSE_col_1 = np.append(MSE_col_1, MSE)\n",
    "#       except:\n",
    "#         print(\"undefined privacy risk\")\n",
    "#         epsilons_1 = np.append(epsilons_1, 0)\n",
    "#         print(\"empirical epsilon = \" + str(0))\n",
    "#         MAPD_col_1 = np.append(MAPD_col_1, MAPD)\n",
    "#         MAE_col_1 = np.append(MAE_col_1, MAE)\n",
    "#         MSE_col_1 = np.append(MSE_col_1, MSE)\n",
    "\n",
    "#     if noise_multiplier == noise_multipliers[3]:\n",
    "#       try:\n",
    "#         epsilons_05 = np.append(epsilons_05,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "#         print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "#         MAPD_col_05 = np.append(MAPD_col_05, MAPD)\n",
    "#         MAE_col_05 = np.append(MAE_col_05, MAE)\n",
    "#         MSE_col_05 = np.append(MSE_col_05, MSE)\n",
    "#       except:\n",
    "#         print(\"undefined privacy risk\")\n",
    "#         epsilons_05 = np.append(epsilons_05, 0)\n",
    "#         print(\"empirical epsilon = \" + str(0))\n",
    "#         MAPD_col_05 = np.append(MAPD_col_05, MAPD)\n",
    "#         MAE_col_05 = np.append(MAE_col_05, MAE)\n",
    "#         MSE_col_05 = np.append(MSE_col_05, MSE)\n",
    "\n",
    "#     if noise_multiplier == noise_multipliers[4]:\n",
    "#       try:\n",
    "#         epsilons_005 = np.append(epsilons_005,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "#         print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "#         MAPD_col_005 = np.append(MAPD_col_005, MAPD)\n",
    "#         MAE_col_005 = np.append(MAE_col_005, MAE)\n",
    "#         MSE_col_005 = np.append(MSE_col_005, MSE)\n",
    "#       except:\n",
    "#         print(\"undefined privacy risk\")\n",
    "#         epsilons_005 = np.append(epsilons_005, 0)\n",
    "#         print(\"empirical epsilon = \" + str(0))\n",
    "#         MAPD_col_005 = np.append(MAPD_col_005, MAPD)\n",
    "#         MAE_col_005 = np.append(MAE_col_005, MAE)\n",
    "#         MSE_col_005 = np.append(MSE_col_005, MSE)\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(elapsed_time)\n",
    "\n",
    "# epsilons_13.mean()\n",
    "\n",
    "# epsilons_3.mean()\n",
    "\n",
    "# epsilons_1.mean()\n",
    "\n",
    "# epsilons_05.mean()\n",
    "\n",
    "# epsilons_005.mean()\n",
    "\n",
    "# np.savetxt(\"epsilons_13_\" + str(samples) + \".csv\", epsilons_13, delimiter=\",\")\n",
    "# np.savetxt(\"MAPD_13_\" + str(samples) + \".csv\", MAPD_col_13, delimiter=\",\")\n",
    "# np.savetxt(\"MAE_13_\" + str(samples) + \".csv\", MAE_col_13, delimiter=\",\")\n",
    "# np.savetxt(\"MSE_13_\" + str(samples) + \".csv\", MSE_col_13, delimiter=\",\")\n",
    "\n",
    "# np.savetxt(\"epsilons_3_\" + str(samples) + \".csv\", epsilons_3, delimiter=\",\")\n",
    "# np.savetxt(\"MAPD_3_\" + str(samples) + \".csv\", MAPD_col_3, delimiter=\",\")\n",
    "# np.savetxt(\"MAE_3_\" + str(samples) + \".csv\", MAE_col_3, delimiter=\",\")\n",
    "# np.savetxt(\"MSE_3_\" + str(samples) + \".csv\", MSE_col_3, delimiter=\",\")\n",
    "\n",
    "# np.savetxt(\"epsilons_1_\" + str(samples) + \".csv\", epsilons_1, delimiter=\",\")\n",
    "# np.savetxt(\"MAPD_1_\" + str(samples) + \".csv\", MAPD_col_1, delimiter=\",\")\n",
    "# np.savetxt(\"MAE_1_\" + str(samples) + \".csv\", MAE_col_1, delimiter=\",\")\n",
    "# np.savetxt(\"MSE_1_\" + str(samples) + \".csv\", MSE_col_1, delimiter=\",\")\n",
    "\n",
    "# np.savetxt(\"epsilons_05_\" + str(samples) + \".csv\", epsilons_05, delimiter=\",\")\n",
    "# np.savetxt(\"MAPD_05_\" + str(samples) + \".csv\", MAPD_col_05, delimiter=\",\")\n",
    "# np.savetxt(\"MAE_05_\" + str(samples) + \".csv\", MAE_col_05, delimiter=\",\")\n",
    "# np.savetxt(\"MSE_05_\" + str(samples) + \".csv\", MSE_col_05, delimiter=\",\")\n",
    "\n",
    "# np.savetxt(\"epsilons_005_\" + str(samples) + \".csv\", epsilons_005, delimiter=\",\")\n",
    "# np.savetxt(\"MAPD_005_\" + str(samples) + \".csv\", MAPD_col_005, delimiter=\",\")\n",
    "# np.savetxt(\"MAE_005_\" + str(samples) + \".csv\", MAE_col_005, delimiter=\",\")\n",
    "# np.savetxt(\"MSE_005_\" + str(samples) + \".csv\", MSE_col_005, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946a6e9",
   "metadata": {},
   "source": [
    "Code with our fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0d0218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration is 0\n",
      "0 [D loss: 0.692554, acc.: 60.00%] [G loss: 0.806731]\n",
      "0 [D loss: 0.539967, acc.: 77.00%] [G loss: 0.711346]\n",
      "FPR is 0.08\n",
      "FNR is 0.98\n",
      "TPR is 0.02\n",
      "TNR is 0.92\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.692078, acc.: 60.00%] [G loss: 0.804765]\n",
      "0 [D loss: 0.539446, acc.: 77.00%] [G loss: 0.710956]\n",
      "FPR is 0.07\n",
      "FNR is 0.98\n",
      "TPR is 0.02\n",
      "TNR is 0.9299999999999999\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.691724, acc.: 60.00%] [G loss: 0.804498]\n",
      "0 [D loss: 0.539045, acc.: 78.00%] [G loss: 0.711384]\n",
      "FPR is 0.07\n",
      "FNR is 0.97\n",
      "TPR is 0.03\n",
      "TNR is 0.9299999999999999\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.691754, acc.: 60.00%] [G loss: 0.804059]\n",
      "0 [D loss: 0.538864, acc.: 78.00%] [G loss: 0.711549]\n",
      "FPR is 0.07\n",
      "FNR is 0.98\n",
      "TPR is 0.02\n",
      "TNR is 0.9299999999999999\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.691563, acc.: 60.00%] [G loss: 0.803883]\n",
      "0 [D loss: 0.538742, acc.: 78.50%] [G loss: 0.711494]\n",
      "FPR is 0.07\n",
      "FNR is 0.98\n",
      "TPR is 0.02\n",
      "TNR is 0.9299999999999999\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "iteration is 1\n",
      "0 [D loss: 0.746940, acc.: 30.50%] [G loss: 0.681006]\n",
      "0 [D loss: 0.733026, acc.: 40.50%] [G loss: 0.663412]\n",
      "FPR is 0.29\n",
      "FNR is 0.6699999999999999\n",
      "TPR is 0.33\n",
      "TNR is 0.71\n",
      "empirical epsilon = -0.09381875521765448\n",
      "0 [D loss: 0.747291, acc.: 30.50%] [G loss: 0.678517]\n",
      "0 [D loss: 0.732314, acc.: 40.00%] [G loss: 0.661952]\n",
      "FPR is 0.28\n",
      "FNR is 0.6699999999999999\n",
      "TPR is 0.33\n",
      "TNR is 0.72\n",
      "empirical epsilon = -0.07755823434587437\n",
      "0 [D loss: 0.746885, acc.: 30.50%] [G loss: 0.678406]\n",
      "0 [D loss: 0.732062, acc.: 40.00%] [G loss: 0.661002]\n",
      "FPR is 0.26\n",
      "FNR is 0.6599999999999999\n",
      "TPR is 0.34\n",
      "TNR is 0.74\n",
      "empirical epsilon = -0.030771658666753545\n",
      "0 [D loss: 0.746876, acc.: 30.50%] [G loss: 0.678049]\n",
      "0 [D loss: 0.731878, acc.: 40.00%] [G loss: 0.660818]\n",
      "FPR is 0.25\n",
      "FNR is 0.65\n",
      "TPR is 0.35\n",
      "TNR is 0.75\n",
      "empirical epsilon = 0.0\n",
      "0 [D loss: 0.746767, acc.: 30.50%] [G loss: 0.678024]\n",
      "0 [D loss: 0.731602, acc.: 40.00%] [G loss: 0.660801]\n",
      "FPR is 0.24\n",
      "FNR is 0.6599999999999999\n",
      "TPR is 0.34\n",
      "TNR is 0.76\n",
      "empirical epsilon = 4.440892098500625e-16\n",
      "iteration is 2\n",
      "0 [D loss: 0.552790, acc.: 70.00%] [G loss: 0.714122]\n",
      "0 [D loss: 0.731260, acc.: 42.00%] [G loss: 0.717195]\n",
      "FPR is 0.46\n",
      "FNR is 0.56\n",
      "TPR is 0.44\n",
      "TNR is 0.54\n",
      "empirical epsilon = -0.2411620568168881\n",
      "0 [D loss: 0.552568, acc.: 70.50%] [G loss: 0.712086]\n",
      "0 [D loss: 0.731472, acc.: 41.50%] [G loss: 0.714049]\n",
      "FPR is 0.46\n",
      "FNR is 0.5700000000000001\n",
      "TPR is 0.43\n",
      "TNR is 0.54\n",
      "empirical epsilon = -0.258861633916289\n",
      "0 [D loss: 0.552420, acc.: 70.50%] [G loss: 0.711488]\n",
      "0 [D loss: 0.731520, acc.: 41.50%] [G loss: 0.712937]\n",
      "FPR is 0.45\n",
      "FNR is 0.56\n",
      "TPR is 0.44\n",
      "TNR is 0.55\n",
      "empirical epsilon = -0.21868920096482958\n",
      "0 [D loss: 0.552411, acc.: 70.50%] [G loss: 0.711193]\n",
      "0 [D loss: 0.731199, acc.: 42.50%] [G loss: 0.713245]\n",
      "FPR is 0.45\n",
      "FNR is 0.5700000000000001\n",
      "TPR is 0.43\n",
      "TNR is 0.55\n",
      "empirical epsilon = -0.23638877806423053\n",
      "0 [D loss: 0.551924, acc.: 71.00%] [G loss: 0.711848]\n",
      "0 [D loss: 0.731086, acc.: 42.50%] [G loss: 0.713009]\n",
      "FPR is 0.45\n",
      "FNR is 0.5700000000000001\n",
      "TPR is 0.43\n",
      "TNR is 0.55\n",
      "empirical epsilon = -0.23638877806423053\n",
      "iteration is 3\n",
      "0 [D loss: 0.821189, acc.: 42.00%] [G loss: 0.774213]\n",
      "0 [D loss: 0.667420, acc.: 58.00%] [G loss: 0.667384]\n",
      "FPR is 0.27\n",
      "FNR is 0.5700000000000001\n",
      "TPR is 0.43\n",
      "TNR is 0.73\n",
      "empirical epsilon = 0.20067069546215105\n",
      "0 [D loss: 0.820289, acc.: 42.00%] [G loss: 0.775108]\n",
      "0 [D loss: 0.665544, acc.: 60.00%] [G loss: 0.670233]\n",
      "FPR is 0.27\n",
      "FNR is 0.54\n",
      "TPR is 0.46\n",
      "TNR is 0.73\n",
      "empirical epsilon = 0.28768207245178085\n",
      "0 [D loss: 0.819763, acc.: 42.00%] [G loss: 0.775740]\n",
      "0 [D loss: 0.664524, acc.: 61.00%] [G loss: 0.671877]\n",
      "FPR is 0.28\n",
      "FNR is 0.5900000000000001\n",
      "TPR is 0.41\n",
      "TNR is 0.72\n",
      "empirical epsilon = 0.10178269430994198\n",
      "0 [D loss: 0.819454, acc.: 42.00%] [G loss: 0.776249]\n",
      "0 [D loss: 0.664162, acc.: 61.50%] [G loss: 0.672275]\n",
      "FPR is 0.28\n",
      "FNR is 0.5800000000000001\n",
      "TPR is 0.42\n",
      "TNR is 0.72\n",
      "empirical epsilon = 0.13353139262452238\n",
      "0 [D loss: 0.819246, acc.: 42.00%] [G loss: 0.776592]\n",
      "0 [D loss: 0.664191, acc.: 61.50%] [G loss: 0.671988]\n",
      "FPR is 0.29\n",
      "FNR is 0.5800000000000001\n",
      "TPR is 0.42\n",
      "TNR is 0.71\n",
      "empirical epsilon = 0.09844007281325251\n",
      "iteration is 4\n",
      "0 [D loss: 0.568820, acc.: 53.00%] [G loss: 0.554759]\n",
      "0 [D loss: 0.835505, acc.: 41.00%] [G loss: 0.701451]\n",
      "FPR is 0.78\n",
      "FNR is 0.25\n",
      "TPR is 0.75\n",
      "TNR is 0.21999999999999997\n",
      "empirical epsilon = -0.1823215567939546\n",
      "0 [D loss: 0.567816, acc.: 53.50%] [G loss: 0.555494]\n",
      "0 [D loss: 0.834519, acc.: 41.00%] [G loss: 0.701899]\n",
      "FPR is 0.81\n",
      "FNR is 0.22999999999999998\n",
      "TPR is 0.77\n",
      "TNR is 0.18999999999999995\n",
      "empirical epsilon = -0.18975653528147268\n",
      "0 [D loss: 0.567196, acc.: 54.00%] [G loss: 0.556222]\n",
      "0 [D loss: 0.833563, acc.: 41.00%] [G loss: 0.703061]\n",
      "FPR is 0.82\n",
      "FNR is 0.20999999999999996\n",
      "TPR is 0.79\n",
      "TNR is 0.18000000000000005\n",
      "empirical epsilon = -0.17261274266699364\n",
      "0 [D loss: 0.567068, acc.: 54.00%] [G loss: 0.556298]\n",
      "0 [D loss: 0.833521, acc.: 41.00%] [G loss: 0.702743]\n",
      "FPR is 0.81\n",
      "FNR is 0.20999999999999996\n",
      "TPR is 0.79\n",
      "TNR is 0.18999999999999995\n",
      "empirical epsilon = -0.16034265007517937\n",
      "0 [D loss: 0.566892, acc.: 54.00%] [G loss: 0.556440]\n",
      "0 [D loss: 0.832573, acc.: 41.00%] [G loss: 0.704718]\n",
      "FPR is 0.82\n",
      "FNR is 0.19999999999999996\n",
      "TPR is 0.8\n",
      "TNR is 0.18000000000000005\n",
      "empirical epsilon = -0.15822400521489402\n",
      "iteration is 5\n",
      "0 [D loss: 0.587195, acc.: 71.50%] [G loss: 0.687752]\n",
      "0 [D loss: 0.762701, acc.: 25.50%] [G loss: 0.615056]\n",
      "FPR is 0.93\n",
      "FNR is 0.13\n",
      "TPR is 0.87\n",
      "TNR is 0.06999999999999995\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.586696, acc.: 72.00%] [G loss: 0.687116]\n",
      "0 [D loss: 0.762017, acc.: 25.50%] [G loss: 0.615145]\n",
      "FPR is 0.91\n",
      "FNR is 0.14\n",
      "TPR is 0.86\n",
      "TNR is 0.08999999999999997\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.586400, acc.: 72.00%] [G loss: 0.686895]\n",
      "0 [D loss: 0.761413, acc.: 26.00%] [G loss: 0.615798]\n",
      "FPR is 0.9\n",
      "FNR is 0.14\n",
      "TPR is 0.86\n",
      "TNR is 0.09999999999999998\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.586342, acc.: 72.00%] [G loss: 0.686753]\n",
      "0 [D loss: 0.761100, acc.: 26.00%] [G loss: 0.616135]\n",
      "FPR is 0.9\n",
      "FNR is 0.13\n",
      "TPR is 0.87\n",
      "TNR is 0.09999999999999998\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "0 [D loss: 0.585991, acc.: 72.00%] [G loss: 0.687063]\n",
      "0 [D loss: 0.760581, acc.: 26.00%] [G loss: 0.616781]\n",
      "FPR is 0.9\n",
      "FNR is 0.12\n",
      "TPR is 0.88\n",
      "TNR is 0.09999999999999998\n",
      "undefined privacy risk\n",
      "empirical epsilon = 0\n",
      "iteration is 6\n",
      "0 [D loss: 0.685384, acc.: 61.50%] [G loss: 0.697720]\n",
      "0 [D loss: 0.626926, acc.: 51.00%] [G loss: 0.594450]\n",
      "FPR is 0.18\n",
      "FNR is 0.77\n",
      "TPR is 0.23\n",
      "TNR is 0.8200000000000001\n",
      "empirical epsilon = -0.06713930283762856\n",
      "0 [D loss: 0.684578, acc.: 61.50%] [G loss: 0.699241]\n",
      "0 [D loss: 0.624592, acc.: 51.50%] [G loss: 0.597878]\n",
      "FPR is 0.18\n",
      "FNR is 0.78\n",
      "TPR is 0.22\n",
      "TNR is 0.8200000000000001\n",
      "empirical epsilon = -0.0800427076735365\n",
      "0 [D loss: 0.683904, acc.: 61.50%] [G loss: 0.700808]\n",
      "0 [D loss: 0.623586, acc.: 51.50%] [G loss: 0.599204]\n",
      "FPR is 0.18\n",
      "FNR is 0.78\n",
      "TPR is 0.22\n",
      "TNR is 0.8200000000000001\n",
      "empirical epsilon = -0.0800427076735365\n",
      "0 [D loss: 0.683653, acc.: 61.50%] [G loss: 0.701505]\n",
      "0 [D loss: 0.623267, acc.: 51.50%] [G loss: 0.599595]\n",
      "FPR is 0.18\n",
      "FNR is 0.79\n",
      "TPR is 0.21\n",
      "TNR is 0.8200000000000001\n",
      "empirical epsilon = -0.09278173345096621\n",
      "0 [D loss: 0.683492, acc.: 61.50%] [G loss: 0.701955]\n",
      "0 [D loss: 0.622586, acc.: 52.50%] [G loss: 0.600435]\n",
      "FPR is 0.17\n",
      "FNR is 0.77\n",
      "TPR is 0.23\n",
      "TNR is 0.83\n",
      "empirical epsilon = -0.053345980705292735\n",
      "iteration is 7\n",
      "0 [D loss: 0.585911, acc.: 79.50%] [G loss: 0.783645]\n",
      "0 [D loss: 0.731818, acc.: 49.00%] [G loss: 0.969199]\n",
      "FPR is 0.23\n",
      "FNR is 0.65\n",
      "TPR is 0.35\n",
      "TNR is 0.77\n",
      "empirical epsilon = 0.083381608939051\n",
      "0 [D loss: 0.585053, acc.: 79.50%] [G loss: 0.782991]\n",
      "0 [D loss: 0.732385, acc.: 49.00%] [G loss: 0.962400]\n",
      "FPR is 0.26\n",
      "FNR is 0.65\n",
      "TPR is 0.35\n",
      "TNR is 0.74\n",
      "empirical epsilon = -0.015504186535965312\n",
      "0 [D loss: 0.584844, acc.: 79.50%] [G loss: 0.782391]\n",
      "0 [D loss: 0.732293, acc.: 49.00%] [G loss: 0.960659]\n",
      "FPR is 0.26\n",
      "FNR is 0.65\n",
      "TPR is 0.35\n",
      "TNR is 0.74\n",
      "empirical epsilon = -0.015504186535965312\n",
      "0 [D loss: 0.584317, acc.: 80.00%] [G loss: 0.783446]\n",
      "0 [D loss: 0.732119, acc.: 49.00%] [G loss: 0.960893]\n",
      "FPR is 0.26\n",
      "FNR is 0.65\n",
      "TPR is 0.35\n",
      "TNR is 0.74\n",
      "empirical epsilon = -0.015504186535965312\n",
      "0 [D loss: 0.584065, acc.: 80.50%] [G loss: 0.783585]\n",
      "0 [D loss: 0.731673, acc.: 49.00%] [G loss: 0.961696]\n",
      "FPR is 0.26\n",
      "FNR is 0.63\n",
      "TPR is 0.37\n",
      "TNR is 0.74\n",
      "empirical epsilon = 0.03774032798284711\n",
      "iteration is 8\n",
      "0 [D loss: 0.584279, acc.: 76.00%] [G loss: 0.804334]\n",
      "0 [D loss: 0.715744, acc.: 36.50%] [G loss: 0.659749]\n",
      "FPR is 0.8\n",
      "FNR is 0.25\n",
      "TPR is 0.75\n",
      "TNR is 0.19999999999999996\n",
      "empirical epsilon = -0.2076393647782445\n",
      "0 [D loss: 0.584099, acc.: 76.00%] [G loss: 0.802847]\n",
      "0 [D loss: 0.714762, acc.: 37.00%] [G loss: 0.660350]\n",
      "FPR is 0.81\n",
      "FNR is 0.21999999999999997\n",
      "TPR is 0.78\n",
      "TNR is 0.18999999999999995\n",
      "empirical epsilon = -0.1749414494963321\n",
      "0 [D loss: 0.584071, acc.: 76.00%] [G loss: 0.801896]\n",
      "0 [D loss: 0.713946, acc.: 39.00%] [G loss: 0.661494]\n",
      "FPR is 0.81\n",
      "FNR is 0.16000000000000003\n",
      "TPR is 0.84\n",
      "TNR is 0.18999999999999995\n",
      "empirical epsilon = -0.09038406146826906\n",
      "0 [D loss: 0.584130, acc.: 76.00%] [G loss: 0.801265]\n",
      "0 [D loss: 0.713402, acc.: 39.00%] [G loss: 0.662428]\n",
      "FPR is 0.82\n",
      "FNR is 0.16000000000000003\n",
      "TPR is 0.84\n",
      "TNR is 0.18000000000000005\n",
      "empirical epsilon = -0.10265415406008334\n",
      "0 [D loss: 0.583974, acc.: 76.00%] [G loss: 0.801392]\n",
      "0 [D loss: 0.713209, acc.: 39.00%] [G loss: 0.662600]\n",
      "FPR is 0.8\n",
      "FNR is 0.15000000000000002\n",
      "TPR is 0.85\n",
      "TNR is 0.19999999999999996\n",
      "empirical epsilon = -0.06453852113757118\n",
      "iteration is 9\n",
      "0 [D loss: 0.803261, acc.: 29.00%] [G loss: 0.636531]\n",
      "0 [D loss: 0.642178, acc.: 67.50%] [G loss: 0.862403]\n",
      "FPR is 0.84\n",
      "FNR is 0.12\n",
      "TPR is 0.88\n",
      "TNR is 0.16000000000000003\n",
      "empirical epsilon = -0.07410797215372185\n",
      "0 [D loss: 0.802945, acc.: 29.00%] [G loss: 0.636309]\n",
      "0 [D loss: 0.641909, acc.: 67.50%] [G loss: 0.861592]\n",
      "FPR is 0.88\n",
      "FNR is 0.08999999999999997\n",
      "TPR is 0.91\n",
      "TNR is 0.12\n",
      "empirical epsilon = -0.08288765980576764\n",
      "0 [D loss: 0.802378, acc.: 29.00%] [G loss: 0.636937]\n",
      "0 [D loss: 0.641952, acc.: 67.50%] [G loss: 0.860308]\n",
      "FPR is 0.87\n",
      "FNR is 0.08999999999999997\n",
      "TPR is 0.91\n",
      "TNR is 0.13\n",
      "empirical epsilon = -0.07145896398214487\n",
      "0 [D loss: 0.802105, acc.: 29.50%] [G loss: 0.637277]\n",
      "0 [D loss: 0.641576, acc.: 67.50%] [G loss: 0.861021]\n",
      "FPR is 0.86\n",
      "FNR is 0.09999999999999998\n",
      "TPR is 0.9\n",
      "TNR is 0.14\n",
      "empirical epsilon = -0.07232066157962602\n",
      "0 [D loss: 0.801699, acc.: 30.00%] [G loss: 0.637823]\n",
      "0 [D loss: 0.641389, acc.: 67.50%] [G loss: 0.861359]\n",
      "FPR is 0.88\n",
      "FNR is 0.09999999999999998\n",
      "TPR is 0.9\n",
      "TNR is 0.12\n",
      "empirical epsilon = -0.09531017980432477\n",
      "iteration is 10\n",
      "0 [D loss: 0.747484, acc.: 36.50%] [G loss: 0.606697]\n",
      "0 [D loss: 0.720502, acc.: 37.50%] [G loss: 0.655809]\n",
      "FPR is 0.75\n",
      "FNR is 0.22999999999999998\n",
      "TPR is 0.77\n",
      "TNR is 0.25\n",
      "empirical epsilon = -0.11279549414534427\n",
      "0 [D loss: 0.746664, acc.: 37.50%] [G loss: 0.607389]\n",
      "0 [D loss: 0.719639, acc.: 38.00%] [G loss: 0.656646]\n",
      "FPR is 0.75\n",
      "FNR is 0.24\n",
      "TPR is 0.76\n",
      "TNR is 0.25\n",
      "empirical epsilon = -0.12783337150988489\n",
      "0 [D loss: 0.746278, acc.: 37.50%] [G loss: 0.608220]\n",
      "0 [D loss: 0.718680, acc.: 38.50%] [G loss: 0.658763]\n",
      "FPR is 0.75\n",
      "FNR is 0.22999999999999998\n",
      "TPR is 0.77\n",
      "TNR is 0.25\n",
      "empirical epsilon = -0.11279549414534427\n",
      "0 [D loss: 0.745291, acc.: 37.50%] [G loss: 0.609944]\n",
      "0 [D loss: 0.718033, acc.: 39.50%] [G loss: 0.660050]\n",
      "FPR is 0.75\n",
      "FNR is 0.21999999999999997\n",
      "TPR is 0.78\n",
      "TNR is 0.25\n",
      "empirical epsilon = -0.09798040836020366\n",
      "0 [D loss: 0.744850, acc.: 37.50%] [G loss: 0.610395]\n",
      "0 [D loss: 0.717820, acc.: 39.50%] [G loss: 0.660208]\n",
      "FPR is 0.76\n",
      "FNR is 0.20999999999999996\n",
      "TPR is 0.79\n",
      "TNR is 0.24\n",
      "empirical epsilon = -0.09662683568907164\n",
      "iteration is 11\n",
      "0 [D loss: 0.655948, acc.: 70.50%] [G loss: 0.760128]\n",
      "0 [D loss: 0.595998, acc.: 55.00%] [G loss: 0.627545]\n",
      "FPR is 0.63\n",
      "FNR is 0.39\n",
      "TPR is 0.61\n",
      "TNR is 0.37\n",
      "empirical epsilon = -0.2113090936672069\n",
      "0 [D loss: 0.654519, acc.: 72.00%] [G loss: 0.762466]\n",
      "0 [D loss: 0.594387, acc.: 56.00%] [G loss: 0.629505]\n",
      "FPR is 0.63\n",
      "FNR is 0.4\n",
      "TPR is 0.6\n",
      "TNR is 0.37\n",
      "empirical epsilon = -0.2311117209633867\n",
      "0 [D loss: 0.653459, acc.: 73.00%] [G loss: 0.764903]\n",
      "0 [D loss: 0.594165, acc.: 56.00%] [G loss: 0.629734]\n",
      "FPR is 0.63\n",
      "FNR is 0.36\n",
      "TPR is 0.64\n",
      "TNR is 0.37\n",
      "empirical epsilon = -0.15415067982725822\n",
      "0 [D loss: 0.653107, acc.: 73.00%] [G loss: 0.765549]\n",
      "0 [D loss: 0.594079, acc.: 56.00%] [G loss: 0.629612]\n",
      "FPR is 0.63\n",
      "FNR is 0.35\n",
      "TPR is 0.65\n",
      "TNR is 0.37\n",
      "empirical epsilon = -0.13580154115906162\n",
      "0 [D loss: 0.652731, acc.: 73.00%] [G loss: 0.766444]\n",
      "0 [D loss: 0.593872, acc.: 56.00%] [G loss: 0.629675]\n",
      "FPR is 0.63\n",
      "FNR is 0.36\n",
      "TPR is 0.64\n",
      "TNR is 0.37\n",
      "empirical epsilon = -0.15415067982725822\n",
      "iteration is 12\n",
      "0 [D loss: 0.658851, acc.: 56.50%] [G loss: 0.629253]\n",
      "0 [D loss: 0.659518, acc.: 53.50%] [G loss: 0.711853]\n",
      "FPR is 0.78\n",
      "FNR is 0.19999999999999996\n",
      "TPR is 0.8\n",
      "TNR is 0.21999999999999997\n",
      "empirical epsilon = -0.10821358464023272\n",
      "0 [D loss: 0.658707, acc.: 56.50%] [G loss: 0.628426]\n",
      "0 [D loss: 0.659554, acc.: 53.50%] [G loss: 0.711199]\n",
      "FPR is 0.76\n",
      "FNR is 0.20999999999999996\n",
      "TPR is 0.79\n",
      "TNR is 0.24\n",
      "empirical epsilon = -0.09662683568907164\n",
      "0 [D loss: 0.657978, acc.: 56.50%] [G loss: 0.629195]\n",
      "0 [D loss: 0.658970, acc.: 53.50%] [G loss: 0.712137]\n",
      "FPR is 0.78\n",
      "FNR is 0.20999999999999996\n",
      "TPR is 0.79\n",
      "TNR is 0.21999999999999997\n",
      "empirical epsilon = -0.12260232209233227\n",
      "0 [D loss: 0.657936, acc.: 56.50%] [G loss: 0.629049]\n",
      "0 [D loss: 0.658955, acc.: 53.50%] [G loss: 0.712184]\n",
      "FPR is 0.78\n",
      "FNR is 0.20999999999999996\n",
      "TPR is 0.79\n",
      "TNR is 0.21999999999999997\n",
      "empirical epsilon = -0.12260232209233227\n",
      "0 [D loss: 0.657489, acc.: 56.50%] [G loss: 0.629336]\n",
      "0 [D loss: 0.658720, acc.: 53.50%] [G loss: 0.712239]\n",
      "FPR is 0.77\n",
      "FNR is 0.19999999999999996\n",
      "TPR is 0.8\n",
      "TNR is 0.22999999999999998\n",
      "empirical epsilon = -0.09531017980432477\n",
      "iteration is 13\n",
      "0 [D loss: 0.744370, acc.: 26.00%] [G loss: 0.661039]\n",
      "0 [D loss: 0.883674, acc.: 48.50%] [G loss: 0.837350]\n",
      "FPR is 0.76\n",
      "FNR is 0.18999999999999995\n",
      "TPR is 0.81\n",
      "TNR is 0.24\n",
      "empirical epsilon = -0.06805346324501552\n",
      "0 [D loss: 0.743795, acc.: 26.00%] [G loss: 0.661337]\n",
      "0 [D loss: 0.883488, acc.: 48.50%] [G loss: 0.837120]\n",
      "FPR is 0.74\n",
      "FNR is 0.16000000000000003\n",
      "TPR is 0.84\n",
      "TNR is 0.26\n",
      "empirical epsilon = 0.0\n",
      "0 [D loss: 0.743275, acc.: 27.00%] [G loss: 0.661859]\n",
      "0 [D loss: 0.883086, acc.: 48.50%] [G loss: 0.837779]\n",
      "FPR is 0.73\n",
      "FNR is 0.18000000000000005\n",
      "TPR is 0.82\n",
      "TNR is 0.27\n",
      "empirical epsilon = -0.013793322132335873\n",
      "0 [D loss: 0.743205, acc.: 27.50%] [G loss: 0.661869]\n",
      "0 [D loss: 0.882995, acc.: 48.50%] [G loss: 0.837710]\n",
      "FPR is 0.76\n",
      "FNR is 0.18000000000000005\n",
      "TPR is 0.82\n",
      "TNR is 0.24\n",
      "empirical epsilon = -0.05406722127027582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 105\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# print(\"theoretical epsilon = \" + str(round(theor_epsilon[0],2))) # print epsilon\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# train GAN on train data\u001b[39;00m\n\u001b[0;32m    104\u001b[0m gan_train \u001b[38;5;241m=\u001b[39m GAN(privacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 105\u001b[0m gan_train\u001b[38;5;241m.\u001b[39mtrain(data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_GAN_real), iterations\u001b[38;5;241m=\u001b[39miterations, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, sample_interval\u001b[38;5;241m=\u001b[39m((iterations\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m), model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_1.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Generate a batch of new customers\u001b[39;00m\n\u001b[0;32m    108\u001b[0m generator \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_1.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 115\u001b[0m, in \u001b[0;36mGAN.train\u001b[1;34m(self, data, iterations, batch_size, sample_interval, model_name, generator_losses, discriminator_acc, correlations, accuracy, MAPD_collect, MSE_collect, MAE_collect)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m#  Train Generator\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Train the generator (to have the discriminator label samples as valid)\u001b[39;00m\n\u001b[0;32m    114\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim))\n\u001b[1;32m--> 115\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombined\u001b[38;5;241m.\u001b[39mtrain_on_batch(noise, valid)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    118\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, acc.: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, d_loss[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39md_loss[\u001b[38;5;241m1\u001b[39m], g_loss))\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:2787\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2783\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2784\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2785\u001b[0m     )\n\u001b[0;32m   2786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2787\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   2789\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:905\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    902\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    906\u001b[0m         args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    907\u001b[0m     )\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    909\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[0;32m    910\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[0;32m    911\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[0;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m trace_function(\n\u001b[0;32m    133\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, tracing_options\u001b[38;5;241m=\u001b[39mtracing_options\n\u001b[0;32m    134\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[0;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[0;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[0;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:598\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    595\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 598\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m api\u001b[38;5;241m.\u001b[39mconverted_call(\n\u001b[0;32m     42\u001b[0m       original_func,\n\u001b[0;32m     43\u001b[0m       args,\n\u001b[0;32m     44\u001b[0m       kwargs,\n\u001b[0;32m     45\u001b[0m       options\u001b[38;5;241m=\u001b[39mconverter\u001b[38;5;241m.\u001b[39mConversionOptions(\n\u001b[0;32m     46\u001b[0m           recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m           optional_features\u001b[38;5;241m=\u001b[39mautograph_options,\n\u001b[0;32m     48\u001b[0m           user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       ))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filedrn41h63.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1380\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m   1381\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m     )\n\u001b[0;32m   1383\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1384\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(run_step, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[0;32m   1385\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1386\u001b[0m     outputs,\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1388\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1389\u001b[0m )\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1681\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1677\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1679\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   1680\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1681\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended\u001b[38;5;241m.\u001b[39mcall_for_each_replica(fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3271\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3269\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4069\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   4067\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   4068\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:1373\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1373\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_step(data)\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\engine\\training.py:1154\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m-> 1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:544\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m  None\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    543\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39minterim\u001b[38;5;241m.\u001b[39mmaybe_merge_call(\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_apply_gradients_fn,\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy,\n\u001b[0;32m   1256\u001b[0m     grads_and_vars,\n\u001b[0;32m   1257\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1345\u001b[0m     distribution\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   1346\u001b[0m         var, apply_grad_to_update_var, args\u001b[38;5;241m=\u001b[39m(grad,), group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[0;32m   1350\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3015\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   3014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3015\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   3016\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2894\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge_fn\u001b[39m(_, \u001b[38;5;241m*\u001b[39mmerged_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmerged_kwargs):\n\u001b[0;32m   2892\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(var, fn, merged_args, merged_kwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n\u001b[1;32m-> 2894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m replica_context\u001b[38;5;241m.\u001b[39mmerge_call(merge_fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3486\u001b[0m, in \u001b[0;36mReplicaContextBase.merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3482\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   3484\u001b[0m merge_fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   3485\u001b[0m     merge_fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 3486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_call(merge_fn, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3493\u001b[0m, in \u001b[0;36mReplicaContextBase._merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3490\u001b[0m _push_per_thread_mode(  \u001b[38;5;66;03m# thread-local, so not needed with multiple threads\u001b[39;00m\n\u001b[0;32m   3491\u001b[0m     _CrossReplicaThreadMode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy))  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3493\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m merge_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3494\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   3495\u001b[0m   _pop_per_thread_mode()\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2892\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update.<locals>.merge_fn\u001b[1;34m(_, *merged_args, **merged_kwargs)\u001b[0m\n\u001b[0;32m   2891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge_fn\u001b[39m(_, \u001b[38;5;241m*\u001b[39mmerged_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmerged_kwargs):\n\u001b[1;32m-> 2892\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(var, fn, merged_args, merged_kwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3013\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3010\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   3011\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3012\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   3014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3015\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   3016\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4083\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   4081\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   4082\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 4083\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_non_slot(var, fn, (var,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args), kwargs, group)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4089\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4085\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   4086\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   4087\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   4088\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 4089\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[0;32m   4091\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(gradient, variable)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:204\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    202\u001b[0m     v_hat\u001b[38;5;241m.\u001b[39massign(tf\u001b[38;5;241m.\u001b[39mmaximum(v_hat, v))\n\u001b[0;32m    203\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_hat\n\u001b[1;32m--> 204\u001b[0m variable\u001b[38;5;241m.\u001b[39massign_sub((m \u001b[38;5;241m*\u001b[39m alpha) \u001b[38;5;241m/\u001b[39m (tf\u001b[38;5;241m.\u001b[39msqrt(v) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon))\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1478\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1474\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m-> 1478\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1480\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1484\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1871\u001b[0m, in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39madd(x, y, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39madd_v2(x, y, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:490\u001b[0m, in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m _op_def_library\u001b[38;5;241m.\u001b[39m_apply_op_helper(\n\u001b[0;32m    491\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAddV2\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    492\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:796\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m must_colocate_inputs \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def\u001b[38;5;241m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    792\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mis_ref]\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    794\u001b[0m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    795\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m   op \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39m_create_op_internal(op_type_name, inputs, dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                              name\u001b[38;5;241m=\u001b[39mscope, input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m    798\u001b[0m                              attrs\u001b[38;5;241m=\u001b[39mattr_protos, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[0;32m    804\u001b[0m outputs \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2652\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2650\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2651\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2652\u001b[0m   ret \u001b[38;5;241m=\u001b[39m Operation\u001b[38;5;241m.\u001b[39mfrom_node_def(\n\u001b[0;32m   2653\u001b[0m       node_def,\n\u001b[0;32m   2654\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2655\u001b[0m       inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   2656\u001b[0m       output_types\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   2657\u001b[0m       control_inputs\u001b[38;5;241m=\u001b[39mcontrol_inputs,\n\u001b[0;32m   2658\u001b[0m       input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m   2659\u001b[0m       original_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_original_op,\n\u001b[0;32m   2660\u001b[0m       op_def\u001b[38;5;241m=\u001b[39mop_def,\n\u001b[0;32m   2661\u001b[0m   )\n\u001b[0;32m   2662\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1160\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1157\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1160\u001b[0m c_op \u001b[38;5;241m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\cdbale\\.conda\\envs\\dp-gan\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:990\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39m_c_graph\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mas\u001b[39;00m c_graph:\n\u001b[1;32m--> 990\u001b[0m   op_desc \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_NewOperation(c_graph,\n\u001b[0;32m    991\u001b[0m                                               compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mop),\n\u001b[0;32m    992\u001b[0m                                               compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m    994\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from absl import logging as absl_logging\n",
    "\n",
    "# Suppress low-level TF C++ logs (0=all, 1=INFO, 2=WARNING, 3=ERROR)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Suppress Python-level TF warnings\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "\n",
    "noise_multipliers = all_noise_multipliers['300']\n",
    "samples = int(data_sizes[0])\n",
    "\n",
    "\"\"\"iteraties en batch size hetzelfde houden.\"\"\"\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epsilons_13 = np.array([])\n",
    "MAPD_col_13 = np.array([])\n",
    "MAE_col_13 = np.array([])\n",
    "MSE_col_13 = np.array([])\n",
    "\n",
    "epsilons_3 = np.array([])\n",
    "MAPD_col_3 = np.array([])\n",
    "MAE_col_3 = np.array([])\n",
    "MSE_col_3 = np.array([])\n",
    "\n",
    "epsilons_1 = np.array([])\n",
    "MAPD_col_1 = np.array([])\n",
    "MAE_col_1 = np.array([])\n",
    "MSE_col_1 = np.array([])\n",
    "\n",
    "epsilons_05 = np.array([])\n",
    "MAPD_col_05 = np.array([])\n",
    "MAE_col_05 = np.array([])\n",
    "MSE_col_05 = np.array([])\n",
    "\n",
    "epsilons_005 = np.array([])\n",
    "MAPD_col_005 = np.array([])\n",
    "MAE_col_005 = np.array([])\n",
    "MSE_col_005 = np.array([])\n",
    "\n",
    "epsilons_001 = np.array([])\n",
    "MAPD_col_001 = np.array([])\n",
    "MAE_col_001 = np.array([])\n",
    "MSE_col_001 = np.array([])\n",
    "\n",
    "TPR_col = np.array([])\n",
    "FPR_col = np.array([])\n",
    "TNR_col = np.array([])\n",
    "FNR_col = np.array([])\n",
    "\n",
    "for iter in range(100):\n",
    "  random.seed(iter)\n",
    "  np.random.seed(iter)\n",
    "  tf.random.set_seed(iter)\n",
    "  print(\"iteration is \" + str(iter))\n",
    "  sampled_churn = churn.sample(frac = 1, random_state = iter)\n",
    "  both_train, evaluation_outside_training = train_test_split(sampled_churn, train_size = int(samples*2/3), test_size = int(samples*1/3), stratify = sampled_churn['conversion'])\n",
    "  train, adversary_training = train_test_split(both_train, train_size = int(samples*1/3), stratify=both_train['conversion'])\n",
    "\n",
    "  scaler0 = MinMaxScaler(feature_range= (-1, 1))\n",
    "  scaler0 = scaler0.fit(train)\n",
    "  train_GAN_real = scaler0.transform(train)\n",
    "  train_GAN_real = pd.DataFrame(train_GAN_real)\n",
    "\n",
    "  scaler1 = MinMaxScaler(feature_range= (-1, 1))\n",
    "  scaler1 = scaler1.fit(adversary_training)\n",
    "  adversary_training_GAN_real = scaler1.transform(adversary_training)\n",
    "  adversary_training_GAN_real = pd.DataFrame(adversary_training_GAN_real)\n",
    "\n",
    "  for noise in noise_multipliers: # we vary the noise multipliers here\n",
    "    random.seed(iter)\n",
    "    np.random.seed(iter)\n",
    "    tf.random.set_seed(iter)\n",
    "\n",
    "    # setting epsilon\n",
    "    N = len(train)\n",
    "    batch_size = 100\n",
    "    ### change for different data sizes\n",
    "    iterations = 10\n",
    "    epochs = iterations/(N/batch_size) # should be 10\n",
    "\n",
    "    # the noise_multiplier is not directly passed to the GAN, but the GAN code reads it from the global environment\n",
    "    noise_multiplier = noise\n",
    "    l2_norm_clip = 4 # see paper in validation section.\n",
    "    delta= 1/N # should be 1/N\n",
    "    theor_epsilon = compute_dp_sgd_privacy(N, batch_size, noise_multiplier,\n",
    "                          epochs, delta) # calculate the theoretical bound of epsilon\n",
    "    N = len(train)/10 # to prevent naive model\n",
    "    num_microbatches = batch_size # see validation section paper.\n",
    "    # print(\"theoretical epsilon = \" + str(round(theor_epsilon[0],2))) # print epsilon\n",
    "\n",
    "    # train GAN on train data\n",
    "    gan_train = GAN(privacy = True)\n",
    "    gan_train.train(data = np.array(train_GAN_real), iterations=iterations, batch_size=batch_size, sample_interval=((iterations-1)/10), model_name = \"train_1.h5\")\n",
    "\n",
    "    # Generate a batch of new customers\n",
    "    generator = load_model('train_1.h5')\n",
    "    noise = np.random.normal(0, 1, (int(samples*1/3), 16))\n",
    "    gen_imgs = generator.predict(noise, verbose = False)\n",
    "    gen_imgs = scaler0.inverse_transform(gen_imgs)\n",
    "    train_GAN = pd.DataFrame(gen_imgs.reshape(int(samples*1/3), 16))\n",
    "    train_GAN.columns = train.columns.values\n",
    "\n",
    "    ####################################################\n",
    "    # round the values of categorical variables, as done by Ponte et al.\n",
    "    ####################################################\n",
    "    train_GAN['treatment'] = train_GAN['treatment'].round()\n",
    "    train_GAN['conversion'] = train_GAN['conversion'].round()\n",
    "    train_GAN['visit'] = train_GAN['visit'].round()\n",
    "    train_GAN['exposure'] = train_GAN['exposure'].round()\n",
    "    \n",
    "    # adversary has access to the model and samples another adversary_sample\n",
    "    gan_adv = GAN(privacy = True)\n",
    "    gan_adv.train(data = np.array(adversary_training_GAN_real), iterations=iterations, batch_size=batch_size, sample_interval=((iterations-1)/10), model_name = \"adversary_1.h5\")\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    generator = load_model('adversary_1.h5')\n",
    "    noise = np.random.normal(0, 1, (int(samples*1/3), 16))\n",
    "    gen_imgs = generator.predict(noise, verbose = False)\n",
    "    gen_imgs = scaler1.inverse_transform(gen_imgs)\n",
    "    adversary_training_GAN = pd.DataFrame(gen_imgs.reshape(int(samples*1/3), 16))\n",
    "    adversary_training_GAN.columns = adversary_training.columns.values\n",
    "\n",
    "    ####################################################\n",
    "    # round the values of categorical variables, as done by Ponte et al.\n",
    "    ####################################################\n",
    "    adversary_training_GAN['treatment'] = adversary_training_GAN['treatment'].round()\n",
    "    adversary_training_GAN['conversion'] = adversary_training_GAN['conversion'].round()\n",
    "    adversary_training_GAN['visit'] = adversary_training_GAN['visit'].round()\n",
    "    adversary_training_GAN['exposure'] = adversary_training_GAN['exposure'].round()\n",
    "\n",
    "    # stap 1, 2\n",
    "    params = {\"bandwidth\": np.logspace(-1, 1, 20)}\n",
    "    grid_train = GridSearchCV(KernelDensity(), params, n_jobs = -1)\n",
    "    grid_train.fit(train_GAN)\n",
    "    # print(grid_train.best_estimator_)\n",
    "    kde_train = grid_train.best_estimator_\n",
    "\n",
    "    grid = GridSearchCV(KernelDensity(), params, n_jobs = -1)\n",
    "    grid.fit(adversary_training_GAN)\n",
    "    # print(grid.best_estimator_)\n",
    "    kde_adversary = grid.best_estimator_\n",
    "\n",
    "    # stap 3\n",
    "    density_train = kde_train.score_samples(train)\n",
    "    density_adversary = kde_adversary.score_samples(train)\n",
    "    TPR = sum(density_train > density_adversary)/len(density_train)\n",
    "\n",
    "    # stap 4\n",
    "    density_train_new = kde_train.score_samples(evaluation_outside_training)\n",
    "    density_adversary_new = kde_adversary.score_samples(evaluation_outside_training)\n",
    "    FPR = sum(density_train_new > density_adversary_new)/len(density_train_new)\n",
    "    TNR = 1 - FPR\n",
    "    FNR = 1 - TPR\n",
    "    print(\"FPR is \" + str(FPR))\n",
    "    print(\"FNR is \" + str(FNR))\n",
    "    print(\"TPR is \" + str(TPR))\n",
    "    print(\"TNR is \" + str(TNR))\n",
    "\n",
    "    TPR_col = np.append(TPR_col, TPR)\n",
    "    FPR_col = np.append(FPR_col, FPR)\n",
    "    TNR_col = np.append(TNR_col, TNR)\n",
    "    FNR_col = np.append(FNR_col, FNR)\n",
    "\n",
    "    # utility\n",
    "    MAPD_train, MAE_train, MSE_train = utility(real_data = train, protected_data = train_GAN)\n",
    "    MAPD_adv, MAE_adv, MSE_adv = utility(real_data = train, protected_data = adversary_training_GAN)\n",
    "    MAPD = (MAPD_train+MAPD_adv)/2\n",
    "    MAE = (MAE_train+MAE_adv)/2\n",
    "    MSE = (MSE_train+MSE_adv)/2\n",
    "    # print(\"MAPD\" + str(MAPD))\n",
    "\n",
    "    ## to save the results per epsilon (a bit lazy admittedly).\n",
    "    if noise_multiplier == noise_multipliers[0]:\n",
    "      try:\n",
    "        epsilons_13 = np.append(epsilons_13,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "        print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "        MAPD_col_13 = np.append(MAPD_col_13, MAPD)\n",
    "        MAE_col_13 = np.append(MAE_col_13, MAE)\n",
    "        MSE_col_13 = np.append(MSE_col_13, MSE)\n",
    "      except:\n",
    "        print(\"undefined privacy risk\")\n",
    "        epsilons_13 = np.append(epsilons_13, 0)\n",
    "        print(\"empirical epsilon = \" + str(0))\n",
    "        MAPD_col_13 = np.append(MAPD_col_13, MAPD)\n",
    "        MAE_col_13 = np.append(MAE_col_13, MAE)\n",
    "        MSE_col_13 = np.append(MSE_col_13, MSE)\n",
    "\n",
    "    if noise_multiplier == noise_multipliers[1]:\n",
    "      try:\n",
    "        epsilons_3 = np.append(epsilons_3,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "        print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "        MAPD_col_3 = np.append(MAPD_col_3, MAPD)\n",
    "        MAE_col_3 = np.append(MAE_col_3, MAE)\n",
    "        MSE_col_3 = np.append(MSE_col_3, MSE)\n",
    "      except:\n",
    "        print(\"undefined privacy risk\")\n",
    "        epsilons_3 = np.append(epsilons_3, 0)\n",
    "        print(\"empirical epsilon = \" + str(0))\n",
    "        MAPD_col_3 = np.append(MAPD_col_3, MAPD)\n",
    "        MAE_col_3 = np.append(MAE_col_3, MAE)\n",
    "        MSE_col_3 = np.append(MSE_col_3, MSE)\n",
    "\n",
    "    if noise_multiplier == noise_multipliers[2]:\n",
    "      try:\n",
    "        epsilons_1 = np.append(epsilons_1,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "        print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "        MAPD_col_1 = np.append(MAPD_col_1, MAPD)\n",
    "        MAE_col_1 = np.append(MAE_col_1, MAE)\n",
    "        MSE_col_1 = np.append(MSE_col_1, MSE)\n",
    "      except:\n",
    "        print(\"undefined privacy risk\")\n",
    "        epsilons_1 = np.append(epsilons_1, 0)\n",
    "        print(\"empirical epsilon = \" + str(0))\n",
    "        MAPD_col_1 = np.append(MAPD_col_1, MAPD)\n",
    "        MAE_col_1 = np.append(MAE_col_1, MAE)\n",
    "        MSE_col_1 = np.append(MSE_col_1, MSE)\n",
    "\n",
    "    if noise_multiplier == noise_multipliers[3]:\n",
    "      try:\n",
    "        epsilons_05 = np.append(epsilons_05,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "        print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "        MAPD_col_05 = np.append(MAPD_col_05, MAPD)\n",
    "        MAE_col_05 = np.append(MAE_col_05, MAE)\n",
    "        MSE_col_05 = np.append(MSE_col_05, MSE)\n",
    "      except:\n",
    "        print(\"undefined privacy risk\")\n",
    "        epsilons_05 = np.append(epsilons_05, 0)\n",
    "        print(\"empirical epsilon = \" + str(0))\n",
    "        MAPD_col_05 = np.append(MAPD_col_05, MAPD)\n",
    "        MAE_col_05 = np.append(MAE_col_05, MAE)\n",
    "        MSE_col_05 = np.append(MSE_col_05, MSE)\n",
    "\n",
    "    if noise_multiplier == noise_multipliers[4]:\n",
    "      try:\n",
    "        epsilons_005 = np.append(epsilons_005,max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR)))\n",
    "        print(\"empirical epsilon = \" + str(max(math.log((1 - (1/N) - FPR)/FNR), math.log((1 - (1/N) - FNR)/FPR))))\n",
    "        MAPD_col_005 = np.append(MAPD_col_005, MAPD)\n",
    "        MAE_col_005 = np.append(MAE_col_005, MAE)\n",
    "        MSE_col_005 = np.append(MSE_col_005, MSE)\n",
    "      except:\n",
    "        print(\"undefined privacy risk\")\n",
    "        epsilons_005 = np.append(epsilons_005, 0)\n",
    "        print(\"empirical epsilon = \" + str(0))\n",
    "        MAPD_col_005 = np.append(MAPD_col_005, MAPD)\n",
    "        MAE_col_005 = np.append(MAE_col_005, MAE)\n",
    "        MSE_col_005 = np.append(MSE_col_005, MSE)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "epsilons_13.mean()\n",
    "\n",
    "epsilons_3.mean()\n",
    "\n",
    "epsilons_1.mean()\n",
    "\n",
    "epsilons_05.mean()\n",
    "\n",
    "epsilons_005.mean()\n",
    "\n",
    "np.savetxt(\"epsilons_13_\" + str(samples) + \".csv\", epsilons_13, delimiter=\",\")\n",
    "np.savetxt(\"MAPD_13_\" + str(samples) + \".csv\", MAPD_col_13, delimiter=\",\")\n",
    "np.savetxt(\"MAE_13_\" + str(samples) + \".csv\", MAE_col_13, delimiter=\",\")\n",
    "np.savetxt(\"MSE_13_\" + str(samples) + \".csv\", MSE_col_13, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"epsilons_3_\" + str(samples) + \".csv\", epsilons_3, delimiter=\",\")\n",
    "np.savetxt(\"MAPD_3_\" + str(samples) + \".csv\", MAPD_col_3, delimiter=\",\")\n",
    "np.savetxt(\"MAE_3_\" + str(samples) + \".csv\", MAE_col_3, delimiter=\",\")\n",
    "np.savetxt(\"MSE_3_\" + str(samples) + \".csv\", MSE_col_3, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"epsilons_1_\" + str(samples) + \".csv\", epsilons_1, delimiter=\",\")\n",
    "np.savetxt(\"MAPD_1_\" + str(samples) + \".csv\", MAPD_col_1, delimiter=\",\")\n",
    "np.savetxt(\"MAE_1_\" + str(samples) + \".csv\", MAE_col_1, delimiter=\",\")\n",
    "np.savetxt(\"MSE_1_\" + str(samples) + \".csv\", MSE_col_1, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"epsilons_05_\" + str(samples) + \".csv\", epsilons_05, delimiter=\",\")\n",
    "np.savetxt(\"MAPD_05_\" + str(samples) + \".csv\", MAPD_col_05, delimiter=\",\")\n",
    "np.savetxt(\"MAE_05_\" + str(samples) + \".csv\", MAE_col_05, delimiter=\",\")\n",
    "np.savetxt(\"MSE_05_\" + str(samples) + \".csv\", MSE_col_05, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"epsilons_005_\" + str(samples) + \".csv\", epsilons_005, delimiter=\",\")\n",
    "np.savetxt(\"MAPD_005_\" + str(samples) + \".csv\", MAPD_col_005, delimiter=\",\")\n",
    "np.savetxt(\"MAE_005_\" + str(samples) + \".csv\", MAE_col_005, delimiter=\",\")\n",
    "np.savetxt(\"MSE_005_\" + str(samples) + \".csv\", MSE_col_005, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b628e",
   "metadata": {},
   "source": [
    "#### Results for $N = 3000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca5ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp-gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
