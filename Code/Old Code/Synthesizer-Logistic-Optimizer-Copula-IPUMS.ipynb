{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f48262",
   "metadata": {},
   "source": [
    "This notebook performs the synthesis using our proposed synthesization approach for the training data only (excludes the holdout data). The synthesis model is a logistic/multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import itertools\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from bayesian_bootstrap import bayesian_bootstrap\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8ccbc",
   "metadata": {},
   "source": [
    "Import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standardized lat/long location data\n",
    "train_data = pd.read_csv(\"Data/cleaned_ipums_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584fe74",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8bcc5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f1f98",
   "metadata": {},
   "source": [
    "Calculate the average hellinger distance of bootstrapped data. Serves as a proxy for the hellinger distance between data sets sampled from the same data generating distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07842f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger(sigma_real, sigma_synth):\n",
    "    d = 1/2 * np.log(np.linalg.det((sigma_real + sigma_synth)/2)/np.sqrt(np.linalg.det(sigma_real) * np.linalg.det(sigma_synth)))\n",
    "    return np.sqrt(1-np.exp(-d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_hellinger(train_data, num_bootstraps):\n",
    "    copula1 = GaussianMultivariate()\n",
    "    copula1.fit(train_data)\n",
    "    full_h = []\n",
    "    for i in range(num_bootstraps):\n",
    "        sampled_data = train_data.sample(n=train_data.shape[0], replace=True).reset_index(drop=True)\n",
    "        copula2 = GaussianMultivariate()\n",
    "        copula2.fit(sampled_data)\n",
    "        h = hellinger(copula1.correlation, copula2.correlation)\n",
    "        full_h.append(h)\n",
    "    return full_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger_distance_ratio(synthetic_dataset, original_data_corr_mat, null_h):\n",
    "    copula2 = GaussianMultivariate()\n",
    "    copula2.fit(synthetic_dataset)\n",
    "    h = hellinger(original_data_corr_mat, copula2.correlation)\n",
    "    return h/null_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8b65d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad154e0b",
   "metadata": {},
   "source": [
    "Test the copula model. According to wikipedia, the Gaussian copula is the joint CDF of a multivariate Gaussian distribution with mean vector 0 and covariance matrix equal to the correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14972140",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ef6c3",
   "metadata": {},
   "source": [
    "Testing logistic and multinomial logistic regression synthesizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_and_standardize(dataset, poly_degree=3, interaction_only=False):\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=poly_degree, interaction_only=interaction_only, include_bias=False)\n",
    "    \n",
    "    X = poly.fit_transform(dataset)\n",
    "    \n",
    "    scaled_X = preprocessing.StandardScaler().fit_transform(X)\n",
    "    \n",
    "    return scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_synthesizer(orig_data, synth_data_sets, target, penalty_param, poly_degree=3, interaction_only=False):\n",
    "    \n",
    "    mn_model = LogisticRegression(penalty='l1', C=penalty_param, solver='saga', max_iter=1000, multi_class='multinomial', random_state=rng)\n",
    "    \n",
    "    X = polynomial_and_standardize(dataset=orig_data, poly_degree=poly_degree, interaction_only=interaction_only)\n",
    "    \n",
    "    sXs = [polynomial_and_standardize(dataset=Y, poly_degree=poly_degree, interaction_only=interaction_only) for Y in synth_data_sets]\n",
    "    \n",
    "    vals = []\n",
    "    \n",
    "    mn_model.fit(X, target)\n",
    "    \n",
    "    rng_mn = default_rng()\n",
    "    \n",
    "    for Y in sXs:\n",
    "        \n",
    "        probs = mn_model.predict_proba(Y)\n",
    "    \n",
    "        v = [np.argmax(rng_mn.multinomial(n=1, pvals=p, size=1)==1) for p in probs]\n",
    "    \n",
    "        vals.append(pd.Series(v, name=target.name))\n",
    "    \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_mn(#overall parameters\n",
    "                    train_data,\n",
    "                    original_cor_mat,\n",
    "                    null_h,\n",
    "                    number_synthetic_datasets,\n",
    "                    # hyperparameters for GMM, end with underscore means Bayesian optimization will choose\n",
    "                    number_gmm_initializations,\n",
    "                    num_components_,\n",
    "                    # hyperparameters for CART, end with underscore means Bayesian optimization will choose\n",
    "                    C_non_white_,\n",
    "                    C_sex_):\n",
    "    \n",
    "    num_samples = train_data.shape[0]\n",
    "    \n",
    "    ########## Code for GMM ############\n",
    "    \n",
    "    # fit GMM model\n",
    "    GMM = GaussianMixture(num_components_, n_init=number_gmm_initializations, init_params=\"k-means++\", random_state=rng).fit(train_data.loc[:,[\"INCWAGE\", \"years_of_educ\", \"potential_experience\"]])\n",
    "    \n",
    "    # list for synthetic datasets\n",
    "    sXs = []\n",
    "    \n",
    "    # generate and store number_synthetic_datasets synthetic datasets\n",
    "    for i in range(number_synthetic_datasets):\n",
    "        sX = GMM.sample(num_samples)[0]\n",
    "        sX = pd.DataFrame(sX)\n",
    "        sX.columns = [\"INCWAGE\", \"years_of_educ\", \"potential_experience\"]\n",
    "        sXs.append(sX)\n",
    "        \n",
    "    ####################################################################################################\n",
    "        \n",
    "    ########### Code for non-white MN ##########\n",
    "    \n",
    "    synth_non_white_vars = multinomial_synthesizer(orig_data=train_data.loc[:,[\"INCWAGE\", \"years_of_educ\", \"potential_experience\"]], \n",
    "                                                   synth_data_sets=sXs, \n",
    "                                                   target=train_data.non_white, \n",
    "                                                   penalty_param=C_non_white_)\n",
    "    \n",
    "    sXs = [pd.concat([Y, synth_non_white_vars[i]], axis=1) for i,Y in enumerate(sXs)]\n",
    "        \n",
    "    ####################################################################################################\n",
    "        \n",
    "    ########### Code for sex MN ##########\n",
    "    \n",
    "    synth_sex_vars = multinomial_synthesizer(orig_data=train_data.loc[:,[\"INCWAGE\", \"years_of_educ\", \"potential_experience\", \"non_white\"]], \n",
    "                                             synth_data_sets=sXs, \n",
    "                                             target=train_data.SEX, \n",
    "                                             penalty_param=C_sex_)\n",
    "    \n",
    "    sXs = [pd.concat([Y, synth_sex_vars[i]], axis=1) for i,Y in enumerate(sXs)]\n",
    "        \n",
    "    ####################################################################################################\n",
    "        \n",
    "    ###### Calculate pMSE ratios ######\n",
    "    hellinger_ratios = [hellinger_distance_ratio(Y, original_cor_mat, null_h) for Y in sXs]\n",
    "    \n",
    "    return hellinger_ratios, sXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b65e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_models_mn(train_data,\n",
    "                       original_cor_mat,\n",
    "                       null_h,\n",
    "                       number_synthetic_datasets,\n",
    "                       number_gmm_initializations,\n",
    "                       random_state):\n",
    "\n",
    "    def evaluate_models(num_components_, C_non_white_, C_sex_, original_cor_mat=original_cor_mat, null_h=null_h):\n",
    "\n",
    "        hellinger_ratios, _   = train_models_mn(train_data=train_data,\n",
    "                                                original_cor_mat=original_cor_mat,\n",
    "                                                null_h=null_h,\n",
    "                                                number_synthetic_datasets=number_synthetic_datasets,\n",
    "                                                number_gmm_initializations=number_gmm_initializations,\n",
    "                                                num_components_=int(num_components_),\n",
    "                                                C_non_white_=C_non_white_,\n",
    "                                                C_sex_=C_sex_)\n",
    "        \n",
    "        return -1 * ((1 - np.mean(hellinger_ratios))**2)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=evaluate_models,\n",
    "        pbounds={\n",
    "            \"num_components_\": (10, 200.99),\n",
    "            \"C_non_white_\": (0.001, 2),\n",
    "            \"C_sex_\": (0.001, 2)\n",
    "        },\n",
    "        random_state=random_state)\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", xi=1e-02)\n",
    "    optimizer.maximize(init_points=5, n_iter=25, acquisition_function=utility)\n",
    "    print(\"Final Result: \", optimizer.max)\n",
    "    return optimizer.max, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d62c3",
   "metadata": {},
   "source": [
    "The default value for $\\alpha = 1e-06$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b295a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd = 20\n",
    "ngi = 5\n",
    "copula = GaussianMultivariate()\n",
    "copula.fit(train_data)\n",
    "original_cor_mat = copula.correlation\n",
    "null_h = null_hellinger(train_data, nsd)\n",
    "# random_states = [np.random.RandomState(1234), np.random.RandomState(4321), np.random.RandomState(10620), np.random.RandomState(91695), np.random.RandomState(31296)]\n",
    "random_states = [np.random.RandomState(1234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ab249",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(null_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c78ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results = [optimize_models_mn(train_data=train_data, original_cor_mat=original_cor_mat, null_h=np.mean(null_h), number_synthetic_datasets=nsd, number_gmm_initializations=ngi, random_state=r) for r in random_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_targets = [np.minimum.accumulate(-i[1].space.target) for i in optimization_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4225965",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_targets[0])\n",
    "plt.scatter(np.arange(len(run_targets[0])), run_targets[0], s=6)\n",
    "# plt.plot(run_targets[1])\n",
    "# plt.scatter(np.arange(len(run_targets[1])), run_targets[1], s=6)\n",
    "# plt.plot(run_targets[2])\n",
    "# plt.scatter(np.arange(len(run_targets[2])), run_targets[2], s=6)\n",
    "# plt.plot(run_targets[3])\n",
    "# plt.scatter(np.arange(len(run_targets[3])), run_targets[3], s=6)\n",
    "# plt.plot(run_targets[4])\n",
    "# plt.scatter(np.arange(len(run_targets[4])), run_targets[4], s=6)\n",
    "# plt.title(\"Running Minimum Objective Value for MNL Synthesis\")\n",
    "# plt.ylim(-0.01, 0.47)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34094ad9",
   "metadata": {},
   "source": [
    "Choose the params that gave the best objective value across all random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eccd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optimization_results[np.argmax([x[0]['target'] for x in optimization_results])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1e3fa",
   "metadata": {},
   "source": [
    "Generate 20 synthetic data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmse_ratios, full_sXs, GMM = train_models_mn(train_data=train_data,\n",
    "                                             number_synthetic_datasets=20,\n",
    "                                             # hyperparameters for GMM\n",
    "                                             number_gmm_initializations=ngi,\n",
    "                                             num_components_=int(best_params['params']['num_components_']),\n",
    "                                             # hyperparameters for CART, end with underscore means Bayesian optimization will choose\n",
    "                                             C_non_white_=best_params['params']['C_non_white_'],\n",
    "                                             C_sex_=best_params['params']['C_sex_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbcf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params['params']['C_sex_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pmse_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(pmse_ratios)\n",
    "plt.xlabel(\"Density\")\n",
    "plt.ylabel(\"pMSE Ratio\")\n",
    "plt.title(\"Distribution of pMSE Ratios\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4b464",
   "metadata": {},
   "source": [
    "# Save the synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sX in enumerate(full_sXs):\n",
    "    sX.to_csv(\"Data/synthetic_datasets/logistic_logistic_pmse_\" + str(i) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cacd36",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
